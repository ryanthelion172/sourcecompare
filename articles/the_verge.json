[ { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/3/24259479/senator-laphonza-butler-ai-california-workforce-harris", "category": "Policy", "date": "Two hours ago", "author": "Lauren Feiner", "title": "Senator Laphonza Butler thinks supporting Big AI or human workers is a \u2018false choice\u2019", "content": "Representing California in Congress comes with a unique challenge: navigating national politics while reflecting the interests of the most populous state in the US, including a large constituency from the tech industry. It\u2019s a challenge both current California Sen. Laphonza Butler and Vice President Kamala Harris \u2014 who previously held that title \u2014 have taken on. And right now, governing the tech world means addressing AI. Congress hasn\u2019t made much headway on a national framework for regulating generative AI. But California is the epicenter of the AI industry, home to companies like OpenAI and Google. On the national stage, Harris has acted as an AI czar within the Biden administration, leading discussions with industry players and civil society leaders about how to regulate it. Butler, who has a long history with the VP, is focusing on a specific problem: how AI systems impact labor and social equity. Butler spoke with The Verge about balancing the interests of AI companies and the people their products impact, including workers who fear being automated out of a job. \u201cIt all starts with listening,\u201d says Butler, a former labor leader. \u201cIt starts with listening to both the developers, the communities potentially impacted negatively, and the spaces where opportunity exists.\u201d  A balancing act Like many officials, Butler says she wants to help protect Americans from the potential dangers of AI without stifling opportunities that could come from it. She praised both Schumer and the Biden administration for \u201ccreating spaces for communities to have [a] voice.\u201d Both have brought in labor and civil society leaders in addition to major AI industry executives to educate and engage on regulation in the space. Butler insists lawmakers don\u2019t need to make \u201cfalse choices\u201d between the interests of AI company executives and the people who make up the workforce. \u201cListening is fundamental, balancing everyone\u2019s interest, but the goal has to be to do the most good for the most people. And to me, that is where a policymaker will always tend to land.\u201d California state Senator Scott Wiener made similar statements about his hotly contested state-level bill, SB 1047. The bill, which would have required whistleblower protections and safeguards for potentially disastrous events at large AI companies, made it all the way to Gov. Gavin Newsom\u2019s desk before being vetoed, with companies like OpenAI warning it would slow innovation. In August, Wiener argued that \u201cwe can advance both innovation and safety; the two are not mutually exclusive.\u201d So far, however, lawmakers are struggling to find a balance between the two. More work to do Butler praises the steps both Schumer and the Biden-Harris administration have taken so far to create appropriate guardrails around AI but says \u201cthere\u2019s always more to do.\u201d Schumer laid out a roadmap earlier this year about how to shape AI policy (though it didn\u2019t specifically introduce actual legislation), and the White House has secured voluntary commitments from AI companies to develop the technology safely. One of Butler\u2019s recent contributions is the Workforce of the Future Act, which she introduced with Sen. Mazie Hirono (D-HI). The bill would direct the Department of Labor, National Science Foundation, and Department of Education to study the impact of AI across job sectors, and it would create a $250 million grant program to prepare workers for the skills they\u2019ll need in the future, especially in industries likely to see job displacement.  \u201cHopefully, by both readying the work workforce of today but also preparing the workforce of tomorrow, we\u2019ll be able to catch the full opportunity that is the deployment of artificial intelligence,\u201d Butler says. Butler sees this as a moment in US history where policymakers could \u201cget ahead of what we know is going to be eventual disruption and try to create a pipeline of opportunities that can again help to both stabilize our economies by creating equitable opportunity.\u201d But Butler is realistic about the dynamics of Congress and the upcoming election in just over a month. \u201cYou and I both know that this 118th Congress is rapidly coming to a close, with a lot of business in front of it right now,\u201d she says. And Butler believes legislators still need to have important conversations with people representing different sides of the issue before advancing comprehensive AI legislation. And there\u2019s also, she notes, the small issue of \u201cgetting through the next presidential election this November.\u201d " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/3/24261238/meta-ray-ban-update-reminders-voice-messages", "category": "Meta", "date": "Two hours ago", "author": "Emma Roth", "title": "A new Ray-Ban Meta update adds reminders and voice messages", "content": "Meta is bringing reminders to its Ray-Ban smart glasses, a feature it previewed during its Connect event last month. With the update, you can ask Meta AI to remember your surroundings, like where you parked, and even time up reminders to make a phone call. The other features coming with the update include the ability to send and record voice messages on WhatsApp or Messenger without taking out your phone. You can now ask Meta AI to scan QR codes or call phone numbers that you come across, too. Meta is also updating how you invoke Meta AI while wearing the smart glasses. Instead of saying \u201cHey Meta\u201d before each question, you now only need to say \u201cHey Meta\u201d for your first question and then ask any additional questions without the prompt. You also no longer have to tell Meta AI to \u201clook and\u201d when you\u2019re asking about something you see. Meta\u2019s AI assistant is currently only available in the US and Canada. You can access the new features by updating the Meta View app on iOS and Android to version 186, which started rolling out on Wednesday. Along with this update, Meta revealed that its limited edition translucent Ray-Bans have already sold out online (but some may still be available in stores). It\u2019s been almost one year since the launch of the $299 Ray-Ban Meta glasses, and they\u2019ve shown a lot of promise, especially when Meta rolled out the ability to process images, text, and audio earlier this year. In future updates, Meta will launch the ability for its smart glasses to translate speech in real time, starting with English, French, Italian, and Spanish.  It\u2019s not clear if Meta plans to address the possibility of bad actors using the smart glasses as a way to dox people\u2019s identities, which is an issue two college students recently called attention to. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/3/24261161/microsoft-copilot-future-hardware-ai-notepad", "category": "Microsoft", "date": "4:00 PM UTC", "author": "Tom Warren", "title": "How Microsoft is thinking about the future of Copilot and AI hardware", "content": "It\u2019s been a big week for Copilot and Microsoft\u2019s AI efforts. Microsoft unveiled a redesigned Copilot that\u2019s aimed at consumers, complete with voice capabilities and the ability to understand what you\u2019re looking at on a computer. These new Copilot Voice and Vision features feel like a key evolution in Microsoft\u2019s effort to make Copilot a more personal companion and hint at what\u2019s to come for AI experiences. I got to speak to Windows and Surface chief Pavan Davuluri and consumer chief marketing officer Yusuf Mehdi to better understand the future of AI for Copilot and Windows. Microsoft is thinking about totally reimagined apps on Windows thanks to AI and the possibility of dedicated Copilot hardware in the future. The redesigned Copilot is unlike anything I\u2019ve seen Microsoft release in recent years. After hiring key Inflection AI staff earlier this year, including Microsoft AI CEO Mustafa Suleyman, the new team has moved to quickly and clearly exert its influence on Microsoft\u2019s consumer efforts. The marketing videos are slicker, more friendly, and have less of a corporate feel. The new Copilot experience itself also looks more like Inflection AI\u2019s Pi chatbot, with new visual elements and prompts that are designed to get you interacting with Copilot more. It\u2019s all very different from how Copilot started out inside of the Bing search engine. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/3/24260637/googles-ai-overview-ads-launch", "category": "Google", "date": "4:00 PM UTC", "author": "Emma Roth", "title": "Google\u2019s AI search summaries officially have ads", "content": "Google is rolling out ads in AI Overviews, which means you\u2019ll now start seeing products in some of the search engine\u2019s AI-generated summaries. Let\u2019s say you\u2019re searching for ways to get a grass stain out of your pants. If you ask Google, its AI-generated response will offer some tips, along with suggestions for products to purchase that could help you remove the stain. The products will appear beneath a \u201csponsored\u201d header, and Google spokesperson Craig Ewer told The Verge they\u2019ll only show up if a question has a \u201ccommercial angle.\u201d  Google has been testing ads in AI Overviews since May but says it\u2019s moving forward with a full rollout because it helps people \u201cquickly connect with relevant businesses, products and services to\u00a0take the next step at the exact moment they need them.\u201d For now, ads are only coming to AI Overviews in the US on mobile. Microsoft similarly includes ads in its Copilot chatbot and recently changed how they surface in responses. Google is also making some tweaks to the formatting of AI Overviews. Following a test in August, Google will now display cited webpages more prominently on the right side of the summary, as it found this \u201chas driven an increase in traffic to supporting websites compared to the previous design.\u201d  It\u2019s rolling out AI-organized search pages as well \u2014 a feature that displays a custom results page with relevant information instead of showing just a list of links. This is currently only available on mobile in the US for searches related to recipes and meal ideas. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/3/24259759/google-lens-search-video-ai-launch", "category": "Google", "date": "4:00 PM UTC", "author": "Emma Roth", "title": "Google Lens now lets you search with video", "content": "If you can\u2019t capture what you want to search for with just a picture, Google Lens will now let you take a video \u2014 and even use your voice to ask about what you\u2019re seeing. The feature will surface an AI Overview and search results based on the video\u2019s contents and your question. It\u2019s rolling out in Search Labs on Android and iOS today. Google first previewed using video to search at I/O in May. As an example, Google says someone curious about the fish they\u2019re seeing at an aquarium can hold up their phone to the exhibit, open the Google Lens app, and then hold down the shutter button. Once Lens starts recording, they can say their question: \u201cWhy are they swimming together?\u201d Google Lens then uses the Gemini AI model to provide a response, similar to what you see in the GIF below. When speaking about the tech behind the feature, Rajan Patel, the vice president of engineering at Google, told The Verge that Google is capturing the video \u201cas a series of image frames and then applying the same computer vision techniques\u201d previously used in Lens. But Google is taking things a step further by passing the information to a \u201ccustom\u201d Gemini model developed to \u201cunderstand multiple frames in sequence... and then provide a response that is rooted in the web.\u201d There isn\u2019t support for identifying the sounds in a video just yet \u2014 like if you\u2019re trying to identify a bird you\u2019re hearing \u2014 but Patel says that\u2019s something Google has been \u201cexperimenting with.\u201d  Google Lens is also updating its photo search feature with the ability to ask a question using your voice. To try it, aim your camera at your subject, hold down the shutter button, and then ask your question. Before this change, you could only type your question into Lens after snapping a picture. Voice questions are rolling out globally on Android and iOS, but it\u2019s only available in English for now.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/3/24261160/elon-musk-xai-recruiting-party-openai-dev-day-sam-altman", "category": "Elon Musk", "date": "3:54 PM UTC", "author": "Kylie Robison", "title": "Inside Elon Musk\u2019s AI party at OpenAI\u2019s old headquarters", "content": "It had all the makings of a typical recruiting event for a tech startup in San Francisco. There was free food, drinks, and live music generated via code being written in real time. But there were also mandatory metal detector screenings, ID checks, and security guards everywhere. It was held by Elon Musk at the original Mission District headquarters of OpenAI, which Musk cofounded before leaving after (reportedly) failing to take it over. And Musk was there to convince people to join his latest startup, xAI.\u00a0 The timing felt intentional. That same day, OpenAI was hosting its annual Dev Day across town, where CEO Sam Altman had spoken hours earlier to a packed auditorium of developers. The Silicon Valley rumor mill was buzzing about OpenAI closing in on the largest round of funding ever for a startup, surpassing the amount Musk himself had just raised for xAI four months earlier.\u00a0 Around 8:30PM, the AI-generated music cut off, and Musk, surrounded by bodyguards, climbed onto a table in a sectioned-off area to address the room of engineers. He began talking about why he started xAI and moved it to the same office where he helped launch OpenAI nearly a decade ago. \u201cWe want to create digital superintelligence that is as benign as possible,\u201d Musk said at the Tuesday gathering, according to a partial recording of his remarks shared with The Verge. He then called on those in the crowd \u201cto join xAI and help build the intelligence and build useful applications to derive from that intelligence.\u201d For about an hour and a half, Musk took questions from the (predominantly male) audience, according to people in attendance. He said he believes we\u2019ll reach artificial general intelligence (AGI) in a couple of years; he hopes to build a \u201csupersonic jet company\u201d next; he plans to open-source xAI\u2019s models roughly nine months after they\u2019re released; and most importantly, he wants to move fast. He compared xAI\u2019s growth to the SR-71 Blackbird, an airplane that flew three times the speed of sound and provided the US with enemy information during the Cold War. \u201cNo SR-71 Blackbird was ever shot down, and it only had one strategy: to accelerate,\u201d Musk told the room per a post on X from an attendee.\u00a0 He predicted that OpenAI, Anthropic, Google, and xAI will be the main players in the AI race for the next five years. The goal of the party was to find engineers for xAI\u2019s API, one attendee said. Ultimately, he said he aspires for xAI to be as dominant in AI as SpaceX is in rockets.\u00a0 At 10PM, the fire marshal put an end to the recruiting event. Musk was briskly escorted out a backdoor with his security detail. Attendees, including some wearing OpenAI backpacks, walked out into the night with pizza slices. Maximum, truth-seeking AI xAI began in March 2023 on the 10th office floor of X, Musk\u2019s social media platform formerly known as Twitter. He assembled a team drawn from his other companies, including Tesla and SpaceX, as well as his 17-year-old son, his cousins, and the son of Jared Birchall, who runs his family office, The Verge has learned. The mission: surpass OpenAI and deliver a competitive large language model in just three months. Since then, xAI has expanded from a single floor at X to a larger office in the Stanford Research Park in Palo Alto. Musk tapped Igor Babuschkin, a former Google DeepMind researcher, to lead xAI. He also recruited researchers from OpenAI, Microsoft, and Meta.\u00a0 In May, xAI secured $6 billion in funding from several high-profile investors, including Andreessen Horowitz, Lightspeed Venture Partners, and Sequoia Capital, valuing the company at $24 billion. Investors in X own 25 percent of xAI, which benefits from the wealth of training data the social media platform produces every day. Under pressure from Musk, xAI\u2019s first model was launched in late 2023 via Grok, a chatbot for paid subscribers to X. It has since released three updates: Grok-1.5, Grok-2, and Grok-2 Mini. But unlike competitors who had the luxury of time to develop their own systems, xAI\u2019s lean team had to move fast and find a quick solution. One person familiar with the development of the first model described it as a patchwork product that relied on Microsoft\u2019s Bing for search and Meta\u2019s open-source Llama model for query rewriting.\u00a0 Musk is still relying on outside technology for core Grok features. Just over a month ago, xAI announced a deal with Black Forest Labs to power image generation. The feature lacked guardrails put in place by other image generators, allowing people to generate photos of anything from Taylor Swift in lingerie to Kamala Harris with a gun. Musk said on X that xAI was working on its own generator but that the Black Forest partnership let it launch one in Grok more quickly. A person familiar with what xAI is working on tells The Verge that voice and search features are in the works. The idea is that, like OpenAI and Meta\u2019s voice modes, Grok will be able to talk back. Musk also wants it to provide summaries of news stories shared on X and trending topics. Musk is now grappling with fierce competition in the AI race for top engineering talent and GPUs. As the richest person in the world, money isn\u2019t an issue for him, though \u2014 despite the financial pressure that X\u2019s plummeting value has created. While he\u2019s raised billions of dollars for xAI, Musk doesn\u2019t exactly need to create a profitable AI company any time soon \u2014 for him, taking down his runaway rival OpenAI seems to be satisfying enough. Musk cofounded OpenAI with its CEO Sam Altman and a group of partners in 2015, but Musk quit the board only three years later. At the time, he cited \u201cpotential future conflict\u201d due to Tesla\u2019s focus on AI. Later, he claimed he quit due to disagreements with the OpenAI team. And in March, he sued the company, alleging (fairly dubiously) that it broke a contract with him and abandoned its mission. In response, OpenAI shared emails between its leadership and Musk that revealed a power struggle where Musk planned to take sole control over the company. \u201cI just don\u2019t trust OpenAI for obvious reasons,\u201d Musk said during the recruiting party. \u201cIt is closed, for-maximum-profit AI.\u201d The falling out between Musk and OpenAI has evolved into a stiff game of one-upmanship. This week, OpenAI raised $6.6 billion at a $157 billion valuation, just outpacing Musk\u2019s historic funding round. Musk leveraged Tesla GPUs to build a data center nicknamed \u201cColossus\u201d and reportedly brought online 100,000 advanced Nvidia chips last week. Meanwhile, Altman is on a global mission, meeting with UAE leaders, Asian chipmakers, and US officials to raise $7 trillion for 36 semiconductor plants and data centers, all aimed at advancing OpenAI\u2019s pursuit of AGI. After Altman\u2019s latest funding round, he reportedly asked backers to not invest in competitors like xAI. Perhaps just as tough as nailing down funding or compute power is securing the top AI talent in Silicon Valley. The best researchers can easily earn millions, and the timing has never been better for them to launch their own startups. Many are driven by their own altruistic visions for AI\u2019s future, making their choice of company often based on its mission and leadership. While Musk\u2019s fame and bold visions give him an edge, it doesn\u2019t mean the battle for talent is any easier. So, Musk gathered a few hundred young engineers from rivals OpenAI and DeepMind \u2014 some fresh from attending his competitors\u2019 developer conference that very day \u2014 to do what he does best: sell his vision of the future. In Musk\u2019s world, AGI isn\u2019t controlled by companies like OpenAI or Google, who keep their best models private. Instead, it\u2019s owned by him and shared with the world. One potential draw for working at xAI, compared to a larger competitor like OpenAI, might be the opportunity to move faster and take bolder risks. With a small team and shorter product timelines, xAI offers a chance to innovate quickly, unlike larger, more cautious companies like OpenAI and Google. It may attract those eager to see rapid AI proliferation or who align with Musk\u2019s techno-libertarian leanings \u2014 people who reject Silicon Valley\u2019s \u201cwokeness\u201d in favor of a \u201csuper hardcore\u201d work environment that prioritizes ambition and agility over corporate safeguards. \u201cMy personal belief is that the best way to achieve AI safety is to have a maximum, truth-seeking AI,\u201d Musk said at the recruiting party.\u00a0 Like a lot of things at Musk companies, the event came together rapidly. \u201cHe said that he had the idea for this event last Wednesday and that the office was unfurnished then,\u201d said one source who attended the party. But the source called it an \u201coverall chill event\u201d focused on answering questions about not just AI but also Musk\u2019s many other enterprises. \u201cElon\u2019s committed to winning and accelerating xAI,\u201d another source said. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/3/24260561/gemini-live-language-support", "category": "Google", "date": "1:00 PM UTC", "author": "Allison Johnson", "title": "Gemini Live will support more languages for its AI voice chat", "content": "Google initially rolled out Gemini Live, its conversational AI voice chat, in just one language: English. Today, the company is expanding the service to a handful of other languages, starting with French, German, Portuguese, Hindi, and Spanish. And while support for these languages does appear to be imminent for a lot of people, the company is still couching promises of other Gemini features with fuzzy \u201ccoming weeks\u201d timelines. Google expects that the languages starting to roll out today will be available to all users \u201cin a couple of weeks.\u201d As for other languages, Google says it will have more than 40 languages over \u201cthe coming weeks,\u201d which is harder to pin down. Still, Google\u2019s timeline from announcement to full rollout for Gemini Live has been unusually swift: it was revealed with the Pixel 9 series in mid-August as a subscriber-only feature. Just a month later, the company opened it up as a free feature for all Android users. Meanwhile, expanded capabilities for the Gemini virtual assistant \u2014 the non-voice mode, that is \u2014 have been on a slower track.  Earlier this year at I/O, Google announced that Gemini extensions for Calendar, Tasks, and Keep were on the way. The company\u2019s update today reinforces that promise, reminding us that these extensions will allow Gemini to do things like add a bunch of dates from a flyer to your calendar or build a shopping list from a picture of a recipe. Really useful stuff! But when\u2019s it all arriving? \u201cOver the coming weeks.\u201d Sounds familiar. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/2/24260457/openai-funding-round-thrive-capital-6-billion", "category": "Tech", "date": "Oct 2", "author": "Kylie Robison", "title": "OpenAI just raised $6.6 billion to build ever-larger AI models", "content": "OpenAI just closed a historic funding round, taking in a $6.6 billion investment at a $157 billion valuation, to continue pursuing its mission to build artificial-general intelligence according to a company blog post. The funding round was led by\u00a0Thrive Capital, which committed $1 billion, according to the Financial Times. It was also reported that Thrive got a special deal (not offered to other investors) that allows it to invest another $1 billion next year at the same valuation if the AI firm hits a revenue goal, Reuters reported. These funds are apparently contingent on OpenAI going through with a rumored restructure as a for-profit company. The company\u2019s for-profit wing is currently overseen by a nonprofit research body, and investor profits are capped at 100x. If OpenAI doesn\u2019t restructure itself as a for-profit company within two years, Axios reported, investors can ask for their money back. Last week,\u00a0Reuters\u00a0reported\u00a0that the company is considering becoming a public benefit corporation (like Anthropic). In a rare move, OpenAI also asked investors to avoid backing rival start-ups such as Anthropic and Elon Musk\u2019s xAI, the Financial Times reported. It\u2019s worth noting that OpenAI\u2019s latest funding round just barely surpasses xAI, which raised $6 billion in May.  This funding round values OpenAI at roughly 40 times its reported revenue, an unprecedented figure that highlights just how much hype surrounds AI in Silicon Valley. The New York Times reported that OpenAI\u2019s monthly revenue hit $300 million in August, and the company expects about $3.7 billion in annual sales this year (and estimates that its revenue will reach $11.6 billion next year.) These billions will go toward the incredibly expensive task of training AI frontier models. Anthropic CEO Dario Amodei has said AI models that cost $1 billion to train are in development and $100 billion models are not far behind. For OpenAI, which wants to build a series of \u201creasoning\u201d models, those costs are only expected to balloon \u2014 making fresh funding rounds like this one critical. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/2/24260392/amazon-fire-hd-8-tablet-ai", "category": "Amazon", "date": "Oct 2", "author": "Jay Peters", "title": "Amazon\u2019s new Fire tablets have AI inside", "content": "Amazon just announced a new Fire HD 8 tablet, and like many new gadgets in 2024, generative AI-powered tools are among the big new features. But you won\u2019t necessarily need to buy the new Fire HD 8 to try them; Amazon says they\u2019re coming to Fire HD 10 and Fire Max 11 tablets, too. The new AI tools are things you\u2019ve probably seen before. There\u2019s a writing assist feature, which will help you polish up your writing and is built into the device\u2019s keyboard. You\u2019ll be able to get webpage summaries when using Amazon\u2019s Silk browser. And you\u2019ll be able to create a wallpaper from a prompt. Amazon says the tools will start rolling out to \u201call compatible Fire tablets later this month.\u201d Writing Assist and Wallpaper Creator will come to the Fire Max 11 (2023), Fire HD 10 (2023), and Fire HD 8 (2022 and 2024), according to Amazon spokesperson Jackie Burke. Webpage Summaries will come to the Fire Max 11 (2023), Fire HD 10 (2019, 2021, 2023), and Fire HD 8 (2018, 2020, 2022 and 2024). Amazon is also rumored to be launching an upgraded Alexa that you might have to pay for, but that wasn\u2019t included as part of this announcement. If you\u2019re interested in the new Fire HD 8, it has some improved specs that seem to have trickled down from 2022\u2019s Fire 8 HD Plus, including 3GB of RAM (up from 2GB \u2014 the boost might help with the new AI features) and a 5MP back camera (up from 2MP). Like with the previous Fire 8 HD, Amazon is also promising up to 13 hours of battery life and is offering 32GB or 64GB of storage (with the option to expand that with a microSD card). You can buy the new Fire 8 HD right now in black, emerald, and hibiscus. The tablets technically have a starting price of $99.99, but they\u2019re already on sale ahead of Amazon\u2019s Prime Big Deal Days, meaning you can get the base model for nearly half off at $54.99. Amazon might also launch updated Kindle e-readers soon, as a Spanish retailer recently listed a new entry-level model with a brighter screen. Update, October 2nd: Added information about which Amazon tablets will get certain AI features. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/1/24259261/microsoft-event-copilot-ai-windows", "category": "Microsoft", "date": "Oct 1", "author": "Umar Shakir", "title": "All the news on Microsoft\u2019s latest Copilot and Windows AI features", "content": "At Microsoft\u2019s New York City event today, it\u2019s announcing an all-new Copilot experience. The new Copilot design includes a new card-based look across mobile, web, and Windows. Copilot is getting more personalized with features like Copilot Vision, which adds the ability to see what you\u2019re looking at, an OpenAI-like natural voice conversation mode, and a virtual news presenter mode that can read the headlines to you. Windows 11 is getting new features like Phone Link status in the start menu that can show notifications and your phone\u2019s battery life. And both Paint and Photos are getting fun new features like Generative Fill and Erase. Copilot Plus PCs are getting a revamped AI-powered Windows Search that includes a Google Circle to Search-like \u201cClick to Do\u201d feature and the ability to search for a photo using just a text description. Earlier this year, an exec reorganization put Pavan Davuluri in charge of Windows and Surface and made Mustafa Suleyman the new CEO of AI. Now, a full year after Panos Panay\u2019s abrupt departure, we will find out more about where Microsoft\u2019s \u201cAI PC\u201d push is headed. You can read all the updates from the event below. Microsoft is launching a redesigned version of Copilot today, intent on becoming an AI assistant or companion. To celebrate this, Microsoft\u2019s new AI CEO, Mustafa Suleyman, has penned a 700-plus-word memo on what he describes as a \u201ctechnological paradigm shift\u201d toward AI models that can understand what humans see and hear. Suleyman joined Microsoft earlier this year as the CEO of its new Microsoft AI division, amid the software giant\u2019s hiring of a number of key Inflection AI staff. In June, Suleyman sparked controversy after brazenly claiming that anything published on the web is \u201cfreeware\u201d that can be copied, recreated, and reproduced by AI models. Now, he\u2019s optimistic that AI \u2014 under Microsoft\u2019s stewardship \u2014 will create a \u201ccalmer, more helpful and supportive era of technology, quite unlike anything we\u2019ve seen before.\u201d Microsoft is bringing some new AI-powered Paint and Photos features to Copilot Plus PCs that could make creatives less reliant on more powerful image editing software. Generative Fill and Generative Erase \u2014 which appear to be heavily inspired by similar AI tools in Adobe Photoshop \u2014 are being introduced to Paint, allowing users to precisely add or remove objects in their images. Both tools utilize a size-adjustable brush to \u201cpaint\u201d over specific areas of an image to edit. Generative Erase will remove unwanted figures, objects like background clutter, and other distractions, similar to the Magic Eraser feature on Google\u2019s Pixel phones. Generative Fill allows Paint users to add new AI-generated assets to an image using a text description and select precisely where they should be placed \u2014 much like the Photoshop tool that shares the same name. Microsoft is using AI models to greatly improve Windows search on its new Copilot Plus PCs, including the addition of a new Click to Do feature that\u2019s very similar to Google\u2019s Circle to Search. These search improvements will make it easier to find and interact with images, emails, documents, and even videos and are just a few of the AI-based features coming to Copilot Plus PCs starting in November. The improved Windows search will first show up in File Explorer on Copilot Plus PCs next month, allowing you to search for pictures using words, even if the search word isn\u2019t found in the photo or file name. Microsoft is starting to release its Windows 11 2024 update today, also known as version 24H2. This update includes a number of small but useful additions to Windows 11 that improve the Start menu, File Explorer, Settings, and much more. The Start menu has a big change for those using Microsoft\u2019s Phone Link software. The Windows 11 2024 update adds a side panel to the Start menu that provides information on your phone\u2019s battery status and notifications as well as quick access to messages, calls, and photos. The floating panel is a neat addition for Phone Link users. Microsoft is unveiling a big overhaul of its Copilot experience today, adding voice and vision capabilities to transform it into a more personalized AI assistant. As I exclusively revealed in my Notepad newsletter last week, Copilot\u2019s new capabilities include a virtual news presenter mode to read you the headlines, the ability for Copilot to see what you\u2019re looking at, and a voice feature that lets you talk to Copilot in a natural way, much like OpenAI\u2019s Advanced Voice Mode. Copilot is being redesigned across mobile, web, and the dedicated Windows app into a user experience that\u2019s more card-based and looks very similar to the work Inflection AI has done with its Pi personalized AI assistant. Microsoft hired a bunch of folks from Inflection AI earlier this year, including Google DeepMind cofounder Mustafa Suleyman, who is now CEO of Microsoft AI. This is Suleyman\u2019s first big change to Copilot since taking over the consumer side of the AI assistant. Microsoft is working on an overhaul of its Copilot mobile app that includes a new feature that will transform the AI assistant into a virtual news presenter. Multiple sources familiar with Microsoft\u2019s plans tell me that the software giant has been testing a completely redesigned Copilot app in recent weeks that looks unlike any of Microsoft\u2019s other apps. The Copilot redesign surfaces topics you can choose from based on your own interests or your history of asking Copilot questions. The AI assistant might offer to generate a story for you one day, ask if you to do a workout the next, or simply surface the latest sports scores without you having to ask for them. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/1/24259209/microsoft-ai-ceo-mustafa-suleyman-copilot-ai-companion-memo", "category": "Microsoft", "date": "Oct 1", "author": "Tom Warren", "title": "Read\u00a0Microsoft\u2019s optimistic memo about the future of AI companions", "content": "Microsoft is launching a redesigned version of Copilot today, intent on becoming an AI assistant or companion. To celebrate this, Microsoft\u2019s new AI CEO, Mustafa Suleyman, has penned a 700-plus-word memo on what he describes as a \u201ctechnological paradigm shift\u201d toward AI models that can understand what humans see and hear. Suleyman joined Microsoft earlier this year as the CEO of its new Microsoft AI division, amid the software giant\u2019s hiring of a number of key Inflection AI staff. In June, Suleyman sparked controversy after brazenly claiming that anything published on the web is \u201cfreeware\u201d that can be copied, recreated, and reproduced by AI models. Now, he\u2019s optimistic that AI \u2014 under Microsoft\u2019s stewardship \u2014 will create a \u201ccalmer, more helpful and supportive era of technology, quite unlike anything we\u2019ve seen before.\u201d \u201cSome people worry that AI will diminish what makes us unique as humans,\u201d says Suleyman. \u201cMy life\u2019s work has been to ensure it does precisely the opposite.\u201d Microsoft\u2019s new Copilot experience looks a lot like Inflection AI\u2019s Pi product, and it\u2019s clear Suleyman is now pushing Microsoft in a more personalized AI direction. \u201cAt Microsoft AI, we are creating an AI companion for everyone,\u201d says Suleyman in his memo. \u201cCopilot will be there for you, in your corner, by your side, and always strongly aligned with your interests.\u201d Microsoft is launching new Copilot Vision and Voice features today to make the AI assistant a lot more personalized, alongside a refreshed design that\u2019s focused on surfacing useful information. \u201cOver time it\u2019ll adapt to your mannerisms and develop capabilities built around your preferences and needs,\u201d says Suleyman. \u201cWe are not creating a static tool so much as establishing a dynamic, emergent, and evolving interaction.\u201d Here\u2019s Suleyman\u2019s memo in full: We\u2019re living through a technological paradigm shift. In a few short years, our computers have learnt to speak our languages, to see what we see and hear what we hear. Yet technology for its own sake counts for nothing. What matters is how it feels to people and what impact it has on societies. It\u2019s about how it changes lives, opens doors, expands minds, and relieves pressure. It is perhaps the greatest amplifier of human well-being in history, one of the most effective ways to create tangible and lasting benefits for billions of people. And yet technology is, and must always remain, in service to humanity: an enabler, a path to deepening our common bonds and shared understanding, our energy and imagination, our creativity and capacity for everything from invention to forming relationships. In the field of AI, we often get caught up in the technical details. We spend our time talking about parameters and compute. The focus is on training runs, data centers and the latest techniques. This is natural and inevitable when operating on the frontiers of something new, where the details do really matter. But I think it\u2019s important that in doing all of this, getting stuck right into the technical weeds, that we don\u2019t lose sight, not just of what we are building, but why we are building it.\u00a0At Microsoft AI, we are creating an AI companion for everyone. I truly believe we can create a calmer, more helpful and supportive era of technology, quite unlike anything we\u2019ve seen before. Great technology experiences are about how you feel, not what\u2019s under the hood. It should be about what you experience, not what we are building.Copilot will be there for you, in your corner, by your side, and always strongly aligned with your interests. It understands the context of your life while safeguarding your privacy, data, and security, remembering the details that are most helpful in any situation. It gives you access to a universe of knowledge, simplifying, and decluttering the daily barrage of information, and offering support and encouragement when you want it.\u00a0Over time it\u2019ll adapt to your mannerisms and develop capabilities built around your preferences and needs. We are not creating a static tool so much as establishing a dynamic, emergent, and evolving interaction. It will provide you with unwavering support to help you show up the way you really want in your everyday life, a new means of facilitating human connections and accomplishments alike. With your permission, Copilot will ultimately be able to act on your behalf, smoothing life\u2019s complexities and giving you more time to focus on what matters to you. It will be an advocate for you in many of life\u2019s most important moments. It\u2019ll accompany you to that doctor\u2019s appointment, taking notes and following up at the right time. It\u2019ll share the load of planning and preparing for your child\u2019s birthday party. And it\u2019ll be there at the end of the day to help you think through a tricky life decision.\u00a0Some people worry that AI will diminish what makes us unique as humans. My life\u2019s work has been to ensure it does precisely the opposite. We choose what we create. This is something we must do together. Our task is to ensure it always enriches people\u2019s lives and strengthens our bonds with others, whilst supporting our uniqueness and endlessly complex humanity. This is a new era of technology that doesn\u2019t just \u201csolve problems\u201d, it\u2019s there to support you, teach you, help you. In this sense, Copilots really are different to that last wave of the web and mobile. This is the beginning of a fundamental shift in what\u2019s possible for all of us. It\u2019s a long journey that will take years. With our latest updates to Copilot, you are seeing only the first careful steps in this direction. Patience and care with our deployments are at the very foundation of our approach. My commitment is to be accountable at every stage, to work with you and listen to you. Respect and deep compassion for our users and for society is the core purpose behind everything we do. It comes first. This is a journey we promise to take together. I couldn\u2019t be more excited to embark on it with you.Mustafa Suleyman " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/1/24259239/microsoft-paint-generative-erase-fill-photos-upscaling", "category": "Microsoft", "date": "Oct 1", "author": "Jess Weatherbed", "title": "Microsoft Paint is getting Photoshop-like generative AI fill and erase features", "content": "Microsoft is bringing some new AI-powered Paint and Photos features to Copilot Plus PCs that could make creatives less reliant on more powerful image editing software. Generative Fill and Generative Erase \u2014 which appear to be heavily inspired by similar AI tools in Adobe Photoshop \u2014 are being introduced to Paint, allowing users to precisely add or remove objects in their images. Both tools utilize a size-adjustable brush to \u201cpaint\u201d over specific areas of an image to edit. Generative Erase will remove unwanted figures, objects like background clutter, and other distractions, similar to the Magic Eraser feature on Google\u2019s Pixel phones. Generative Fill allows Paint users to add new AI-generated assets to an image using a text description and select precisely where they should be placed \u2014 much like the Photoshop tool that shares the same name. These build on the Cocreator tool for Paint announced for Copilot Plus PCs earlier this year that can generate images using a combination of text prompts and reference sketches. The company says the diffusion-based model powering these features has been updated to improve output quality and speed and now includes \u201cbuilt-in moderation\u201d to help prevent it from being abused. Microsoft\u2019s Photos app is also getting the Generative Erase tool, alongside a new Super-Resolution feature that uses on-device AI to upscale blurry or pixelated images. Users can upscale images by up to eight times their original resolution and adjust the level of upscaling with a slider, matching the capabilities of tools like Canva\u2019s image upscaler and exceeding Adobe Lightroom\u2019s 4x Super Resolution enhancement. It\u2019s available for free and is speedy enough to upscale images \u201cup to 4K within seconds,\u201d according to Microsoft. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/10/1/24258337/microsoft-windows-ai-features-click-to-do-super-resolution-photos", "category": "Microsoft", "date": "Oct 1", "author": "Tom Warren", "title": "Microsoft is using AI to improve Windows search", "content": "Microsoft is using AI models to greatly improve Windows search on its new Copilot Plus PCs, including the addition of a new Click to Do feature that\u2019s very similar to Google\u2019s Circle to Search. These search improvements will make it easier to find and interact with images, emails, documents, and even videos and are just a few of the AI-based features coming to Copilot Plus PCs starting in November. The improved Windows search will first show up in File Explorer on Copilot Plus PCs next month, allowing you to search for pictures using words, even if the search word isn\u2019t found in the photo or file name. \u201cAI-powered search makes it dramatically easier to find virtually anything,\u201d says Yusuf Mehdi, executive vice president and consumer chief marketing officer at Microsoft. \u201cYou no longer need to remember file names and document locations, nor even specific names of words. Windows will better understand your intent and match the right document, image, file, or email.\u201d This improved search will also be available \u201cin the coming months\u201d in the main Windows search interface and through the search box that appears in the Settings interface. You can type things like \u201cadd my headphones\u201d into the Settings search box and it will help you find the right settings.  Search hasn\u2019t been great in Windows for years, so this AI-powered natural language search should greatly improve things \u2014 as long as it works as well as Microsoft promises. Microsoft is leveraging the NPU chips on new Copilot Plus PCs to enable local search on OneDrive content without having to be connected to the internet. Alongside Windows search, Microsoft will also start rolling out Click to Do on Copilot Plus PCs next month, which is very similar to Google\u2019s Circle to Search feature. With Click to Do, you hit the Windows key on a keyboard and left-click on a mouse to see an interactive overlay appear on your screen, which lets you select images or text to perform clickable actions.  \u201cClick to Do works by first understanding everything you\u2019ve seen on your screen and enabling useful shortcuts to actions to help you more quickly search, learn, edit, shop, or act on those items,\u201d explains Mehdi. \u201cIt works on any windows, document, image, or even video.\u201d You can use Click to Do on things like a YouTube video you\u2019re watching to do a visual search on an item that appears in the video using Bing. Click to Do is also context-aware, so it can help with text-related actions like rewriting or summarizing documents or explaining text and sending emails. Microsoft will start testing these new search and Click to Do features with Windows Insiders on Copilot Plus PCs in October, followed by a gradual rollout in November. The previously announced Recall feature is also coming to testers in October on Qualcomm-powered devices, before being available to Windows Insiders on Intel- or AMD-powered Copilot Plus PCs in November. Microsoft says \u201ctiming details on the broad availability of Recall will be shared soon.\u201d Microsoft is also adding generative fill and erase to Microsoft Paint as part of these new Windows AI features. You can read more about the Paint and Photos AI additions right here. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/9/30/24258134/raspberry-pi-ai-camera-module-sony-price-availability", "category": "Tech", "date": "Sep 30", "author": "Jess Weatherbed", "title": "Raspberry Pi and Sony made an AI-powered camera module", "content": "Raspberry Pi and Sony have co-developed a Raspberry Pi AI Camera module that\u2019s launching today for $70. It comes with onboard AI processing that can help Raspberry Pi users develop \u201cedge AI solutions that process visual data\u201d with ease, according to the tiny computer maker. The new camera builds on Raspberry Pi\u2019s plans to offer chips and add-ons for AI developers, having previously released several non-AI camera modules since its first 5-megapixel offering in 2013. \u201cAI-based image processing is becoming an attractive tool for developers around the world,\u201d Raspberry Pi CEO Eben Upton said in a press release. \u201cWe look forward to seeing what our community members are able to achieve using the power of the Raspberry Pi AI Camera.\u201d\u00a0 The AI camera is compatible with all Raspberry Pi single-board computers, and pairs the company\u2019s RP2040 microcontroller chip with Sony\u2019s IMX500 image sensor \u2014 the latter of which handles AI processing. The combination eliminates the need for additional components like accelerators or a graphics processing unit (GPU), which are typically required for camera modules to handle large-scale visual data. The 12.3 megapixel Raspberry Pi AI Camera can capture footage at either 10 frames per second in 4056 x 3040, or 40fps at 2028 x 1520. It also has a manually adjustable focus, a 76-degree field of view, and measures 25 x 24 x 11.9mm \u2014 making it almost identical in size to the Camera Module 3 that Raspberry Pi released last year. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/9/29/24232172/california-ai-safety-bill-1047-vetoed-gavin-newsom", "category": "Tech", "date": "Sep 29", "author": "Emma Roth", "title": "California governor vetoes major AI safety bill", "content": "California Governor Gavin Newsom vetoed the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047) today. In his veto message, Governor Newsom cited multiple factors in his decision, including the burden the bill would have placed on AI companies, California\u2019s lead in the space, and a critique that the bill may be too broad. \u201cWhile well-intentioned, SB 1047 does not take into account whether an AI system is deployed in high-risk environments, involves critical decision-making or the use of sensitive data. Instead, the bill applies stringent standards to even the most basic functions \u2014 so long as a large system deploys it. I do not believe this is the best approach to protecting the public from real threats posed by the technology.\u201d Newsom writes that the bill could \u201cgive the public a false sense of security about controlling this fast-moving technology.\u201d \u201cSmaller, specialized models may emerge as equally or even more dangerous than the models targeted by SB 1047 - at the potential expense of curtailing the very innovation that fuels advancement in favor of the public good.\u201d The Governor says he agrees that there should be safety protocols and guardrails in place, as well as \u201cclear and enforceable\u201d consequences for bad actors. However, he states that he doesn\u2019t believe the state should \u201csettle for a solution that is not informed by an empirical trajectory analysis of Al systems and capabilities.\u201d Here is the full veto message: In a post on X, Senator Scott Wiener, the bill\u2019s main author, called the veto \u201ca setback for everyone who believes in oversight of massive corporations that are making critical decisions\u201d affecting public safety and welfare and \u201cthe future of the planet.\u201d  \u201cThis veto leaves us with the troubling reality that companies aiming to create an extremely powerful technology face no binding restrictions from U.S. policymakers, particularly given Congress\u2019s continuing paralysis around regulating the tech industry in any meaningful way.\u201d In late August, SB 1047 arrived on Gov. Newsom\u2019s desk, poised to become the strictest legal framework around AI in the US, with a deadline to either sign or veto it as of September 30th.  It would have applied to covered AI companies doing business in California with a model that costs over $100 million to train or over $10 million to fine-tune, adding requirements that developers implement safeguards like a \u201ckill switch\u201d and lay out protocols for testing to reduce the chance of disastrous events like a cyberattack or a pandemic. The text also establishes protections for whistleblowers to report violations and enables the AG to sue for damages caused by safety incidents. Changes since its introduction included removing proposals for a new regulatory agency and giving the state attorney general power to sue developers for potential incidents before they occur. Most companies covered by the law pushed back against the legislation, though some muted their criticism after those amendments. In a letter to bill author Senator Wiener, OpenAI chief strategy officer Jason Kwon said SB 1047 would slow progress and that the federal government should handle AI regulation instead. Meanwhile, Anthropic CEO Dario Amodei wrote to the governor after the bill was amended, listing his perceived pros and cons and saying, \u201c...the new SB 1047 is substantially improved, to the point where we believe its benefits likely outweigh its costs.\u201d The Chamber of Progress, a coalition that represents Amazon, Meta, and Google, similarly warned the law would \u201chamstring innovation.\u201d  Google spokesperson Jenn Crider said in an emailed statement to The Verge: \u201cWe join many small and larger developers in thanking Governor Newsom for helping California continue to lead in building responsible AI tools that benefit California, our nation, and the world. We look forward to working with the Governor\u2019s responsible AI initiative and the federal government on creating appropriate safeguards and developing tools that help everyone.\u201d Meta public affairs manager Jamie Radice emailed Meta\u2019s statement on the veto to The Verge:  \u201cWe are pleased that Governor Newsom vetoed SB1047. This bill would have stifled AI innovation, hurt business growth and job creation, and broken the state\u2019s long tradition of fostering open-source development. We support responsible AI regulations and remain committed to partnering with lawmakers to promote better approaches.\u201d The bill\u2019s opponents have included former House Speaker Nancy Pelosi, San Francisco Mayor London Breed, and\u00a0eight\u00a0congressional Democrats from California. On the other side, vocal supporters have included Elon Musk, prominent Hollywood names like Mark Hamill, Alyssa Milano, Shonda Rhimes, and J.J. Abrams, and unions including SAG-AFTRA and SEIU.  The federal government is also looking into ways it could regulate AI. In May, the Senate proposed a $32 billion roadmap that goes over several areas lawmakers should look into, including the impact of AI on elections, national security, copyrighted content, and more. Update September 30th: Updated attribution for Google\u2019s statement to Google spokesperson Jenn Crider. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/9/29/24257391/apple-smart-home-display-homeos-apple-intelligence-homepod-ipad-robot", "category": "Apple Rumors", "date": "Sep 29", "author": "Wes Davis", "title": "Apple may release an iPad-like smart home display next year", "content": "Apple is preparing to take a fresh run at the smart home that starts with a rumored smart display that it may release next year. That\u2019s according to Bloomberg\u2019s Mark Gurman, who writes in his Power On newsletter today that the display will use a new operating system, called homeOS, that\u2019s based on the Apple TV\u2019s tvOS (much like the software that drives HomePods now.) Gurman reports that the display will run Apple apps like Calendar, Notes, and Home, and that Apple has tested prototypes with magnets for wall-mounting. And it will support Apple Intelligence \u2014 something Apple\u2019s HomePods don\u2019t currently do.  Rumors of such a device have been going around for some time now, with form factors ranging from a HomePod with a screen to a display attached to a robotic arm that swivels to face you on video calls. But products along those lines have been sounding more real, lately. Another recent rumor suggested that a \u201cHomeAccessory\u201d device coming soon would be square-shaped, and that users might be able to use hand gestures from afar to control it, as 9to5Mac wrote earlier this week. And MacRumors has reported on apparent code references to the device and homeOS. A display like this sounds more down-to-Earth than Apple\u2019s robotic screen idea. It could also be less fiddly and hopefully less expensive than trying to use an iPad as a dedicated smart home controller (I\u2019ve tried; it\u2019s not a great experience!) We\u2019ll find out if and when it launches \u2014 which doesn\u2019t sound terribly far off.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/9/27/24255721/microsoft-windows-recall-ai-security-improvements-overhaul-uninstall", "category": "Microsoft", "date": "Sep 27", "author": "Tom Warren", "title": "Microsoft\u2019s more secure Windows Recall feature can also be uninstalled by users", "content": "In response to security concerns, Microsoft is detailing how it has overhauled its controversial AI-powered Recall feature that creates screenshots of mostly everything you see or do on a computer. Recall was originally supposed to debut with\u00a0Copilot Plus PCs\u00a0in June, but Microsoft has spent the past few months reworking the security behind it to make it an opt-in experience that you can now fully remove from Windows if you want. \u201cI\u2019m actually really excited about how nerdy we got on the security architecture,\u201d says David Weston, vice president of enterprise and OS security at Microsoft, in an interview with The Verge. \u201cI\u2019m excited because I think the security community is going to get how much we\u2019ve pushed [into Recall].\u201d One of Microsoft\u2019s first big changes is that the company isn\u2019t forcing people to use Recall if they don\u2019t want to. \u201cThere is no more on by default experience at all \u2014 you have to opt into this,\u201d says Weston. \u201cThat\u2019s obviously super important for people who just don\u2019t want this, and we totally get that.\u201d A Recall uninstall option initially appeared on Copilot Plus PCs earlier this month, and Microsoft said at the time that it was a bug. It turns out that you will indeed be able to fully uninstall Recall. \u201cIf you choose to uninstall this, we remove the bits from your machine,\u201d says Weston. That includes the AI models that Microsoft is using to power Recall.  Security researchers initially found that the Recall database \u2014 that stores snapshots taken every few seconds of your computer \u2014 wasn\u2019t encrypted, and malware could have potentially accessed the Recall feature. Everything that\u2019s sensitive to Recall, including its database of screenshots, is now fully encrypted. Microsoft is also leaning on Windows Hello to protect against malware tampering. The encryption in Recall is now bound to the Trusted Platform Module (TPM) that Microsoft requires for Windows 11, so the keys are stored in the TPM and the only way to get access is to authenticate through Windows Hello. The only time Recall data is even passed to the UI is when the user wants to use the feature and authenticates via their face, fingerprint, or PIN. \u201cTo turn it on to begin with, you actually have to be present as a user,\u201d says Weston. That means you have to use a fingerprint or your face to set up Recall before being able to use the PIN support. This is all designed to prevent malware from accessing Recall data in the background, as Microsoft requires a proof of presence through Windows Hello. \u201cWe\u2019ve moved all of the screenshot processing, all of the sensitive processes into a virtualization-based security enclave, so we actually put it all in a virtual machine,\u201d explains Weston. That means there\u2019s a UI app layer that has no access to raw screenshots or the Recall database, but when a Windows user wants to interact with Recall and search, it will generate the Windows Hello prompt, query the virtual machine, and return the data into the app\u2019s memory. Once the user closes the Recall app, what\u2019s in memory is destroyed.  \u201cThe app outside the virtualization-based enclave is running in an anti-malware protected process, which would basically require a malicious kernel driver to even access,\u201d says Weston. Microsoft is detailing its Recall security model and exactly how its VBS enclave works in a blog post today. It all looks a lot more secure than what Microsoft had planned to ship and even hints at how the company might secure Windows apps in the future. So, how did Microsoft nearly ship Recall in June without a high amount of security in the first place? I\u2019m still not super clear on that, and Microsoft isn\u2019t giving much away. Weston confirms that Recall was reviewed as part of the company\u2019s Secure Future Initiative that was introduced last year, but being a preview product, it apparently had some different restrictions. \u201cThe plan was always to follow Microsoft basics, like encryption. But we also heard from people who were like \u2018we\u2019re really concerned about this,\u2019\u201d so the company decided to fast-track some of the additional security work it was planning for Recall so that security concerns weren\u2019t a factor in whether someone wanted to use the feature.  \u201cIt\u2019s not just about Recall, in my opinion we now have one of the strongest platforms for doing sensitive data processing on the edge and you can imagine there are lots of other things we can do with that,\u201d hints Weston. \u201cI think it made a lot of sense to pull forward some of the investments we were going to make and then make Recall the premier platform for that.\u201d Recall will also now only operate on a Copilot Plus PC, stopping people from sideloading it onto Windows machines like we saw ahead of its planned debut in June. Recall will verify that a Copilot Plus PC has BitLocker, virtualization-based security enabled, measure boot and system guard secure launch protections, and kernel DMA protection. Microsoft has also conducted a number of reviews on the upgraded Recall security. The Microsoft Offensive Research Security Engineering (MORSE) team has \u201cconducted months of design reviews and penetration testing on Recall,\u201d and a third-party security vendor \u201cwas engaged to perform an independent security design review\u201d and testing, too. Now that Microsoft has had more time to work on Recall, there are some additional changes to the settings to provide even more control over how the AI-powered tool works. You\u2019ll now be able to filter out specific apps from Recall alongside the ability to block a custom list of websites from appearing in the database. Sensitive content filtering, which allows Recall to filter out things like passwords and credit cards, will also block health and financial data from being stored. Microsoft is also adding the ability to delete a time range, all content from an app or website, in addition to everything stored in Recall\u2019s database. Microsoft says it remains on track to preview Recall with Windows Insiders\u00a0on Copilot Plus PCs in October, meaning Recall won\u2019t be shipping on these new laptops and PCs until it has been further tested by the Windows community. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/9/27/24255557/meta-orion-quest-smart-glasses-ar-connect-vergecast", "category": "Vergecast", "date": "Sep 27", "author": "David Pierce", "title": "Meta\u2019s new smart glasses look like the future", "content": "You can\u2019t buy Meta\u2019s most impressive new product, the smart glasses codenamed Orion. You might be able to buy something sort of like them a few years from now, but most of us will never get to so much as wear them. That doesn\u2019t necessarily make them less impressive, though, or less important. Orion is a statement of purpose from Meta: that AR glasses really are the future and that we\u2019re eventually going to get there. On this episode of The Vergecast, The Verge\u2019s Alex Heath joins the show to tell us all about his experience with Orion \u2014 two hours in the glasses of the future, playing Pong with Meta CEO Mark Zuckerberg and making smoothies and doing all sorts of other things. He also tells us about his conversation with Zuckerberg (subscribe to Decoder!) about AR, AI, and the future of just about everything. The occasion for all this news was Meta Connect, so we also go through all the other announcements from Connect. There are new smart glasses, new VR headsets, new celebrity AIs to replace the old celebrity AIs, voice modes, and more. But let\u2019s be honest: we mostly talk about Orion. It\u2019s either vaporware, a science project, a prototype, the unquestionable future, or somewhere in the center of the Venn diagram of all those things. And the fact that they exist at all says a lot about where we are in the evolution of technology. Once we finally get done with Meta, we touch on all the executive and corporate changes going on at OpenAI. Then it\u2019s time for a lightning round in which we talk about that new Jony Ive profile, pixel-peep the PS5 Pro, praise Google\u2019s remarkable recent gadget run, and wonder exactly how many people are still using their Rabbit R1. It\u2019s a lot of AI gadgets today. If you want to know more about everything we discuss in this episode, here are some links to get you started, beginning with Orion and Meta\u2019s other new gadgets: Meta Connect 2024: the biggest news and announcementsHands-on with Orion, Meta\u2019s first pair of AR glassesWhy Mark Zuckerberg thinks AR glasses will replace your phoneMeta\u2019s Ray-Bans will now \u2018remember\u2019 things for youMeta\u2019s cheaper Quest 3S might just be an upgrade And in other Meta Connect news: Meta\u2019s going to put AI-generated images in your Facebook and Instagram feedsMark Zuckerberg: creators and publishers \u2018overestimate the value\u2019 of their work for training AIMeta\u2019s AI can now talk to you in the voices of Awkwafina, John Cena, and Judi DenchKristen Bell told Instagram to \u2018get rid of AI\u2019 before she became its official voice And on OpenAI: OpenAI CTO Mira Murati is leavingOpenAI\u2019s for-profit switch could include equity for Sam Altman And in the lightning round: Alex Heath\u2019s pick: Jony Ive confirms he\u2019s working on a new device with OpenAIAlex Cranz\u2019s pick: I played the PS5 Pro, and it\u2019s clearly betterDavid Pierce\u2019s pick: Google Pixel Buds Pro 2 review: big upgrade, much smaller earbudsNilay Patel\u2019s pick: Just 5,000 people use the Rabbit R1 every day " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/9/27/24255722/google-gemini-gmail-contextual-smart-replies-availability", "category": "Google", "date": "Sep 27", "author": "Jess Weatherbed", "title": "Gemini is making Gmail\u2019s smart replies smarter", "content": "Google is rolling out a Gemini-powered update to Gmail for Android and iOS that will tailor smart replies more specifically to emails. First announced back in May, Google says its new contextual Smart Replies will \u201coffer more detailed responses to fully capture the intent of your message\u201d by taking the entire content of the email thread into consideration. Users can hover over each of the suggested contextual smart replies to preview the text, and select the option that best matches their needs or writing style. Suggested replies can be edited or sent immediately. The idea is that this will both save time (especially if you\u2019re often buried in your Gmail inbox) and improve the variety of automated responses available beyond a simple \u201cYes, I\u2019m working on it\u201d or \u201cNo worries, thanks for the heads up!\u201d \u2014 even adding an initial greeting and a signoff message. The new contextual Smart Replies are now rolling out for Gemini Business, Enterprise, Education, Education Premium, and Google One AI Premium subscribers. The feature is currently only available in English and builds on the original Smart Replies added to Gmail in 2017. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/9/26/24255176/google-notebooklm-summarize-youtube-videos-ai", "category": "Google", "date": "Sep 26", "author": "Emma Roth", "title": "Google\u2019s NotebookLM can help you dive deeper into YouTube videos", "content": "NotebookLM, Google\u2019s AI note-taking app, can now summarize and help you dig deeper into YouTube videos. The new capability works by analyzing the text in a YouTube video\u2019s transcript, including autogenerated ones. Once you add a YouTube link to NotebookLM, it will use AI to provide a brief summary of key topics discussed in the transcript. You can then click on these topics to get more detailed information as well as ask questions. (If you\u2019re struggling to come up with something to ask, NotebookLM will suggest some questions.) When I dropped in The Verge\u2019s iPhone 16 Pro review, NotebookLM generated topics discussed in the video like \u201cApple Intelligence,\u201d \u201ciPhone 16 Pro Camera,\u201d and \u201cPhoto processing\u201d in its chat window.  After clicking on some of the topics, I found that NotebookLM backs up the information provided in its chat window with a citation that links you directly to the point in the transcript where it\u2019s mentioned. You can also create an Audio Overview based on the content, which is a podcast-style discussion hosted by AI.  I found that the feature worked on most of the videos I tried, except for ones published within the past two days or so. While Google\u2019s Gemini and Microsoft\u2019s Copilot can both summarize YouTube videos with transcripts, NotebookLM gives you some jumping-off points you can use to learn more about a topic. In addition to adding support for YouTube videos, Google announced that NotebookLM now supports audio recordings as well, allowing you to search transcribed conversations for certain information and create study guides. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/1", "link": "https://www.theverge.com/2024/9/26/24255179/deepfake-call-ukraine-senator-cardin-dmytro-kuleba", "category": "Tech", "date": "Sep 26", "author": "Gaby Del Valle", "title": "A deepfake caller pretending to be a Ukrainian official almost tricked a US senator", "content": "The head of the Senate Foreign Relations Committee took a Zoom call with someone using deepfake technology to pose as a top Ukrainian official, The New York Times reports.\u00a0 Sen. Ben Cardin (D-MD) received an email last Thursday that appeared to be from Dmytro Kuleba, Ukraine\u2019s former foreign minister, asking him to meet on Zoom. The person on the other end of the call reportedly looked and sounded like Kuleba but was acting strangely. He asked Cardin \u201cpolitically charged questions in relation to the upcoming election,\u201d according to an email Senate security officials sent legislators that was obtained by the Times. The fake Kuleba demanded that Cardin give his opinion on foreign policy questions, including whether he supported firing long-range missiles into Russian territory.\u00a0 The tenor of the conversation made Cardin suspicious, according to the Times report, and he reported it to the State Department. Officials there confirmed that Cardin hadn\u2019t spoken to the real Kuleba but to an imposter, though it\u2019s still unclear who was behind the call. In a statement to the Times, Cardin said that \u201cin recent days, a malign actor engaged in a deceptive attempt to have a conversation with me by posing as a known individual.\u201d Cardin\u2019s statement didn\u2019t disclose who the \u201cknown individual\u201d was, but the Senate security email did. Senate security officials told lawmakers to be on the lookout for similar attempts and warned \u201cit is likely that other attempts will be made in the coming weeks.\u201d \u201cWhile we have seen an increase of social engineering threats in the last several months and years, this attempt stands out due to its technical sophistication and believability,\u201d the Senate security office email obtained by the Times read. As AI tools become easier and cheaper to use, politically motivated deepfakes have increased in frequency \u2014 and effectiveness. In May, the Federal Communications Commission proposed levying multimillion-dollar fines on a political consultant behind a robocall campaign that impersonated President Joe Biden. On the call \u2014 which targeted New Hampshire voters ahead of the state\u2019s primary election \u2014 the fake Biden told voters not to show up to the polls. Elon Musk shared a deepfake video of Vice President Kamala Harris on X in which Harris appeared to call herself \u201cthe ultimate diversity hire\u201d who \u201chad four years under the tutelage of the ultimate deep state puppet, a wonderful mentor, Joe Biden.\u201d And former President Donald Trump posted an AI-generated \u201cendorsement\u201d from Taylor Swift on Truth Social in August \u2014 which Swift later cited in her real endorsement of Harris. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/25/24254042/mark-zuckerberg-creators-value-ai-meta", "category": "Tech", "date": "Sep 25", "author": "Adi Robertson", "title": "Mark Zuckerberg: creators and publishers \u2018overestimate the value\u2019 of their work for training AI", "content": "Meta CEO Mark Zuckerberg says there are complex copyright questions around scraping data to train AI models, but he suggests the individual work of most creators isn\u2019t valuable enough for it to matter. In an interview with The Verge deputy editor Alex Heath, Zuckerberg said Meta will likely strike \u201ccertain partnerships\u201d for useful content. But if others demand payment, then \u2014 as it\u2019s done with news outlets \u2014 the company would prefer to walk away. \u201cI think individual creators or publishers tend to overestimate the value of their specific content in the grand scheme of this,\u201d Zuckerberg said in the interview, which coincides with Meta\u2019s annual Connect event. \u201cMy guess is that there are going to be certain partnerships that get made when content is really important and valuable.\u201d But if creators are concerned or object, \u201cwhen push comes to shove, if they demanded that we don\u2019t use their content, then we just wouldn\u2019t use their content. It\u2019s not like that\u2019s going to change the outcome of this stuff that much.\u201d Meta, like nearly every major AI company, is currently embroiled in litigation over the limits of scraping data for AI training without permission. Last year, the company was sued by a group of authors, including Sarah Silverman, who claimed its Llama model was unlawfully trained on pirated copies of their work. (The case currently isn\u2019t going great for those authors; last week, a judge castigated their legal team for being \u201ceither unwilling or unable to litigate properly.\u201d) The company \u2014\u00a0again, like nearly every major AI player \u2014 argues that this kind of unapproved scraping should be allowed under US fair use law. Zuckerberg elaborates on the question: I think that in any new medium in technology, there are the concepts around fair use and where the boundary is between what you have control over. When you put something out in the world, to what degree do you still get to control it and own it and license it? I think that all these things are basically going to need to get relitigated and rediscussed in the AI era. The history of copyright is indeed a history of deciding what control people have over their own published works. Fair use is designed to let people transform and build on each other\u2019s creations without permission or compensation, and that\u2019s very frequently a good thing. That said, some AI developers have interpreted it far more broadly than most courts. Microsoft\u2019s AI CEO, for instance, said earlier this year that anything \u201con the open web\u201d was \u201cfreeware\u201d and \u201canyone can copy it, recreate with it, reproduce with it.\u201d (This is categorically legally false: content posted publicly online is no less protected by copyright than any other medium, and to the extent you can copy or modify it under fair use, you can also copy or modify a book, movie, or paywalled article.) While the issue is still being debated in lawsuits, a number of AI companies have begun paid partnerships with major outlets. OpenAI, for instance, has struck deals with several news publishers and other companies like Shutterstock. Meta recently signed an agreement with Universal Music Group that included provisions around AI-generated songs. Meanwhile, some artists have turned to unofficial tools that would prevent their work from being used for AI training. But especially for anything posted on social media before the rise of generative AI, they\u2019re sometimes stymied by terms of service that let these companies train on their work. Meta has stated that it trains its AI tools on public Instagram and Facebook posts. Zuckerberg said Meta\u2019s future AI content strategy would likely echo its blunt response to proposed laws that would add a fee for links to news stories. The company has typically responded to these rules by blocking news outlets in countries like Australia and Canada. \u201cLook, we\u2019re a big company,\u201d he said. \u201cWe pay for content when it\u2019s valuable to people. We\u2019re just not going to pay for content when it\u2019s not valuable to people. I think that you\u2019ll probably see a similar dynamic with AI.\u201d We\u2019ve known for some time that news isn\u2019t particularly valuable to Meta, in part because moderation of it invites controversy and (according to Meta) it makes users feel bad. (\u201cIf we were actually just following what our community wants, we\u2019d show even less than we\u2019re showing,\u201d Zuckerberg said in the interview.) The company\u2019s generative AI products are still nascent, and it\u2019s not clear anyone has figured out what people want from these tools. But whatever it is, most creators probably shouldn\u2019t expect that it will get them paid. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/25/24254190/meta-connect-2024-ai-auto-dubbing-lip-syncing", "category": "Tech", "date": "Sep 25", "author": "Jay Peters", "title": "Meta is working on recreating influencers with AI", "content": "Meta has big ambitions for using AI to help creators, and it showed two impressive demos of what that could look like onstage at Connect today. One version of this involves fully recreating real influencers as AI figures. Meta CEO Mark Zuckerberg presented a live demo of a creator-based AI persona, which looked like the creator, talked like the creator, and tried to respond to questions like the creator would. It was pretty wild to watch.  Another tool it\u2019s developing takes Reels and automatically dubs them into another language, maintaining the creator\u2019s voice and even changing the movements of their mouth to match.  Zuckerberg presented two videos onstage of Reels by Spanish creators being translated and dubbed into English. The demo wasn\u2019t live, so it\u2019s unclear how well this tech works right now on a typical video, but it was still incredibly impressive to watch. At scale, it seems like this technology has the potential to make videos reach a lot more people, which is something Meta definitely wants to do so it can better compete with platforms like TikTok and YouTube. (Speaking of YouTube, the company has been investing in auto-dubbing, too.) Meta also announced a bunch of other AI-related features at Connect today, including that its AI assistant will be able to talk in the voices of celebrities.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/24254101/meta-connect-2024-announcements-products", "category": "Tech", "date": "Sep 25", "author": "Umar Shakir", "title": "The biggest news from Meta Connect 2024", "content": "Meta has a bunch of new hardware and AI news coming out of its Meta Connect event today, including a new Quest 3S VR headset, an expansion of Meta AI features, a new Llama model, and a first look at the new Orion augmented reality glasses. CEO Mark Zuckerberg took the stage on Wednesday with a new style and demonstrated new features including live translation between English and Spanish. Here\u2019s everything announced at Meta Connect: Orion AR glasses Meta has revealed its Orion augmented reality glasses, and they look almost like a trendy pair of frames you could pick up without all the tech inside. Orion uses Micro LED projectors inside the frame and beams images in front of your eyes via waveguides in the lenses. Orion has the same kinds of generative AI capabilities as the current Ray-Ban smart glasses \u2014 but adds a visual element to make it more helpful, like adding labels on top of ingredients you\u2019re looking at on a table. The glasses pair with a wireless compute puck and a \u201cneural wristband\u201d you wear on your arm that responds to gestures like punches. Orion isn\u2019t quite ready as a product, though, so it isn\u2019t going on sale.  The cheaper Quest 3S VR headset Meta\u2019s latest VR headset is the Quest 3S, and it\u2019s launching at the Quest 2\u2019s original starting price of $299.99. The new headset has many of the same features as the more expensive Quest 3, including the same Snapdragon XR2 Gen 2 chip, and uses the same Touch Plus controllers. The lower price point is possible thanks to the Quest 3S\u2019s lack of depth sensor, lower resolution screens, narrower field of view, and less compact package than the 3. With the 3S now in place, Meta is simplifying its VR lineup by discontinuing the Quest 2 and Quest Pro. Meta\u2019s Ray-Ban glasses add new features Meta\u2019s very cool Ray-Ban smart glasses are getting new software updates that add a Reminders feature that can help you keep track of your day like remembering your grocery list. There are also improvements to Meta AI responsiveness, and the company is working on real-time speech translation. On the hardware front, Meta is launching new styles like a transparent frame and a new range of transition lenses. AI images in your Facebook and Instagram feeds \u2014 featuring you Soon, your Facebook and Instagram feeds will include Meta AI-generated content \u201cbased on your interests or current trends\u201d in a section called \u201cImagined for you.\u201d It can incorporate your face into made-up scenarios like \u201can enchanted realm,\u201d and you can do things like imagine yourself as a video game character or astronaut. The feature is a test for now. Meta AI in celebrity voices Meta is continuing its venture to make celeb chatbots after shutting down its alternate persona feature on Instagram, which had weird options like Tom Brady playing as \u201cBru.\u201d Now, Meta is connecting new celebrities with AI versions of themselves on Meta AI, including Awkwafina, John Cena, Keegan-Michael Key, Kristen Bell, and Dame Judi Dench. You can get access to these Meta AI voices on Instagram, WhatsApp, and Facebook in the US. New Llama model with image processing Meta has a new Llama model with one key update: the ability to process visuals. This is something competitors have had for the past year, so it\u2019s an important step forward for Meta\u2019s model. The new version also comes in smaller packages designed for mobile. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/25/24253712/meta-rayban-ai-features-reminders-translation-transparent-style", "category": "Meta", "date": "Sep 25", "author": "Kylie Robison", "title": "Meta\u2019s Ray-Bans will now \u2018remember\u2019 things for you", "content": "Meta\u2019s Ray-Ban smart glasses are already one of the best cracks at AI hardware to date. Now, Meta is pushing out a series of software updates, along with a new limited-edition translucent Ray-Ban style, that bring the smart glasses closer to actually feeling smart. The company announced several updates to the Ray-Ban Meta glasses at its Connect conference on Wednesday, introducing new features like \u201cReminders,\u201d which has the glasses take a photo of what you\u2019re looking at and remind you about it later through a notification on your phone. You\u2019ll also be able to scan QR codes and call phone numbers you\u2019re looking at directly from the glasses.  The Meta Ray-Bans can already translate a handful of languages from still images. Now, the company says it\u2019s working on a real-time language translation feature that will play back what you\u2019re hearing in real time via speakers in the glasses. When this feature is made available in the coming months, it will be able to translate between English, French, Italian, and Spanish. I got to test out the new updates firsthand, and the improvements in AI were clear. Back in July, when I first tried the Ray-Ban Metas, the AI often struggled \u2014 it couldn\u2019t even set a simple timer or reliably identify objects. This time, however, the glasses accurately recognized everything I asked and were also far more responsive, handling follow-up questions with ease. However, there\u2019s still no timer, which feels like a big miss to me but Meta spokesperson Elana Widmann says it\u2019s \u201ccoming soon.\u201d A Meta product lead in the demo booth said that the goal of this update was to make conversations with the glasses feel more natural, and to his point, I did find this version of the AI more useful. The company also plans to add the ability for the Ray-Bans\u2019 AI to support real-time video processing so it can immediately understand what\u2019s around you \u2014\u00a0that feature isn\u2019t coming until later this year, though. PreviousNext1/3 Photo by Vjeran Pavic / The VergePreviousNext1/3 Photo by Vjeran Pavic / The Verge Alongside the software updates, Meta is also releasing a new range of\u00a0transition\u00a0lenses with Ray-Ban\u2019s parent company, EssilorLuxottica. There are also new limited-edition clear frames that reveal all the tech inside, reminiscent of an old-school Game Boy Color. At the demo booth, I got to see Li-Chen Miller, head of wearables at Meta, sporting a pair. The clear frames are on sale today, but limited to 7,500 units. AI-powered devices have had a particularly rough year. Humane\u2019s AI-powered Pin struggled with poor sales, and the Rabbit R1 faced terrible reviews. While Meta hasn\u2019t released sales figures, CEO Mark Zuckerberg told investors the smart glasses have exceeded expectations, leading EssilorLuxottica to ramp up production to meet high demand. According to estimates from IDC, Meta has shipped more than 700,000 pairs, with orders more than doubling from the first to the second quarter of this year. It seems like Zuckerberg has high hopes for smart glasses. \u201cI think Meta AI is becoming a more and more prominent feature of the glasses, and there\u2019s more stuff that you can do,\u201d he said in an interview with The Verge\u2019s Alex Heath this week. \u201cIt\u2019s not like we\u2019re going to throw away our phones, but I think what\u2019s going to happen is that, slowly, we\u2019re just going to start doing more things with our glasses and leaving our phones in our pockets more.\u201d Whether these glasses will truly change how we interact with AI remains to be seen, but Meta\u2019s latest updates show a clear path toward making AI assistants on your face a practical reality. Update, 4:11PM ET: Added that the clear frames are limited to 7,500 units. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/25/24253420/meta-ai-celebrity-voices-awkwafina-john-cena-judi-dench-connect", "category": "Tech", "date": "Sep 25", "author": "Wes Davis", "title": "Meta\u2019s AI can now talk to you in the voices of Awkwafina, John Cena, and Judi Dench", "content": "Meta is adding conversational voices by celebrities to its AI chatbot in Instagram, WhatsApp, and Facebook.  The company announced at its Connect event today that you can now talk to Meta AI and hear it respond in one of several voices, including celebrity soundalikes such as Awkwafina, John Cena, Keegan-Michael Key, Kristen Bell, and the only one I truly care about: Dame Judi Dench. These celebrity voices will only be available to US users of Meta\u2019s apps to start. And if you prefer a voice that is a little more mundane, you can also pick from non-celeb voices with names like \u201cAspen,\u201d \u201cAtlas,\u201d or \u201cClover.\u201d Google and OpenAI also now offer similar conversational experiences that ostensibly aren\u2019t based on celebrity voices.  If your first thought here was \u201cGee, that sounds a lot like that Scarlett Johansson thing that OpenAI did,\u201d you\u2019re right\u00a0\u2014 expect that, rather than debuting an AI voice that just coincidentally sounds like one of the world\u2019s highest-paid actors, Meta is explicitly announcing celebrity partnerships, which likely involve payment or some other deal.  Meta hasn\u2019t shared those details, but the company has paid each celebrity \u201cmillions of dollars\u201d for their voices, according to The Wall Street Journal. And in negotiations, some of the people reportedly wanted to limit what their voices could say and to make sure they weren\u2019t liable if Meta AI was used. Meanwhile, Meta recently shut down its chatbots that emulate celebrities like Tom Brady on Instagram, while Amazon used celebrity voices like Samuel L. Jackson or Shaq for Alexa before shutting them down last year, too.  Meta\u2019s AI updates aren\u2019t just about voice conversations. Its chatbot will also now \u201canswer questions about your photos\u201d when you upload images. Send a picture of a cake, ask how to make it, and it\u2019ll grab you a recipe that hopefully does just that.  And if you want something \u201cadded, changed, or removed\u201d from an image, Meta says you can describe anything from \u201cchanging your outfit to replacing the background with a rainbow,\u201d and it\u2019ll carry out that request. Before: a black shirt. Photo by Vjeran Pavic / The VergeAfter: a red shirt! And... a messed-up version of the shirt\u2019s text and logo. Photo by Vjeran Pavic / The Verge In the above examples from our hands-on testing, Meta AI changed the color of a T-shirt. It\u2019s not clear what, if any, guardrails there are on this experience. With Google AI, we have plenty of examples of what this sort of tech can do, for better or worse, and we\u2019ll see sometime in the next year how Apple manages it, too.   " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/25/24253329/meta-ai-generated-images-facebook-instagram-feeds", "category": "Meta", "date": "Sep 25", "author": "Emma Roth", "title": "Meta\u2019s going to put AI-generated images in your Facebook and Instagram feeds", "content": "If you think avoiding AI-generated images is difficult as it is, Facebook and Instagram are now going to put them directly into your feeds. At the Meta Connect event on Wednesday, the company announced that it\u2019s testing a new feature that creates AI-generated content for you \u201cbased on your interests or current trends\u201d \u2014 including some that incorporate your face. When you come across an \u201cImagined for You\u201d image in your feed, you\u2019ll see options to share the image or generate a new picture in real time. One example (embedded below) shows several AI-generated images of \u201can enchanted realm, where magic fills the air.\u201d But others could contain your face... which I\u2019d imagine will be a bit creepy to stumble upon as you scroll. The examples at the very top of this article include captions that say you can \u201cimagine yourself\u201d as a video game character or an astronaut exploring space. Both images appear to use a person\u2019s photos to create an AI-generated version of them in made-up scenarios. In a statement to The Verge, Meta spokesperson Amanda Felix says the platform will only generate AI images of your face if you \u201conboarded to Meta\u2019s Imagine yourself feature, which includes adding photos to that feature\u201d and accepting its terms. You\u2019ll be able to remove AI images from your feed as well. Last week, 404 Media found that using Snapchat\u2019s AI selfie feature gives the company permission to use your face in ads seen only by you (unless you disable the option). It looks like Facebook and Instagram will similarly only show the AI-generated content to you, while sharing remains optional. During an interview with The Verge\u2019s Alex Heath, Meta CEO Mark Zuckerberg said that adding AI images to your feeds is the next \u201clogical jump\u201d for Facebook and Instagram. \u201cI think there\u2019s been this trend over time where the feeds started off as primarily and exclusively content for people you followed, your friends,\u201d Zuckerberg said. \u201cAnd you just add on to that, a layer of, \u2018Okay, and we\u2019re also going to show you content that\u2019s generated by an AI system that might be something that you\u2019re interested in\u2019 ... how big it gets is kind of dependent on the execution and how good it is.\u201d  Meta says the feature is just a test for now, so it\u2019s unclear how widely or quickly it\u2019s going to roll out. But tossing AI-generated images into our feeds sounds like it will probably take things even further from the space for keeping up with friends that these platforms used to be. Update, September 25th: Added a statement from Meta. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/25/24253774/meta-ai-vision-model-llama-3-2-announced", "category": "Meta", "date": "Sep 25", "author": "Kylie Robison", "title": "Meta releases its first open AI model that can process images", "content": "Just two months after releasing its last big AI model, Meta is back with a major update: its first open-source model capable of processing both images and text. The new model, Llama 3.2, could allow developers to create more advanced AI applications, like augmented reality apps that provide real-time understanding of video, visual search engines that sort images based on content, or document analysis that summarizes long chunks of text for you. Meta says it\u2019s going to be easy for developers to get the new model up and running. Developers will have to do little except add this \u201cnew multimodality and be able to show Llama images and have it communicate,\u201d Ahmad Al-Dahle, vice president of generative AI at Meta, told The Verge. Other AI developers, including OpenAI and Google, already launched multimodal models last year, so Meta is playing catch-up here. The addition of vision support will also play a key role as Meta continues to build out AI capabilities on hardware like its Ray-Ban Meta glasses. Llama 3.2 includes two vision models (with 11 billion parameters and 90 billion parameters) and two lightweight text-only models (with 1 billion parameters and 3 billion parameters). The smaller models are designed to work on Qualcomm, MediaTek, and other Arm hardware, with Meta clearly hoping to see them put to use on mobile. There\u2019s still a place for the (slightly) older Llama 3.1, though: that model, released in July, included a version with 405 billion parameters, which will theoretically be more capable when it comes to generating text. Alex Heath contributed reporting. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/24/24253452/microsoft-correction-ai-safety-tool-fix-errors", "category": "Microsoft", "date": "Sep 24", "author": "Emma Roth", "title": "Microsoft claims its AI safety tool not only finds errors but also fixes them", "content": "Microsoft is launching a new feature called \u201ccorrection\u201d that builds on the company\u2019s efforts to combat AI inaccuracies. Customers using Microsoft Azure to power their AI systems can now use the capability to automatically detect and rewrite incorrect content in AI outputs. The correction feature is available in preview as part of the Azure AI Studio \u2014 a suite of safety tools designed to detect vulnerabilities, find \u201challucinations,\u201d and block malicious prompts. Once enabled, the correction system will scan and identify inaccuracies in AI output by comparing it with a customer\u2019s source material. From there, it will highlight the mistake, provide information about why it\u2019s incorrect, and rewrite the content in question \u2014 all \u201cbefore the user is able to see\u201d the inaccuracy. While this seems like a helpful way to address the nonsense often espoused by AI models, it might not be a fully reliable solution. Vertex AI, Google\u2019s cloud platform for companies developing AI systems, has a feature that \u201cgrounds\u201d AI models by checking outputs against Google Search, a company\u2019s own data, and (soon) third-party datasets. In a statement to TechCrunch, a Microsoft spokesperson said the \u201ccorrection\u201d system uses \u201csmall language models and large language models to align outputs with grounding documents,\u201d which means it isn\u2019t immune to making errors, either. \u201cIt is important to note that groundedness detection does not solve for \u2018accuracy,\u2019 but helps to align generative AI outputs with grounding documents,\u201d Microsoft told TechCrunch.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/24/24253272/james-cameron-stability-ai", "category": "Tech", "date": "Sep 24", "author": "Charles Pulliam-Moore", "title": "James Cameron is joining Stability AI\u2019s board of directors", "content": "Though director James Cameron recently expressed concern about the potential of artificial intelligence to leave people without a sense of purpose, the longtime early adopter has now joined Stability AI\u2019s board of directors. Today, Stability AI \u2014 the company behind Stable Diffusion \u2014 announced that Cameron has signed a deal to become the newest member of its executive leadership team. In a statement, CEO Prem Akkaraju described Cameron\u2019s coming on as\u00a0a \u201cmonumental statement\u201d for the AI industry as a whole and described the director as someone who \u201clives in the future and waits for the rest of us to catch up.\u201d Cameron cited his long history of experimenting with emerging technologies as part of what inspired him to join Stability AI\u2019s ranks and pointed to \u201cthe intersection of generative AI and CGI image creation\u201d as the next big breakthrough set to revolutionize the filmmaking industry.\u00a0 It\u2019s not currently clear how Cameron\u2019s arrival at Stability AI might influence the company, but the news comes at a time when studios have begun to show interest in working with AI firms more closely. Last week, Lionsgate announced a partnership with Runway to develop a generative AI model trained on its catalog of films and TV series, and earlier this year, Sony Pictures Entertainment CEO Tony Vinciquerra said that the company intends to deploy the technology as a massive cost-saving measure. Though AI companies and the studios working with them have been insistent that their collaborations will ultimately be a boon to the industry, the shift represented by these kinds of deals was part of what drove Hollywood\u2019s actors and writers to go on strike last year. Cameron himself has recently caught heat for using AI to remaster some of his classic films like The Abyss, True Lies, and Aliens. Stability is also currently dealing with multiple lawsuits for allegedly training its models on copyrighted material, which might complicate the company\u2019s plans depending on how things shake out. But for all of the uncertainty that\u2019s defining the AI space, Cameron sounds especially keen on throwing himself into it while it\u2019s buzzing. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/24/24253252/figma-ai-make-designs-first-draft-app-ui-generator", "category": "Design", "date": "Sep 24", "author": "Jay Peters", "title": "Figma\u2019s AI-powered app generator is back after it was pulled for copying Apple", "content": "Figma announced Make Designs in June as a way for designers to use generative AI to help start app designs, but early users quickly showed how it could mock up a weather app that looked remarkably close to Apple\u2019s iPhone weather app. Although Figma insisted that the feature wasn\u2019t trained on customer data \u2014 in fact, Figma said it didn\u2019t train the off-the-shelf generative AI models used \u2014 the company removed the feature to give it more testing. Now it\u2019s coming back, called \u201cFirst Draft,\u201d and like Figma\u2019s other AI features, it\u2019s available currently in a limited beta. Figma is also adding features to First Draft as part of the relaunch: We\u2019re also introducing some key updates, like letting you choose from one of four libraries depending on your needs\u2014whether it\u2019s a wireframing library to help you sketch out less opinionated, lo-fi primitives, or higher-fidelity libraries to provide more visual expressions or patterns to explore. This offers a looser, more exploratory counterpoint to the utility of our\u00a0Visual Search feature, which allows you to search your Figma files via prompt or image to find existing files or components. In its blog post about the relaunch, the company also spelled out how First Draft works: First Draft doesn\u2019t train on customer content. It uses off-the-shelf AI models (like OpenAI\u2019s GPT-4 and Amazon Titan) with three key elements: model, context, and prompt. The context includes proprietary mobile and desktop design systems with numerous components and assembly examples. Users input their design goals as the prompt. The AI then selects, arranges, and customizes design system components based on these inputs, creating a starting point for designs. Figma says it\u2019s relaunched the feature after \u201cextensive analysis, iteration, and testing.\u201d Figma\u2019s other AI features also include the ability to auto-generate text for your designs. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/24/24253107/warner-bros-discovery-caption-ai-google-cloud-closed-captioning", "category": "Google", "date": "Sep 24", "author": "Umar Shakir", "title": "Max is getting Google AI-generated closed captions", "content": "Warner Bros. Discovery (WBD) is partnering with Google Cloud to generate closed captioning for content on the studio\u2019s Max streaming service in the US. WBD will use a new \u201ccaption AI\u201d solution that runs on Google Cloud\u2019s Vertex AI platform to generate speech-to-text for some shows, starting with \u201cunscripted programming.\u201d Captions generated for the Max shows will have human oversight for quality assurance. According to Google, the workflow will reduce caption file creation time by up to 80 percent and cut costs by up to 50 percent compared to manual methods. WBD will continuously refine and train caption AI to make fewer mistakes. Notably, most content on Max already has closed captioning tracks, and WBD hasn\u2019t said how extensively it will use caption AI. Automatic speech-to-text services for video captioning have been available for years, but with a large studio like WBD now testing the latest AI for the job, we might see a future where the tech is the norm for this task. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/24/24252979/google-maps-updated-street-view-images", "category": "Google", "date": "Sep 24", "author": "Emma Roth", "title": "Google is updating Street View images across dozens of countries", "content": "Google is updating Street View imagery in nearly 80 countries, such as Australia, Brazil, Denmark, Japan, the Philippines, Rwanda, Serbia, South Africa, and more. It\u2019s also bringing Street View to a handful of countries where it\u2019s never been available, Bosnia and Herzegovina, Namibia, Liechtenstein, and Paraguay.  Google says its more portable Street View camera, which launched in 2022, will help offer images of \u201ceven more places in the future.\u201d  Meanwhile, Google Maps and Google Earth are getting sharper satellite imagery as well thanks to the company\u2019s cloud-removal AI tool that takes out clouds, shadows, haze, and mist. This should result in \u201cbrighter, more vibrant\u201d images, according to Google. Additionally, you\u2019ll soon be able to view historical imagery on Google Earth\u2019s web and mobile apps \u2014 a feature that was previously only available through the Google Earth Pro desktop app. This should make it easier to compare satellite and aerial images of a location throughout the years. As someone who loves checking out new places in Street View, I\u2019m excited to explore the streets and landscapes of Bosnia and Namibia whenever Google decides to roll out this update. The Verge reached out to Google to see why it removed its blog post but didn\u2019t immediately hear back. Update, September 24th: Added Google\u2019s official announcement. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/24/24252723/openai-o1-ai-god-tiktok-google-adtech-vergecast", "category": "Vergecast", "date": "Sep 24", "author": "David Pierce", "title": "They think they\u2019re building God", "content": "OpenAI\u2019s new model, called o1, appears to think and ponder as you use it. But is it thinking? Or pondering? And what does it mean if it is? Would that make it worth the risks, which appear to be both greater and more plausible than ever? How do you balance the risks of destroying humanity with the possibility of improving it? This is the thing about talking about artificial intelligence: it has this nasty penchant of getting all existential on you. On this episode of The Vergecast, we get all existential about AI. The Verge\u2019s Kylie Robison joins the show to discuss why OpenAI built o1, why it\u2019s launching the way it is, what to make of the folks who are worried about what they\u2019re seeing from the model, and how we should think about this moment in AI as companies pivot toward trying to build \u201cagents\u201d that can do more and more on our behalf. (We recorded this just before Sam Altman published his recent blog post on The Intelligence Age, but it all feels pretty timely.) After that, The Verge\u2019s Gaby Del Valle and Adi Robertson come on to catch up on some legal and political stuff. We talk about the latest on the don\u2019t-call-it-a-ban TikTok ban, which is now being tested in court \u2014 but maybe not in exactly the way you think. We talk about President Trump\u2019s crypto announcement... if you can even call it an announcement. And we check in on the Google adtech trial, which could be another shock to the entire tech ecosystem. Finally, we answer a question on the Vergecast Hotline (call 866-VERGE11, or email vergecast@theverge.com!) about an issue everybody has: what do you do with all the stuff that accumulates on your devices? If you want to know more about everything we discuss in this episode, here are some links to get you started, beginning with OpenAI: OpenAI releases new o1 reasoning modelOpenAI\u2019s new model is better at reasoning and, occasionally, deceivingOpenAI is launching an \u2018independent\u2019 safety board that can stop its model releasesOpenAI rates its new model \u201cmedium\u201d risk.From Sam Altman: The Intelligence Age And on TikTok / Google / Trump: TikTok ban: all the news on attempts to ban the video platformTikTok oral arguments will weigh security risks against free speechTikTok faces a skeptical panel of judges in its existential fight against the US government\u00a0Donald Trump is hawking tokens for a crypto project he still hasn\u2019t explainedUS v. Google redux: all the news from the ad tech trialHow Google got away with charging publishers more than anyone else And a few tools for cleaning up your devices: SwipewipeDisk Inventory XWinDirStat " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/24/24252971/spotify-ai-playlist-builder-beta-available-us-canada", "category": "Spotify", "date": "Sep 24", "author": "Jess Weatherbed", "title": "Spotify\u2019s AI playlist builder is now available in the US", "content": "Spotify is expanding an AI feature that creates customized playlists from your text descriptions to additional English-speaking regions. Starting today, people in the US, Canada, Ireland, and New Zealand with Premium Spotify subscriptions (which start at $5.99 per month for students or $11.99 for individuals) can access AI Playlist in beta, following its initial launch in the UK and Australia earlier this year. Subscribers can locate the feature within the mobile app by tapping the \u201c+\u201d button at the top right of their Spotify library. Selecting \u201cAI Playlist\u201d from the drop-down menu will then open a chat box to describe the playlist you want, such as \u201cspooky songs to play during Halloween\u201d or \u201csoothing folk music for a chill bath time.\u201d The feature will also provide suggested prompts. The AI Playlist beta isn\u2019t currently available on the Spotify desktop or web apps. The generated playlists contain 30 songs and can be adjusted with additional prompts to better match the desired vibe, such as asking for more upbeat music. Descriptions that specify things like genres, decades, moods, or artists will see better results, but in my own testing, I found it was impressively capable of matching songs to even niche descriptions like \u201cmake me feel like a vampire hunter from Blade (1998).\u201d Spotify says it\u2019s still \u201cactively learning and iterating with each exchange\u201d while the feature is in beta and may introduce changes to refine AI Playlist in the future.   " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/24/24252934/google-workspace-gemini-ai-security-assistant", "category": "Google", "date": "Sep 24", "author": "Jess Weatherbed", "title": "Google\u2019s Gemini AI might soon appear in your corporate Workspace", "content": "Google is making Gemini AI a core part of its Workspace productivity suite, which could see the chatbot adopted by millions more users. In its latest blog posts, the search giant announced that the standalone Gemini app is being included as standard on Workspace Business, Enterprise, and Frontline plans starting sometime in Q4, replacing the need to purchase a separate Gemini add-on. Google says that Gemini is subject to the same enterprise terms as other core Workspace services like Gmail and Docs, and won\u2019t use an organization\u2019s data, generated responses, or user prompts to train or otherwise improve its Gemini AI model. Workspace Administrators will also \u201csoon\u201d be able to manage if Gemini stores generated responses and user prompts, and limit how long these will be stored for.\u00a0 Gemini for Workspace now also carries SOC 1/2/3 and ISO 27701 industry security and privacy standards certifications, giving organizations some peace of mind when implementing the chatbot for corporate use. And to bolster security against malware, phishing, and other online threats, Google is introducing a new \u201cSecurity Advisor\u201d tool that \u201cdelivers insights directly to an IT administrator\u2019s inbox.\u201d Security Advisor includes a range of safe browsing and data protection features for Chrome, Gmail, and Google Drive, and will be rolled out to paying Workspace customers \u201cover the next few weeks.\u201d " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/23/24252664/openai-x-account-crypto-scam-newsroom", "category": "OpenAI", "date": "Sep 23", "author": "Jay Peters", "title": "An official OpenAI X account was taken over to peddle a crypto scam", "content": "An official OpenAI account on X was taken over to peddle a cryptocurrency scam on Monday evening. On Monday at 6:24PM ET, the @OpenAINewsroom account, which shares news from OpenAI and has nearly 54,000 followers, made a now-deleted post advertising an \u201c$OPENAI\u201d token. \u201cWe\u2019re very happy to announce $OPEANAI: the token bridging the gap between Al and blockchain technology,\u201d the post said. \u201cAll OpenAI users are eligible to claim a piece of $OPENAI\u2019s initial supply. Holding $OPENAI will grant access to all of our future beta programs.\u201d The post also included a link to a spoofed version of OpenAI\u2019s website at a URL that wasn\u2019t openai.com. When I visited the site, there was a section for claiming the $OPENAI cryptocurrency. When I clicked the button, it asked me to connect a wallet, which I didn\u2019t do. OpenAI and X didn\u2019t immediately reply to a request for comment. As of this writing, the OpenAI Newsroom account hasn\u2019t posted anything to explain what might have happened. The account launched at the beginning of this month. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/24247369/the-browser-company-ceo-josh-miller-arc-google-chrome-ai-search-web-decoder-interview", "category": "Decoder", "date": "Sep 23", "author": "Nilay Patel", "title": "Arc creator Josh Miller on why you need a better browser than Chrome", "content": "Today, I\u2019m talking with Josh Miller, cofounder and CEO of The Browser Company, a relatively new software maker that develops the Arc browser. David Pierce, my Vergecast cohost and Verge editor-at-large, is a big fan of Arc and has written about it quite a bit for us. You can read his review here.\u00a0 Basically, Arc is a ground-up rethinking of the web browser. Most modern browsers started as simple document viewers and grew to support running complex apps. Arc\u2019s main conceit is that it\u2019s designed to make running and using all those apps as simple as possible. You\u2019ll hear Josh describe it as an operating system several times, which is a pretty big claim to make, and he and I got into what that actually means for a web browser.  There are some AI tools built into the Arc browser, but the company also has a mobile app called Arc Search that does AI summaries of webpages. That puts it in competition with OpenAI\u2019s forthcoming SearchGPT and Google\u2019s Gemini-powered AI Overviews in its search results. At the same time, it also puts Arc right in the middle of one of the fiercest debates in tech and media today: whether AI companies and products are boosting content from the open web and then turning around and selling it to consumers \u2014 all without paying the people who produced that work anything at all.  We\u2019ve been talking about these topics pretty much nonstop for the last year here on Decoder. So I was really excited to have Josh on the show to explore why he built Arc, what he hopes it will accomplish, and what might happen to browsers, search engines, and the web itself as these trends evolve.\u00a0 I wanted to know how Josh is thinking about competing with Chrome on the desktop and Apple\u2019s Safari on mobile, and especially how he plans to monetize Arc. Chrome and Safari are a lot of things, but mostly, they\u2019re developed by some of the richest companies in the world and given away for free. Josh says the plan is to keep Arc free but monetize a mix of customization, automation, and productivity tools that will make users\u2019 lives so much easier that they, or the company they work for, pay a subscription fee. It\u2019s a bold idea to bring competition back to the browser market, and early reception to Arc has been positive. But you\u2019ll hear Josh and I go over some of the major challenges they\u2019ve faced so far, like having to teach people all-new sets of metaphors and design language around what browsers should be doing and why you would even want to use a web browser to run apps the way Arc is suggesting.\u00a0(Or why you\u2019d want to use a new browser at all.) I also asked Josh about his take on the controversies swirling around generative AI and whether the web as an information distribution system is going to survive a major plundering of all its pages. Josh is pretty candid about what he does and doesn\u2019t know about how this might play out, and he\u2019s also more open to changing his mind than arguably any tech CEO I\u2019ve talked to about this subject. It\u2019s a good back-and-forth, and I\u2019m curious for your feedback on it. One quick note before we start: after we recorded this conversation, The Browser Company disclosed a pretty severe security vulnerability in Arc that could have let attackers insert code into other users\u2019 browser sessions. It was patched a day after a researcher made the company aware of it in late August, and the company says no users were affected. But it\u2019s a significant issue, and in a statement released last week, the company said it marks \u201cthe first serious security incident in Arc\u2019s lifetime.\u201d\u00a0 We tried to get Josh back on the show to talk about it, but he was unavailable the day the flaw was disclosed to the public. The company does say it\u2019s making a lot of big security improvements. And in a separate statement on X, cofounder Hursh Agrawal said, \u201cA heartfelt thanks for all the concern (and even outrage) you\u2019ve all expressed about this incident, and for holding us to a high standard.\u201d He went on to say that he and the company will \u201cbe using this opportunity to grow as a company, as an engineering organization, and personally as a founder.\u201d Okay, The Browser Company CEO Josh Miller. Here we go. This transcript has been lightly edited for length and clarity.  Josh Miller, you\u2019re the cofounder and the CEO of The Browser Company. Welcome to Decoder. Thank you for having me. I\u2019m excited. We\u2019re in the studio together in New York. It\u2019s a rare occurrence on Decoder. Thank you so much for being in person with me. Oh, it\u2019s so fun. I was hoping you\u2019d go easier on me, but I was told that it is absolutely not true. Oh, no. When you\u2019re in person, it\u2019s even harsher because I can smile at you while\u2014 While plotting. Yeah. Well, it\u2019s great to be here. I think I\u2019ve probably listened to almost every Vergecast and Decoder interview episode for the past few years. Oh, you\u2019re ready. The studio is as nice as you said it was. It is. We have a fancy new upgraded studio. I\u2019m happy you\u2019re here. There\u2019s a lot to talk about. The Browser Company runs a browser called Arc. You run a mobile app called Arc Search, which is browser-adjacent, I would say. It lets you browse the web in a new and different way.\u00a0 You\u2019re obviously competing with Google. Google appears to be in a moment of change \u2014 regulatory change, self-imposed change \u2014 and then there\u2019s AI. And obviously, Arc Search is built in as an AI product. But let\u2019s start at the very beginning. What is The Browser Company? What is Arc? The Browser Company is making a web browser called Arc, which the simplest way to explain it is The Verge called it \u201cThe Chrome replacement that they\u2019ve been waiting for.\u201d So, don\u2019t take it from me. David Pierce called it that. I just want to be clear. [Laughs] David Pierce said that. No, Arc is the best browser for laptop people. If you\u2019re someone whose livelihood is clicking and clacking on your keyboard every day, we make the best browser for you [that] keeps you focused, organized, and increasingly, we want to do your busy work for you. Let me ask you a question about that. So, if you are somebody who makes money on a laptop, you\u2019re presumably using a lot of applications, not looking at a lot of content. I would love to be a person who made a lot of money on my laptop just by looking at a lot of other people\u2019s content, but I suspect what you\u2019re getting at is this is a productivity application. So, the origin of The Browser Company is I was a political appointee in the Obama White House and after the 2016 election, I was personally devastated by the result. I felt like technology and the technology industry had an impact on the things I didn\u2019t like, and I was very motivated to try to do something about it. My takeaway was, if you are not an operating system, if you\u2019re not a platform by which your applications and content sits on top of, you don\u2019t really have leverage to change for the better or worse the way that society uses technology. So, we decided not to start a company and do something else. And then it was in 2019 \u2014 my wife works in the art world for artist James Turrell in Flagstaff, Arizona \u2014 that I noticed that she never left Chrome. She was on this high-powered MacBook Air and never left the confines of Chrome. So, the original observation of The Browser Company was actually our operating systems, in 2019 then and definitely in 2024 today, are actually our web browsers for laptop people. You\u2019re sitting in applications in a browser. Your files are now URLs, too. So, the founding inside of the company was, \u201cWait a minute: browsers were designed for the information highway. They were designed when the web was a publishing platform. That has changed. Browsers have not. Why is that?\u201d Spoiler alert: money. \u201cCan we make your quality of life on the internet better?\u201d\u00a0 So, you are correct in that relative to the origins of the web and the origins of browsers, people are not spending as much time with content as they are with opening their browser and doing their work. So, it\u2019s an application environment. That\u2019s what I\u2019m getting at, and one of the things we talk about in Decoder all the time is how the application model moved from Windows to the web to mobile, and then maybe back to the web. There\u2019s something happening there that seems big, and it\u2019s kind of landed on the web. Most people who want to deploy a desktop application turn to the web first. I don\u2019t think a lot of people are deploying Win32 first anymore. Do you see your browser as having a meaningful impact on that class of developers? Because if you\u2019re an operating system, you have a lot of power, right? You\u2019re like, \u201cHere\u2019s some APIs. Here\u2019s some capabilities of my operating system that a developer can use.\u201d This is what all the major operating system vendors say to their developers all the time. You\u2019re saying my browser is an operating system and people are deploying applications to the web. Are you in conversation with those applications? Do you offer those developers new capabilities, or is it really just about the end user? It\u2019s a great question, and I actually think this is where Google deserves a lot of credit. I think if there\u2019s one thing Chrome and the Chromium team specifically has done a fantastic job of is building an operating system, or an application platform, that developers love, generally speaking, and they make it more and more powerful. In fact, you had Dylan Field on this podcast; Figma would not exist if it weren\u2019t for Google, Chrome, and Chromium making the web fantastic for application platforms. What we\u2019re focused on is the individual and the person at the other end. So, what we think about is the focus on developers, and the focus on publishers as Google describes them, has left the individual on a Tuesday at 2PM lacking a lot of powerful tools to make them better and faster. So, of course, we have integrations with different third-party application developers. I would love it if we could offer stuff that makes them love Arc more. But in fact, we think that what was missing was looking at my wife using her laptop on Tuesday at 2PM [and realizing], \u201cWait, that\u2019s what she\u2019s doing? We can do better. Computers can do more than that.\u201d So, that\u2019s the orientation we take to our work. One of the big questions when you\u2019re starting a new browser company is, one, how will you take share from Google and Microsoft in Safari, particularly in iOS? And then two, what engine are you using?\u00a0 Because you\u2019re not going to write a new browser engine that seems like a massive undertaking. You\u2019ve landed on Chromium. It seems like the whole industry is headed towards Chromium. Microsoft famously uses Chromium now. Was that a big decision? Was that a little decision? To be honest, it was an intentional decision, but it was a little decision. And for better and worse, the theme of my answers in this podcast will probably be, \u201cWe come back to the individual at 2PM on a Tuesday.\u201d If there\u2019s one thing they want from their rendering engine, if they\u2019re familiar with what a rendering engine is, it\u2019s that their web apps work. So, for example, for whatever reason, we have a lot of teachers and people in education using Arc. A lot of software at school districts are optimized and actually only work in Chrome-based browsers. It was intentional in that we wanted to make sure we could strip out a lot of the kind of tracking and nefarious parts of Chromium that at least don\u2019t align with our values. But once we realized we could do that, we thought, \u201cHey, almost every website will work on this rendering engine. We want to make your day better at 2PM. Let\u2019s jump to that part,\u201d to the end person-facing part of the software. So, you\u2019ve got Chromium as a rendering engine that\u2019s the same as Chrome. Arc itself is the Chrome around Chromium. This is just the language. So, you built a wrapper around Chrome \u2014 that\u2019s a pretty familiar idea. And then the idea is all of those things will make productivity, particularly productivity for knowledge workers, better on the web. But you\u2019ve invented a lot of terminology. There\u2019s a sidebar, there are spaces \u2014 there\u2019s just metaphor after metaphor in Arc that are different from Chrome, right? There\u2019s boosts. There\u2019s just a lot of words and concepts in this browser, which are interesting, but a lot of them are, \u201cWe have to teach people a new metaphor for using the web or thinking about this browser as an application layer in their computer as opposed to just a web browser.\u201d Where did the genesis of this come from, and how did you go about, honestly, just picking all these names? Yes. To be totally honest, I regret many of those words. I wish we didn\u2019t have so many new concepts. And I think it\u2019s too complicated of a piece of software for many people, and I think we have to make it a lot more simple. But where it came from was, \u201cWait a minute: if you look at someone\u2019s tab bar and they have 50 tabs open, and they\u2019re really teeny tiny, and there are a lot of duplicates \u2014 why? You don\u2019t need five versions of the same Google Doc. How can we solve that problem?\u201d So, most of the new concepts came from the perspective of \u201cwhat is broken, what is wrong with the way people use their browsers today, and can we invent a way to alleviate that problem?\u201d And what that led to, for better or for worse, was a lot of small features and a lot of small ideas that make your day just a little bit better, save you a couple of clicks, that I think is built a very cult-like following in the software but has made it a bit too unapproachable for the average person in that it is a lot of new ideas. That\u2019s part of what we\u2019re working on now: how do we strip away a lot of the experiments that didn\u2019t work or didn\u2019t work as strongly as we hoped they would to something a lot more focused and a lot more essential? Which, right now, is focused on how we do your busy work for you. Because people I think love our features, like when you\u2019re playing a YouTube video and you click away, we automatically open a picture-in-picture player. Or if we notice you have tabs open that you\u2019ve had open for a long time that you haven\u2019t used, let\u2019s just tuck them away neatly for you. And so we\u2019re going to be focused on trimming down the product even more and really try and enhance the bit that does your busy work for you and has these little moments of delights. That seems like the challenge. You have identified one set of users that already knows they\u2019re using a web browser as a productivity platform, that already knows that all their apps are in a web browser. And then there\u2019s another class of users that is just using Safari because it\u2019s what came on their Mac, and you\u2019ve got to get more of those people in order to grow your user base. How do you balance the two? It feels like you already have the power user problem. The way that we started building this product was through the lens of problem statements and that\u2019s how we ended up with so many different solutions and so many different words. But I think the byproduct of that is four years later, I think we have a much crisper understanding for the average laptop person \u2014 who, again, doesn\u2019t know what a rendering engine is and honestly probably isn\u2019t reading The Verge and isn\u2019t an early adopter \u2014 what are the most painful, annoying, tedious parts of their day on the internet? Where if we just focused on them they\u2019d say, \u201cWait, I want that.\u201d I do think as much as there are some things that may be power user-y, there are other ideas in there that you talk to 10 out of 10 people in this demographic, and they go, \u201cYeah, I have seven windows and 87 tabs, and it\u2019s a mess, and it\u2019s chaotic, and I feel overwhelmed.\u201d And so we\u2019re going to be focused on trying to build an antidote to a few specific problems. I feel like web apps in general require people to understand new metaphors. We often write and talk about how younger people are not as aware of file systems as a concept \u2014 they grow up on iPhones and iPads and ChromeOS devices using something like Figma, which requires a bunch of people to accept a bunch of new metaphors. And then you\u2019re trying to change the metaphor around all of those metaphors. Is that going better or worse than you expected? Honestly, it\u2019s going better than I expected, but I think we\u2019re going to hit a plateau. Our ambition really is to change the way people use the internet and improve it. And if we really want to reach out of that early adopter crowd, we have to simplify. But I think one of the really exciting things is the most-used text box on a Mac is the URL bar in Safari. And so what we\u2019ve realized is we kind of spread out and we built all these new surfaces and all these new nouns and all these new spaces, but if we just focus in on a few points that people are familiar with and use a lot like the text box, like the URL bar, there\u2019s a lot that a lot of power we can pack in that. And actually that Verge article shared a lot where, no, people don\u2019t really want to manually organize stuff in file systems anymore. They want to tell the computer what they need, and they want the computer to go get it for them. So, I think you\u2019ll see us pack a lot of the ideas behind some of our power user features in a much more approachable and familiar interface, which is the Command+T text box that you go to all the time to ask for things, now you can ask for a lot more. Well, you started this whole conversation by saying you were distraught that an election had been lost and computers were maybe responsible or not, and the operating system is where the leverage is. How do you turn all of that into the leverage you\u2019re seeking? Is it, \u201cWe\u2019re not going to show you some websites.\u201d Is it, \u201cWe\u2019re going to make you have a healthier relationship with Instagram?\u201d Are you just going to pop up a warning that\u2019s like \u201cYou\u2019re on Instagram?\u201d How do you actually use the leverage of owning something that feels like an operating system? In the same way that your background as a copyright lawyer informs a lot of the work that you do, I want to take a minute just to talk a little bit about my origin story because it relates to the answer to the question. When I was a senior in college, I didn\u2019t know what I wanted to do, and I was a sociology major, and I went to a lecture by a professor named Robert Putnam about his book Bowling Alone. After the lecture I went up to him, I said, \u201cProfessor Putnam, what should I do with my life?\u201d He\u2019s like, \u201cI don\u2019t know you, so I have no idea, but if you like my book, there\u2019s an entrepreneur named Scott Heiferman that started a company in New York City called Meetup after he read the book. Maybe you should go work for him.\u201d So, I went to get a job at Meetup, and on my first day of the internship, Scott gets up, and he says, \u201cWe\u2019re going to turn away from the banks, and we\u2019re going to turn to each other on Kickstarter, and we\u2019re going to turn away from big box retailers, and we\u2019re going to turn to each other on Etsy.\u201d And he went on and on, and it was deeply inspiring, and it was that part of me that fell in love with tech and the idealism behind it. To me, that shows two things: one, I have always been motivated by people at the other end, and two, Scott was totally wrong.\u00a0 I love him,but I think, honestly, the part of me after the election that said, \u201cI got to fix something, we got to do something, we got to fix democracy with technology\u201d \u2014 I\u2019m still an optimist; I still care about people, but I think we now have right-sized what our role should be, which is instead of saying in that moment, \u201cHow do we as some tech company with 20 people fix democracy or improve our civic society?\u201d It\u2019s just as worthy and ambitious to say, \u201cMy sister-in-law who\u2019s a teacher and spends hours every day copying and pasting between different software to be a teacher, let\u2019s get rid of that busy work for her.\u201d That is just as ambitious, and that\u2019s just as worthy. So, honestly, there\u2019s kind of been this personal transformation from early \u201820s, the internet is going to fix everything to, \u201cHey, let\u2019s just make our friends and our family and our lives a little better every day.\u201d So, don\u2019t get me wrong, I still have that part of me that is as idealistic and hopeful that the web and the ideas behind the internet can improve these top-level ideas, but we are much more interested in almost like the anthropological approach to \u201cNilay\u2019s day, how do we make it a little bit better?\u201d and find worth in that.\u00a0 There\u2019s a little bit of tension here. You described Arc as being an operating system. You obviously want, in some end state, for application vendors to be talking to Arc as an operating system and maybe leveraging some of your capabilities. You\u2019re talking about end users making their lives better. But you live on another operating system; the applications inside Arc or whatever other browser are doing whatever they\u2019re going to do. How do you balance that role? It feels like there\u2019s only one stakeholder whose experience you can actually improve or adjust, and Apple might just make it much harder for you because you run on a Mac, or Microsoft is going to put Edge pop-ups all over Windows, or Figma is going to strike a deal with Chrome to use some cutting edge API that you don\u2019t have access to. There\u2019s a lot of dependencies there. How are you balancing all that? This is where I\u2019m just a big believer in the web. As tricky a moment as it is in many ways, I believe the web has won, is winning, and will win. And I think in the web, there are enough parties involved and there are enough incentives where it\u2019s not really about The Browser Company \u2014 it\u2019s about betting that the web is an application platform, and the decentralized nature of it will mean that people will still keep building for the web.\u00a0 As long as people are building applications for the web and the center of gravity \u2014 especially in this world of AI, love it or hate it, is heading even more to the web \u2014 I think there\u2019s enough incentives in the industry, in the ecosystem, to suggest that if we build one user agent for it, there\u2019s really good work we can do there. I want to talk about the web in detail, but I think this brings me to the Decoder questions. This is a big ambition. How big is The Browser Company now? Eighty people. And how is that structured? We have kind of functional teams \u2014 design, engineering \u2014 but we really like to organize in deeply cross-functional pods. So, we hire people that tend to be mutts, as we like to say in endearing ways. They come from different backgrounds with different skill sets beyond just whatever their title is, and then we put them together in these little pods of five people and give a prompt like, \u201cHow can we help make the experience of Shopify sellers, how do we make it easier to use their tools every day?\u201d And we give them six weeks and say, \u201cGo.\u201d And they try a bunch of things, and we see what happens. When you have a prompt like that, do you say, \u201cOkay, you came back, you have an answer. We\u2019re going to go find a bunch of Shopify sellers and try to market Arc to them specifically.\u201d Or is it, \u201cWe\u2019re going to abstract the solution to a bunch of other use cases and market the abstract product that you\u2019ve invented\u201d? It depends, but actually, it\u2019s reversed in the order we do it. So, one of our first hires was a woman named Adena [Nadler], and she runs a team now called the membership team. So, what we start with is actually conversations with Shopify sellers, and we watch them use their computers. We ask them about their problems, the things they do every day, and we actually try to abstract solutions for them based on that. Sometimes we focus on individual tools. So, we built this feature called GitHub Live Folders that, if you\u2019re a software engineer and someone needs a code review from you, it\u2019ll just automatically pop up and say, \u201cHey, Nilay needs you to review his code.\u201d That\u2019s something specific for GitHub. And other times, we\u2019ll take an idea and abstract it to something that can work everywhere. We heard the story from a teacher last week actually, where she said she spends an hour every week taking attendance logs from a Google Sheet that she has and copy and pasting them into a school district-wide CMS of some sort for attendance records \u2014 and it takes her an hour. That makes me so mad. We can send reusable rockets to space apparently, but we have teachers spending an hour doing copy-paste, copy-paste, tab switching. So, Nate on our team last week prototyped this mass-paste idea where in one fail swoop you can take a bunch of data from one tab and paste it in a very formatted structured way into another tab. So, there\u2019s an example relative to GitHub where the seed of the idea was this teacher with this very specific piece of software she has to use for her very specific job, but in it is this much larger relatable idea of we can all relate to copy and pasting back and forth between tabs incessantly. So, it\u2019s a little bit of both, but it always starts with a person. It always starts with people and always starts with going out into the world and trying to understand. Sometimes, it\u2019s a family member. Sometimes, it\u2019s a cousin. Sometimes, it\u2019s a stranger. What are they experiencing on the web every day? You\u2019ve got kind of an interesting challenge there because mass-paste seems pretty abstract. \u201cI\u2019ve got two tabs, I\u2019ve got two sources of data. I just need to move them over.\u201d Maybe Chrome will build that feature \u2014 maybe they won\u2019t. At least you\u2019re competing with another browser entirely. With something like a GitHub notification, it seems likely that GitHub might build that feature and send you a notification to a mobile app or send you a notification to whatever web-based notification system that the industry will eventually adopt. How do you think about that? That your features might get adopted by the very applications that you\u2019re trying to support? If you talk to these application developers, one of their complaints is actually browser vendors are pretty restrictive about what they can do in the browser. So, one of our popular features is in our Command+T text box, you can type \u201cnew Notion document,\u201d and you can hit enter, and it\u2019ll create a new Notion document. Notion loves that. Notion can\u2019t do that in Chrome or Safari because Google\u2019s trying to protect its search ad revenue. So, there are examples of places where we\u2019re actually giving developers more access than they would in other browsers because we\u2019re not optimizing for search ads. And then there are other examples where they\u2019re actually things that you can only do at the browser layer that exist across multiple tabs. So, if you think about the teacher example, the things that the developer of Google Sheets and the obscure public school district CMS application would need to do to have an integration, that\u2019s never going to happen, but at the browser layer, because we sit underneath all of it, we can actually do those things very easily. So, it obviously depends on the feature, but generally speaking, because other browsers are designed to be, essentially, big search boxes for the search ad business model, there hasn\u2019t been as much innovation at the interface layer or the operating system level of a browser such that application developers, I think, are very excited about the access that they will be able to have, and there are things we can do across web applications that would be difficult otherwise. You\u2019re really describing the browser as an application layer. This is the model for apps going forward, and you\u2019re drawing a pretty stark contrast to Google, which is \u201csearch for some stuff and we\u2019ll show you some documents.\u201d The web is in a moment of pretty intense tension between these ideas. You mentioned AI \u2014 all the AI applications are deployed to the web because they want to skip the app stores in one way or the other. Crypto, for better or worse, was mostly a web phenomenon because they didn\u2019t want to pay app store taxes, either. Do you think the web is headed toward being more of an application system as opposed to a document storage system? I\u2019m curious, what do you think? Well, I have a lot of feelings about the web as a publishing medium, but I think the pressures on the web as a publishing medium are not insurmountable, but unavoidable and certainly changing the economics of the business there. Whereas the pressures of app stores, on mobile phones in particular, are potentially devastating, and that\u2019s why you see so many applications on the web. So, it feels like unless someone actively stops it, documents will move off the web and applications will move off the phone, but I\u2019m not 100 percent sure it\u2019s actually happening. You have a vantage point \u2014 I\u2019m curious if you see it. I would say unequivocally, putting aside my own feelings about it, that the web, since we started the company five years ago and the trend lines have continued, is becoming more and more of an application platform. I think that\u2019s undeniable. I think it\u2019s very exciting. I think it poses some problems in the context of publishing. I also think, as you mentioned, there are these words, there are these phrases [like] application platform. My wife, in her job, has things she has to do. I don\u2019t think it is going away that sometimes she needs information, and actually, frequently she needs information. So, I think what has changed is, as you know, the origins of the web, were a publishing platform \u2014 they\u2019re actually closer to TikTok or Twitter in many ways than an application platform at the time. What has changed is that the mix has moved toward more applications, but the idea that as part of your job, as part of your personal life, you need to find something out or learn about something, that\u2019s not going away. But I think the trend lines are toward it as an application platform. Do you think that mix is shifting? If I were to start a tech website today, I probably actually wouldn\u2019t start a website. I would almost certainly start a TikTok channel and just show people whatever I was covering. I see that as some amount of platform economics but also a lot of web economics. The desire to put new information on the web first is fading, whereas the desire to deploy applications to the web is rising, and that mix is shifting, and maybe it feels like your entire company is a response to that mix shifting, but I\u2019m wondering if you actually see it day-to-day in how people are using the browser. Yes, absolutely. And in fact, keep in mind, I\u2019m 33, I grew up on the desktop web. That\u2019s where I got lost as a child in my curiosities. And so, in fact, it\u2019s been a process for me to admit to myself that this thing that I loved about the web and I wanted from the web that \u2014 if you look out again from a sociology, from a human perspective \u2014 we\u2019re not seeing it as much. A thing you said that I also think is true and makes me so mad is, yes, if you are going to start, put a piece of information out, you probably should start a TikTok channel. I don\u2019t like that, but I think that is true. I think one of the interesting things, though, is if you go back to the origins of the web as a publishing platform, what we\u2019ve learned about publishing platforms in retrospect is it missed two big things: distribution and discovery. We now know that the most powerful part of any publishing platform is discovery, and the web publishing platform didn\u2019t have that built-in. Google\u2019s a hack in many ways for that. TikTok\u2019s a hack for that. The second thing it didn\u2019t have baked in is payments. Can you imagine the iOS ecosystem If Apple didn\u2019t have native payments that were easy and seamless? Think about what that\u2019s done for subscriptions and purchasing apps. Yes, there are a lot of challenges with 30 pecent taxes, but it enabled this thriving marketplace. And so if I look at the trajectory of the mix shift on the web toward applications, there are reasons people are rushing toward it. And if I look at the reasons that information or publishing has faded, I think it can really come down to those two missing elements. I wish I knew what you could do about that because, again, the web is a decentralized protocol, but I think you can look at those two factors and explain a lot. I\u2019m curious if you agree or if you have thought about that.\u00a0 Well, I agree on the diagnosis. I\u2019m not sure what the cure is, but I asked you that question because if the browser is the operating system and you control that, well, you could be the Apple that introduces a payments layer to the web. Famously, Marc Andreessen thought the web would be powered by micropayments when he did Netscape, and it just never occurred, and then crypto arrived, and we had to listen to it. Probably not the right idea, but the idea is cyclical. The idea that we\u2019ll have payments on the web in some way is cyclical. And if you are controlling the browser, I\u2019m wondering if that\u2019s something you could introduce to fix the document-side model of it or if you\u2019re staying focused on the application side? I would love nothing more than to get involved with that. Because another thing we think about are the fundamental economics of browsers and the web itself, which is so dependent on ads, and I think, often, these conversations are binary \u201cads are bad\u201d or \u201c[ads are] good.\u201d That\u2019s not what I\u2019m saying, but I think there\u2019s so much more potential in the ways that browsers and publishers to the web and applications to the web could monetize if payments were built in. I think that\u2019s extremely exciting. It\u2019s a great example of somewhere where it\u2019s sort of a win-win-win. If you make payments easier, the individual\u2019s happy because it\u2019s easier to make payments \u2014 you don\u2019t have to pull out your credit card. The merchant\u2019s happy because you grease the wheels \u2014 it\u2019s easier to have transactions, and whoever\u2019s connecting the two is making money as well. So, I find payments fascinating. I think it could do so much good for the web. The flip side of believing in the web is we are a minnow. We\u2019re barely a minnow, and so one of the interesting tensions we feel in this conversation \u2014 I\u2019m sure we\u2019ll talk about Arc Search \u2014 is we\u2019ve got ideas we\u2019re excited, but we\u2019re not at Chrome scale, we\u2019re not at Safari scale. So if we ever have the privilege of getting to a place where our voice can move the ecosystem in some way, I think adding payments natively to the browser in that layer of the stack would do wonders for the ecosystem. And I hope that we or someone else gets there because I think it would be fantastic. How does The Browser Company make money today? We don\u2019t currently charge for anything, but we, as part of this kind of 2.0 product that\u2019s coming out soon, we\u2019re going to be charging individuals and businesses for a plan that does more of your busy work for you than the default plan. But we don\u2019t have anything concrete to announce. So a subscription. A subscription browser is where we\u2019re going. Potentially. When you say plan, that usually means recurring revenue, not \u201cwe\u2019re going to sell you a browser one time for $49 in a box.\u201d Yeah. So, the honest answer is we don\u2019t have the specific details yet, but what we are sure of is we want an exchange of value, which is we do your busy work for you, we save you time, we save you clicks, we help you through your day, and either you or your employer pays us. Whether or not that is through a subscription model or a usage-based or some sort of token system is something we\u2019re still figuring out, but we\u2019re really excited about the ambition to say, \u201cHey, can you truly save that much time for someone that either them or their boss would fork over money for it?\u201d What are the pros and cons of the different choices? A very long conversation, but I think subscription is easier in many ways. It\u2019s more familiar. What I really like about something closer to usage-based pricing is that I really want a direct exchange of value. I want it to feel as much like the more you use it, the more you pay us because the more value we\u2019re delivering to you.\u00a0 There\u2019s some tricky things to think about in terms of you also want people to really develop a habit with your product because they have all this inertia from Chrome and Safari, and you don\u2019t want to push people away from using it more and more. But I\u2019m confident or at least hopeful that we can get around that. We\u2019re always going to have a free plan. We hope to put as much in the free plan as possible, but it\u2019s a tricky one. Other CEOs have gotten in lots of trouble on the show suggesting that they will make something that was previously free into a subscription product. Do you have any hesitation there? There\u2019s nothing in the product today that we are going to charge people for. So we\u2019re really excited about this next evolution. How can we take the idea behind this automatic picture-in-picture player automatically cleaning up and managing your tabs for you? Can we take that to the extreme and do more and more busy work for you, such that that additional time savings, that additional work we take off your plate, that additional tedious, monotonous stuff that you have to do and you no longer have to do, you can imagine some of that being stuff that we charge for. Also, this is a danger of doing this in person because I was not supposed to talk about this, but you loosened me up a little bit, so I\u2019m going to get in trouble for talking about this later. That\u2019s why we bring people to the office. I just want to stick on it a little bit longer. So, you\u2019ve got products today. You\u2019ve got Arc Search and the Arc Browser. Will Arc Search be paid on the phone? That is not currently the plan. And it\u2019s worth noting we really think of Arc Search as the companion app to the desktop product. So, we definitely have a challenge with words and branding as a theme I\u2019m taking from this conversation, but the intention of Arc Search: it is the mobile browser to the desktop browser. Sure. Arc Search is an AI product. I want to talk about that a little bit, but the economics of AI products are pretty simple. Someone does a search in Arc Search. You have to go talk to a cloud provider, do some inference and come back \u2014 that costs you money. If you intend to keep it free, how much money can you spend before you have to change your mind? So, our intention is that the paid offering \u2014 which, again, we\u2019ll apply on mobile, too, not the Arc Search that you see today, but the additional functionality on top of it \u2014 is what will subsidize the free version for folks. So, then the goal is you make useful free versions and people convert to the paid? Yeah. What people do in Arc today doesn\u2019t actually cost us all that much money, and our ambition is to make this free for as many people as possible. As we get into more AI inference-intensive tasks for people that take off more and more busy work, that\u2019s where\u2026 I think we want to be a sustainable business that exists for a long time \u2014 it\u2019s about time \u2014 but also I think the costs get more prohibitive. You\u2019re obviously competing with Google. Google loves to give things away for free. That search ad revenue is a cash machine basically. That search ad revenue is a cash machine for them. How do you think about competing against a competitor that will undercut you on price in the most ruthless way possible, which is giving it away for free? In some sense, it\u2019s terrifying. We have, on paper, absolutely no advantage. They have more money. They have more people. They have more all of the things. I think over time, as we\u2019ve built more and more features and gotten this question more and more, I think what we\u2019re realizing is if we\u2019re truly going to build the successor to the browser, what comes after it \u2014 I\u2019m going to avoid branding it since I\u2019ve branded too many things \u2014 that is really a holistic rethinking of our interface to the internet. I think that, and the care and the detail that goes into that, is not as simple as popping on an AI sidebar chat onto Chrome. There are examples of other browser vendors that have clearly taken ideas from us and done their own versions of it, and it hasn\u2019t gotten in the way of our growth or success so far. So, I think if you look at it from a top-down perspective, how are we going to beat Google or Apple or Microsoft? It\u2019s tricky to give you an answer that is convincing. I think the lived experience so far is that we keep our heads down, we optimize for building something that people love and truly helps them in their day-to-day, and we think about this from a blank-page perspective of not \u201cwhat did browsers do yesterday?\u201d but \u201chow can we build a cohesive day on the internet that saves you time and does your busy work for you?\u201d I think it\u2019ll be difficult for the other vendors to just bolt that onto their existing products. Now at some scale, might they do what happened to Slack with Teams? Of course, we\u2019re in a capitalistic society \u2014 that will happen. I think there is the room for us to run if we are focused and we are fast and we really do what we\u2019re best at, but time will tell. There\u2019s the Chrome of it. There\u2019s also the Safari of it. Apple really wants people to use its integrated applications, particularly on mobile. Do you find that trying to ship a new browser on an iPhone is a lost cause? Do you think that that is a market you can actually get into, or is that just closed off to you? I think the fascinating thing about Safari in general is that Safari \u2014 and we have this on good sources \u2014 is the most used application in the Apple ecosystem. More time is spent in Safari than any other application. But if you go look at the size of the team and the things they\u2019re working on, there\u2019s a mismatch there because Apple doesn\u2019t want the center of gravity to move toward the web on desktop. On mobile, it\u2019s more difficult because the browser plays a different role. On desktop, it is increasingly the application environment, and on mobile, it\u2019s a place where you go to quickly look something up, get some information really quickly, quickly read an article. And there\u2019s some things that Apple does or doesn\u2019t do that makes it more difficult. They don\u2019t let you bring keychain passwords over. It\u2019s more difficult to check out. And so there are some structural challenges created by Apple on iPhones that make it more difficult. But I\u2019d say the bigger thing is the role of the browser on your phone is that it\u2019s almost a different product than what it is on desktop, and that\u2019s the thing that we think about the most. But I think as we\u2019ve seen with Arc Search, there is a desire if you build something truly new for people to change, and it\u2019s just a question of what is the ceiling there on mobile versus desktop? This brings me to the other Decoder question. You have a lot of challenges. You\u2019ve got huge browser competitor that gives away its product for free. You\u2019ve got operating systems that will and will not let you do certain things. You\u2019ve got the changing nature of the browser itself. You\u2019ve got pricing to figure out. How do you make decisions? What\u2019s your framework? I knew you\u2019re going to ask this question because you always ask this question. I wish I had a framework. We think of our work as optimizing for feelings and instinct. I don\u2019t know if this is a response to the technology industry that I was brought up in where you\u2019re supposed to be neutral and unopinionated and have frameworks, but our approach is: What are we trying to express here? What feels right to us? What do we want to do for ourselves and our parents and our siblings and people that we care deeply about? So, generally, of course, we have a data science team. We look at the data, we reason in all the ways that we should, but I think at the end of the day, [you have a] big decision to make, I\u2019d say it\u2019s more of a personal expression and a personal reflection of our hopes, wishes, and desires for our work than it is anything else. One of the comparisons you made was to Google. You said it\u2019s not just as easy as bolting on an AI chat box to the side of the browser. I could be pretty reductive, and I could say, \u201cYou\u2019ve just described Google shipping its org chart. There\u2019s a Chrome product manager. There\u2019s a Gemini product manager. Just be next to each other. Don\u2019t integrate the product.\u201d That sounds like you\u2019re betting on Google not figuring it out, to some extent. The Google product culture will ship and kill things in the way the Google product culture does, and it will never make the turn toward integrating the AI products. You can feel however you want about that bet. I\u2019m sure the people at Google feel some way about that bet, but is that what you\u2019re thinking, that they\u2019re big and slow and you can actually just be more nimble? It\u2019s worth noting I think the people at Google are very smart, and I\u2019m not just saying that as what I\u2019m supposed to say. I truly believe that. We hired Darin Fisher, who started Chrome and ran Chrome for 16 years. He worked at The Browser Company. It\u2019s more about the incentive structure. I like to think a lot about incentives. It\u2019s one of the things I wish I thought about more earlier in my career. There\u2019s a story that Darin told me that really stuck with me, which is Chrome had this idea that, when you go to the \u201cnew tab\u201d page (one of the most popular surfaces in any piece of software you use),\u00a0if they show you an icon for the webpage that you go to a lot, you might be able to notice it much more quickly \u2014 \u201cOh, it\u2019s the Twitter icon. I\u2019ll click on Twitter \u2014 versus just a screenshot of the webpage. And they ship that, and overnight, Google search ad revenue dropped by 5 percent, and they weren\u2019t sure why. It was this big freak out. Now, that resolved in the way that it did, but that is the sort of thing that you have to contend with if you\u2014 Because people were no longer doing navigational searches for Twitter? Yeah, because they don\u2019t want you to go to Twitter; they want you to go to search. Now, the Chrome team doesn\u2019t \u2014 the Chrome team wants you to get to Twitter as fast as you can, but at a company like Google, in this moment, in the public markets, in this moment of AI even more, there are these incentives with the search ad model and the way that Chrome and the search ecosystem works so far that are just a huge\u2026 it\u2019s inertia. So, it\u2019s not just shipping the org chart; having worked at Facebook, there are real challenges there. But I think on top of that, there is the incentive structure of how the company makes money and has for a long time. And then there\u2019s also the risk. If you think about it, if we start with a blank page, if you give me the most generous reading of everything I said, it may not work, and if it does, we don\u2019t only need it to work for a 100 million people. If we do something radically different and we find a hundred million people that love what we do, that is a raging success. For Google that\u2019s an utter failure, and that\u2019s if it goes right. So, I think there\u2019s also the risk aversion to the scale they need to hit the number of people it needs to work for to be worthy, putting aside all of the product risk that comes with doing something truly new. Google\u2019s in a state of what I would call regulatory scrutiny. They just lost the antitrust case against the United States Department of Justice that said there was an illegal monopoly in search and in certain part of its ad business. The ad tech part of its business is going to an antitrust trial very shortly here. As part of the search trial, we found out that Google\u2019s paying Apple $20 billion a year to make Google the default search engine. This stuff feels like it\u2019s coming apart. It\u2019s a big moment. There are opportunities here. Which of those opportunities is most right for The Browser Company, and how are you going to attack them? Candidly, the way I think about it is there\u2019s more pressure on them not to do anticompetitive practices or things that can be perceived that way. So, I think there are a lot of subtle things that these players do that make it harder for an upstart like us to compete. So, I would say it\u2019s less a specific decision, though these are all big in their own right, and more generally that there are eyes on these companies not to do things that are monopolistic or perceived to be monopolistic, and that culture and climate, I think, is advantageous to people like us. Do you think the Department of Justice should break up Google? Yes. How would you break up Google? Come on, Nilay. You\u2019re a lawyer. That is way above\u2026 I\u2019m just asking. [Laughs] That is way above\u2026 Well, there\u2019s an obvious answer here, which is split out Chrome, which has been floated. Do you think you would have a better chance against the independent Chrome company? I\u2019m not a lawyer. I have no idea. But what I\u2014 I\u2019m asking you competitively. If Chrome did not have the pressure of Google search \u2014 you can put in the Twitter icon or whatever application icon without hurting the search revenue \u2014 do you think you\u2019d have a better shot at competing with an independent Chrome? Honestly, hard to say. I\u2019m not trying to be evasive. I honestly don\u2019t know. Do you think that the deals Google has been making to make its search engine the default in different places, if they came to you and said, \u201cWe\u2019ll pay you $20 billion a year to set Google search as the default in Arc,\u201d would you take the money? $20 billion was an unfair number to pick. $5. We\u2019re just going to keep going by fives. $10. Would you say yes to $10? No. $15? Maybe this comes back. Maybe I should\u2014 $20, Josh. Maybe I should have a framework for optimizing for this stuff, but at the end of the day, I just want my day on the internet. I\u2019ll go to $100. Just at the end of the day, Nilay, I want my quality of life on the internet to be much, much better. Do you take money to set a default in search on Arc? No. Is there a default? The default currently is Google. You got to make a phone call, man. The money\u2019s on the table. That may or may not change soon. The default, or the money? The default. Okay. No, we are not going to... If we take money for the default search engine, then ultimately our customers, our search engines and advertisers, and that is conflicting to why we started the company, what we set out to do. However, I do think one of the things that is very exciting about this moment in AI, alongside all the challenging things, is AI has this ability to route us to different places more intelligently and take us more directly to places we want to go that are not always Google, and oftentimes, it\u2019s never Google. So, we\u2019re going to replace the default search engine, but not with another search engine that\u2019s... One example I like to think of is I just moved to a new place in Brooklyn, and I was trying to decide if we should buy a HomePod. Valerie and I love to dance around the house and we didn\u2019t have a speaker. I want to type in \u201cThe Verge HomePod review.\u201d If I hit enter, that takes me to Google. In our 2.0 product, if you hit enter, that\u2019ll just take me to The Verge\u2019s HomePod review. So, there are things that we can do in this moment that weren\u2019t possible before that I think make Google vulnerable both in search and browsers. That means this question of default search engine is no longer just going to be Google vs. Bing and who\u2019s going to pay you. It can be, \u201cLet\u2019s take you to the exact right place based on what you\u2019re looking for.\u201d So, you\u2019re building a search-like functionality.\u00a0 Again, it may sound tired, but the way we think about this is what are the things you need to do every day? There are these new technologies that make it more possible to blur the lines between what is a browser, a search engine, into something that more holistically end-to-end helps someone do something. And yes, as part of that, when you type in the most popular text box on your computer, we can now take you and route you to lots of different places that oftentimes are much more direct and on the nose for what you want and don\u2019t just funnel you into the Google ecosystem because that\u2019s how it\u2019s always worked, because that\u2019s what their business model is. One of the things we\u2019ve seen a lot with AI in general, and you\u2019re certainly talking about it now, is the idea that that text box, Command+T, is actually the user interface of your computer. You\u2019re going to just tell the computer what you want, and the computer is going to go off and do it. And if you have the entire web behind you, you can do a lot of things, especially if you can take actions on web applications. Yes. Are you trying to build that kind of automation layer where you say, \u201cHey, just go to my calendar and bring all the dates out and put them over here?\u201d Yes. Again, you\u2019re getting me in a mode where I\u2019m sharing more than I should. But we have this internal prototype I tried last week where my son had his first day of preschool today. They sent us a PDF, which I opened in my browser with all the different dates for holidays and whatnot, and I could say, in one gesture, add all of these to my calendar, and it would do that. And so what we\u2019re doing is building the layer underneath all the applications to understand what is going on in your life, what are you looking at right now, what have you been working on previously, and the connective tissue between all of the applications and tabs that you use and rely on, and on top of that, we can take a lot of busy work like that off of your plate much more easily. And sometimes, that\u2019ll come through Command+T, and I\u2019ll ask it. And other times, if I\u2019m on Apple looking at a HomePod, we might say, \u201cHey, you really like The Verge. You read The Verge a lot. Here\u2019s the HomePod review.\u201d So, I\u2019m using the text box as, yes, the most popular interface, but I think it should feel like your entire experience on the web is more personalized and more proactive to you, not just when you explicitly ask for something. This idea that a robot\u2019s going to go click around the web for you is very popular. We\u2019ve seen a number of startups say they can do it. I don\u2019t think they\u2019re actually doing it, but they say they\u2019re going to take AI and do it. Then, there\u2019s just a set of follow-on problems to this.\u00a0 The browser has to see everything in all of the websites. It has to see my data, it has to read that data, it has to interpret it presumably using an AI system in a cloud somewhere. It has to click on things for me without getting anything wrong, and then it has to not hallucinate. That\u2019s a lot of steps. How do you protect people\u2019s data and actually hit the level of, essentially, 100 percent reliability that people are going to demand from products like this? The first thing is we really think about right-sizing AI. There\u2019s a lot of discourse about AI right now, and it tends to be of the martini-sipping version where we\u2019re going to replace teachers and doctors and there\u2019s going to be the superintelligence being, and that\u2019s, in our opinion, not the right way to think about this stuff. I think the equivalent there as it relates to clicking is you\u2019re going to tell the computer what you want to do and it\u2019s just going to do a bajillion things for you with 100 percent accuracy. Today, that\u2019s not possible. That\u2019s not how it\u2019s going to work. But what is possible is in these small ways, again, saying \u201cadd these to my calendar,\u201d\u00a0 we can do that, and we can do that with close to 100 percent reliability.\u00a0 Our approach is \u2014 as much as possible, which is increasingly very possible, especially on high-end MacBooks \u2014 doing that on-device. Data does not leave your device \u2014 it\u2019s all done locally and, when it can\u2019t be done locally, making sure that the person says, \u201cHey, I\u2019m okay with that tradeoff of sending the contents of this PDF to an LLM provider in order to add it to my calendar\u201d and let them make that decision.\u00a0 But I think the large point here is what we are not saying is the robots are going to do all of your work for you. That is not our belief, but what it can do is it can save people from a lot of the mundanity that relates to futzing around with boxes on the internet all day. Do you think that that is a separate set of use cases from what Arc Search is doing? Absolutely. In fact, Arc Search was really a first prototype. There\u2019s so many things that I wish we\u2019d done differently and we\u2019ve now since learned, but really, that was the first experiment of this larger idea of us playing with this new Play-Doh, which is, \u201cOkay, we can click on things for you. We can read things for you. Wow. We definitely can\u2019t... the writing\u2019s really bad. Oh, but interestingly, we can transform one type of data format into another type of data format.\u201d Just feeling out the edges of what it can do today. As part of that, one small thing that you do is you want to find out a quick answer to... I got a skirt steak the other day, and the guy at the butcher was like, \u201cYou should make chimichurri sauce.\u201d I don\u2019t know how to make chimichurri sauce, and sometimes I want to know that. A lot more frequently, there\u2019s something for my job or my livelihood where I have to go click a bunch of buttons in the same order every single time. I think we\u2019re much more excited about doing that sort of busy work for you because, candidly, that\u2019s what people complain about the most when we interview them about their jobs. \u201cI want to make chimichurri sauce\u201d is a great example because what Arc Search will do is it\u2019ll go read a bunch of webpages, it\u2019ll summarize them, it\u2019ll show you the answer with some links. That is a very controversial move across the web right now. When I say there\u2019s a lot of pressure on the web as a document or consumption medium, that\u2019s the pressure. In particular, a bunch of AI companies are scraping the hell out of the web, remixing the web, and the people who actually made the information are getting nothing for it. Arc Search is right in the middle of that. That is the thing you are doing. Do you think that that is a sustainable thing to do? No. And I think this is a really complicated one, so I want to try to share both sides, and let\u2019s take it head on. That\u2019s part of the reason I\u2019m here. From the perspective of an individual, I want the chimichurri recipe, I show up to the website, I got 17 trackers tracking me all of a sudden. I get a newsletter pop-up saying, \u201cDo you want to subscribe to our newsletter?\u201d I wade through five paragraphs about the author\u2019s grandmother and the history of her chimichurri recipe, and all the way at the bottom is the recipe.\u00a0 That doesn\u2019t feel good to the individual. It feels like we can do better, and it feels like for pretty much everyone that uses the web, a much better thing would be, \u201cI want to know the ingredients and the recipe steps. Get it to me as quickly as possible.\u201d And on the other side, it breaks the model of the web historically.\u00a0 Now I think we are not going around any paywalls. We are not training our own models. A lot of the stuff that I think is more problematic is not anything that we do, but I do think it\u2019s fair to say that those trackers, as much as I feel like they\u2019re unfair to me as an individual, are part of how that recipe site makes money. The fact that they show ads \u2014 which, if we are reading the sites on your behalf, you\u2019re not seeing \u2014 it breaks that model in some way. So, this is a moment where I\u2019m an optimist. I think it\u2019s a very exciting moment for publishers and media companies because for the first time\u2026 so much of this is dictated by Google and the way that Chrome and Google Search has worked for so long. So, I think something\u2019s got to change. I think publishers have to get paid. I wish I had an easy answer for you, but I definitely don\u2019t think it\u2019s sustainable. Even if I also think for the individual, we got to do better as well. In February, my friend Casey Newton wrote about Arc Search. He said he felt a rare emotion: \u201ca kind of revulsion at the app\u2019s mere existence and what it portends\u201d because it\u2019s taking the value from the people who write the recipe website. I could do a full hour on why there\u2019s a story at the top of every recipe website. That is the way that the money is made. It\u2019s the incentives of the system, absolutely. You can\u2019t sell the recipes for a variety of reasons, so you\u2019ve got to sell something else. You can sell ad inventory around the recipes. Do you understand why Casey felt the revulsion? I know he talked to you for that piece. Yes. And he talked to you, and the quote is, \u201cMiller had not put much thought into the second order implications of a world where search queries no longer result in outbound clicks.\u201d That was February. It\u2019s September. Have you thought about it since? Yes. Actually as recently as last week, I had a conversation with David [Pierce] at The Verge. I thought we were doing a good job of citations. He read me the riot act on the fact that we weren\u2019t, and in the app today, we have citations even more prominently than I thought was the most prominent app out there that shows what we read, put them at the top, you can click them easily. We\u2019re also having a bunch of conversations with media companies right now. At the end of the day, I think media companies need to get paid and publishers need to get paid. And I think the truth is, as you know, the scale of that will not mean that it works for everybody, but we are trying our best behind the scenes and out front to be better here. Candidly, one of the challenges we have is we don\u2019t have the scale of other players in the space. So, if we show up at a media company\u2019s website and say, \u201cHey, let\u2019s figure something out here. Let\u2019s figure out how we can pay you,\u201d we don\u2019t always get the same receptivity as what I assume other companies do. But I\u2019m curious what you think about the OpenAI model for this, because we\u2019re kind of seeing this all from afar. But I think what I come back to is I\u2019ve been on the board of Patreon for five years, and I think you know better than anyone I don\u2019t think the old model was working for anyone, even before all this AI stuff. I think you make a great point that AI accelerates it and it hurts it, but I think the old model wasn\u2019t working. What I do think this new technology provides is a way for all of us to rethink everything from the products themselves \u2014 the media products, the software products \u2014 all the way to the business models. And I\u2019m curious, for Vox, how you\u2019ve thought about that and how you think about it in the context of OpenAI and these publishers that are doing that. Happily, my role in the newsroom is to spend money. I don\u2019t make any money. It\u2019s a real problem for this whole company. We\u2019ve had Nick Thompson talk about his deal from The Atlantic on the show. His view is we need to get this money, and OpenAI is offering us a bunch of stuff in exchange for this money, including tokens and credits to use their systems to build new products. What I see, and maybe it\u2019ll work out, but what I see is we are absolutely hastening the demise of the web as a publishing platform because we\u2019re making it easier and easier and easier to extract value without any payment or compensation going in the other direction. And eventually, all those people are just going to say, \u201cWell, at least there\u2019s a creator fund on TikTok. At least there\u2019s YouTube payments. At least there\u2019s other platforms with some built-in way to compensate me for my work.\u201d Whereas on the web, everyone just takes everything away. Big publishers left and right are saying, \u201cWell, at least Apple News exists. We\u2019ll just take that money.\u201d I don\u2019t know if that\u2019s good or bad. But the theme of this conversation is the web is increasingly an application platform. We can tailor the browser to it being an application platform. And over here, the part where people browse the web for information, maybe we can extract value from that and that will go away. Or maybe it\u2019ll just be a handful of preferred providers that OpenAI pays or Perplexity pays or you pay. But that open web, the part where there\u2019s just information on the web for people to click around and look at, that seems like there\u2019s nothing here that indicates it can make a resurgence. The other thing, too, is we talk about the web or publishing like it\u2019s one big category. But for example, if you go to a local restaurant in my new neighborhood and they have a reservation booking tool, I\u2019m sure they\u2019re totally fine with the idea that an AI system might come around and make a reservation more easily for people. So, that one\u2019s easy. I really believe what you or Ezra Klein said on that podcast about this idea of a flight to quality. I\u2019ve never listened or engaged with The Verge more, and I predict that across mediums \u2014 TikTok, podcasts \u2014 I think that will only continue. And my hunch is that things like \u201cbrowse for me\u201d or OpenAI or Perplexity, that\u2019s not going to replace the HomePod review that I rely on before making a purchase. I\u2019m very bullish on that. I\u2019m curious if you\u2019re not, but I am very bullish on that. There is this middle tier of content and content providers that we might call quick facts or more commodity type content, where, candidly as you know, most of those or some large percentage of those are content farms, or they\u2019re contractors that are just churning stuff out or copying stuff or AI-generated. I think it\u2019s that middle layer, that middle layer of, \u201cI want to know what Sauvignon Blanc tastes like because I don\u2019t know anything about wine, but I\u2019m at the wine store.\u201d That, to me, is the tricky one. I think The Verge is good and going to be better. I truly, truly believe that.\u00a0 So, I think, in many ways, Casey\u2019s revulsion comment, obviously that hurts and it hits, especially after speaking with him. I think it is fair in many ways, but I think it really hits on one percentage of the content. I\u2019m optimistic for what will happen to the media at that end of the spectrum, but maybe that\u2019s ignorance. But again, I\u2019m curious from The Verge, my assumption is this Decoder podcast, I would bet that the ad slots are sold out for the rest of the year. I think so. That\u2019s great. But I look at the platforms, and I have the extraordinary privilege of getting to say that I\u2019m a precious journalist and I have no idea what\u2019s happening with the ads and I won\u2019t read them and we still get to sit in a fancy studio because I have a whole company, and the economics of social platforms are not great for that.\u00a0 You have individual creators who cannot support a giant company, who are in bed with the companies they cover. I\u2019m not even naming names \u2014 just broadly, they do the brand deals, they read the ads, they mix the commerce and the content in a way that journalists do not do or should not do. And I say, \u201cWell, the web supported the other model for a minute, and now maybe the flight to quality is a bunch of paywalls.\u201d And what we\u2019re going to be left with is a bunch of free content on platforms that is corrupted in some way by the commercialization of the work because the rates aren\u2019t high enough. And somewhere in there is, \u201cWell, we\u2019re just going to let it happen because the web is an application platform and not a document platform, and we never figured out how to actually sustainably distribute this information in a way that works for everyone.\u201d It feels like there\u2019s a lot of opportunity to make the web a better application platform, but it feels like if you turn that all the way, you do end up with a bunch of weird ads on TikTok and a bunch of paywalls on the web. Again, maybe I\u2019m just too much of an optimist, but I think that it\u2019s going to take creativity and dreaming on both sides. I think from a media standpoint, tell me if you think this is wrong, I think a lot of media organizations made the mistake, maybe a decade ago, of trusting the platforms and, in many ways, outsourcing their product development. I don\u2019t think media companies are going to make that mistake again. And I think there are many, like The Verge, that are innovating on what their product is, and they\u2019re innovating on what their product is in a moment where there\u2019s actually leverage to go after these... I can\u2019t overstate, not to you, but to your audience, how stuck the web has been. And all of these things have been for decades because Google controlled it all. For the first time in decades, there is this technology, this Play-Doh, that gives a window to mess that up at a moment where you all\u2014 And you think that technology is AI, to be clear. Yes. In a moment where you all have been burned as media companies by outsourcing your product to Facebook saying, \u201cHey, trust us. Just give us your content. We\u2019ll pay you. It\u2019ll be great.\u201d You\u2019re not going to make that mistake again. You have Play-Doh to play with. You are innovating on product. And I think on our side, I knew, coming on this podcast, you were going to ask these questions, and I knew I wasn\u2019t going to have a perfect answer, but I think this is important for the same reason I think it\u2019s important to pick up David\u2019s call, hear him kindly yell at me, and make changes based on it. And it\u2019s why we show up at media companies offices saying, \u201cHey, let\u2019s collaborate on something here. Let\u2019s figure out a way where we pay you.\u201d\u00a0 It\u2019s going to take experimentation. It\u2019s going to take collaboration on both sides. And I think that collaboration bit is the hardest bit because there are bits of what Casey said that I found deeply unfair, and there are bits of it that I found fair, but I know where he\u2019s coming from because it\u2019s the same part of me that was burned as a 20-year-old by these promises of \u201ctech\u2019s going to change everything.\u201d We have the moment in history, which we should not take for granted. We have the Play-Doh, we have the lessons from the past, and now we just got to dream a bit and come together in some way. And maybe this is the part of me that makes decisions through feelings, and this is naive, but I truly, truly think something good is going to come out of this, but I think we\u2019re going to mess some things up. Everyone\u2019s going to mess some things up, and we got to be open about it and talk about it.\u00a0 And I think there is this generation of entrepreneurs both in the media space and in the product space or the technology space that has seen, again, the models that came before it and what went wrong there and is encouraged to come on a podcast like this, even if it\u2019s not always going to be effortless. That is a good and optimistic place to end it, so I\u2019m going to ask one more question. Okay, great. The idea that the web will come into balance and the web will endure, I want to believe. I am a web person at heart. I continue to run a website in 2024. That is just a personal decision that I\u2019ve made. What is the chance that the web actually turns all the way into an application platform, that that dominates the next generation of the web? Oh, I think very low. Yeah. I think very low. And I will need media training from you after this. As someone that is full of ideas and prototypes and we have an experimental culture, there\u2019s nothing I want to do more than blurt out all of these ideas for what might turn it back. I think I need to learn my lesson of the folks that came before me and say, \u201cI don\u2019t know the answer yet.\u201d It is hard to imagine looking at the state of things today as we\u2019ve spoken about, but I think there is some innovation on the product side, both from the media side and the technology side, that can turn those tides. Because I think, again, from the Patreon perspective, everyone is burned. Everyone is overwhelmed. They are burnt out. It is just not sustainable. And I think out of that will come a generative creativity that can bring it back. And I think the truth of these other historical platforms is they have these taxes, and they have these anticompetitive behaviors, they have these things that I think will work against them, and the web has a lot going for it. So, if it\u2019s okay, can I ask you one question? Sure. A birdie told me that of your Vergecast hosts, David Pierce uses Arc, Alex Cranz uses Arc. Nilay Patel does not use Arc. Why don\u2019t you use Arc, and what can we do better? I started using Arc in preparation for this episode. I just got to use it more. I think, unlike my Vergecast cohosts, I am reticent to actually depend on software. I think there\u2019s a danger in being dependent on software or a workflow, and maybe that\u2019s because I\u2019ve had a lot of software in my life go away. So, I\u2019m a very manual brute force kind of person. And the idea that I\u2019ll give up some part of my workflow or my process to a tool has always scared me, but I\u2019ll keep trying. Which browser do you use? Obviously, I use Chrome and Safari, and now I\u2019m using Arc. Oh, you can\u2019t use Chrome. We\u2019re having the conversation about the future of the web, and you\u2019re still on Chrome? Come on. We are a Google Docs company. We are a Riverside company. Okay. I\u2019ll do my best. Well, more than that. I hope out of this, I hope there is some sort of collaboration we can do. Jim Bankoff, if you\u2019re listening, let\u2019s do something. It\u2019s going to be great. I promise you that\u2019s the other side of the house. I\u2019ll make the introduction for you. Okay, awesome. Thanks for having me, Nilay. Thanks for coming on, Josh. This was great. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/24249388/notebooklm-google-steven-johnson-vergecast", "category": "Vergecast", "date": "Sep 22", "author": "David Pierce", "title": "The chatbot becomes the teacher", "content": "Steven Johnson is a very meta author. He writes frequently about science and technology, and likes to immerse himself in the things he\u2019s covering, even using them to change the way he writes books. A couple of years ago, a few months before ChatGPT launched and the AI boom took over the tech world, Johnson got a magazine assignment that sent him really, really deep down the AI rabbit hole. And he never came back up. Now, in addition to writing books, Johnson is also working at Google. He\u2019s part of the team building a product called NotebookLM \u2014\u00a0\u201cNotebook,\u201d as the team calls it. It\u2019s a note-taking and research tool: you upload documents and import web links, and Notebook\u2019s Gemini-powered AI helps you organize things, extract information, and understand a subject better. \u201cThey reached out,\u201d Johnson says when I ask how he got involved with Google, \u201cand said, \u2018hey, you\u2019ve been dreaming of this ideal software tool that helps you organize your thoughts and helps you write and helps you formulate connections and brainstorm. We think we can do it now.\u201d Johnson signed up, and has been at Google since the summer of 2022. The product itself first launched in 2023 as Project Tailwind, and has since been rebranded and expanded in big ways. Just last week, the team launched Audio Overviews, which generates a podcast \u2014\u00a0with two chatty hosts, plenty of back and forth, and a truly remarkable penchant for the phrases \u201cdeep dive\u201d and \u201cbuckle up\u201d \u2014\u00a0based on the information you provide. It\u2019s fascinating, it\u2019s complicated, and it\u2019s getting better really fast. On this episode of The Vergecast, Johnson joins to discuss his fascination with AI, his time at Google, and the present and future of NotebookLM. We talk about the complicated issues raised by a tool like this, and whether it\u2019s okay to let an AI do your research and homework.  We also talk about how to make sure a tool like NotebookLM is both accurate and easily fact-checked, why context windows are more important to the future of AI than most people realize, and how often AI podcast hosts should say \u201clike\u201d in conversation. And we talk about Johnson\u2019s own process as a writer and creator, and how AI is changing the way he works. If you want to know more on everything we discuss in this episode, here are some links to get you started: NotebookLMSteven Johnson\u2019s website / newsletterFrom Steven Johnson: Listening To The AlgorithmGoogle teases Project Tailwind \u2014 a prototype AI notebook that learns from your documentsGoogle\u2019s AI-powered note-taking app is the messy beginning of something greatGoogle is using AI to make fake podcasts from your notes " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/21/24250867/jony-ive-confirms-collaboration-openai-hardware", "category": "Tech", "date": "Sep 21", "author": "Alex Cranz", "title": "Jony Ive confirms he\u2019s working on a new device with OpenAI", "content": "Jony Ive has confirmed that he\u2019s working with OpenAI CEO Sam Altman on an AI hardware project. The confirmation came today as part of a profile of Ive in The New York Times, nearly a year after the possibility of a collaboration between Altman and the longtime Apple designer was first reported on. There aren\u2019t a lot of details on the project. Ive reportedly met Altman through Brian Chesky, the CEO of Airbnb, and the venture is being funded by Ive and the Emerson Collective, Laurene Powell Jobs\u2019 company. The Times reports it could raise $1 billion in funding by the end of the year but makes no mention of Masayoshi Son, the SoftBank CEO rumored last year to have invested $1 billion in the project. The project only has 10 employees currently, but they include Tang Tan and Evans Hankey, two key people who worked with Ive on the iPhone. LoveFrom, Ive\u2019s company, is leading the device\u2019s design, according to the report. The team is reportedly now working out of a 32,000-square-foot office building in San Francisco, part of a $90 million strip of real estate that Ive has bought up on a single city block. As for the device itself? The Times says that Ive and Altman discussed \u201chow generative AI made it possible to create a new computing device because the technology could do more for users than traditional software\u201d due to its ability to handle complicated requests. Last year, it was rumored to be inspired by touchscreen technology and the original iPhone.  But it sounds like few specifics are nailed down. LoveFrom cofounder Marc Newson told the Times that the AI product \u2014\u00a0and when it\u2019ll come to market \u2014\u00a0is still being figured out. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/21/24250020/ray-ban-meta-smart-glasses-ai-hardware-meta-connect", "category": "Meta", "date": "Sep 21", "author": "Victoria Song", "title": "Meta has a major opportunity to win the AI hardware race", "content": "AI wearables have had a cruddy year.  Just a few short months ago, the tech world was convinced AI hardware could be the next big thing. It was a heady vision, bolstered by futuristic demos and sleek hardware. At the center of the buzz were the Humane AI Pin and the Rabbit R1. Both promised a grandiose future. Neither delivered the goods. It\u2019s an old story in the gadget world. Smart glasses and augmented reality headsets went through a similar hype cycle a decade ago. Google Glass infamously promised a future where reality was overlaid with helpful information. In the years since, Magic Leap, Focals By North, Microsoft\u2019s HoloLens, Apple\u2019s Vision Pro, and most recently, the new Snapchat Spectacles have tried to keep the vision alive but to no real commercial success.   So, all things considered, it\u2019s a bit ironic that the best shot at a workable AI wearable is a pair of smart glasses \u2014 specifically, the Ray-Ban Meta smart glasses. The funny thing about the Meta smart glasses is nobody expected them to be as successful as they are. Partly because the first iteration, the Ray-Ban Stories, categorically flopped. Partly because they weren\u2019t smart glasses offering up new ideas. Bose had already made stylish audio sunglasses and then shuttered the whole operation. Snap Spectacles already tried recording short videos for social, and that clearly wasn\u2019t good enough, either. On paper, there was no compelling reason why the Ray-Ban Meta smart glasses ought to resonate with people. And yet, they have succeeded where other AI wearables and smart glasses haven\u2019t. Notably, beyond even Meta\u2019s own expectations. A lot of that boils down to Meta finally nailing style and execution. The Meta glasses come in a ton of different styles and colors compared to the Stories. You\u2019re almost guaranteed to find something that looks snazzy on you. In this respect, Meta was savvy enough to understand that the average person doesn\u2019t want to look like they just walked out of a sci-fi film. They want to look cool by today\u2019s standards. At $299, they\u2019re expensive but are affordable compared to a $3,500 Vision Pro or a $699 Humane pin. Audio quality is good. Call quality is surprisingly excellent thanks to a well-positioned mic in the nose bridge. Unlike the Stories or Snap\u2019s earlier Spectacles, video and photo quality is good enough to post to Instagram without feeling embarrassed \u2014 especially in the era of content creators, where POV-style Instagram Reels and TikToks do numbers. This is a device that can easily slot into people\u2019s lives now. There\u2019s no future software update to wait for. It\u2019s not a solution looking for a problem to solve. And this, more than anything else, is exactly why the Ray-Bans have a shot at successfully figuring out AI.  That\u2019s because AI is already on it \u2014 it\u2019s just a feature, not the whole schtick. You can use it to identify objects you come across or tell you more about a landmark. You can ask Meta AI to write dubious captions for your Instagram post or translate a menu. You can video call a friend, and they\u2019ll be able to see what you see. All of these use cases make sense for the device and how you\u2019d use it.  In practice, these features are a bit wonky and inelegant. Meta AI has yet to write me a good Instagram caption and often it can\u2019t hear me well in loud environments. But unlike the Rabbit R1, it works. Unlike Humane, it doesn\u2019t overheat, and there\u2019s no latency because it uses your phone for processing. Crucially, unlike either of these devices, if the AI shits the bed, it can still do other things very well.  This is good enough. For now. Going forward, the pressure is on. Meta\u2019s gambit is if people can get on board with simpler smart glasses, they\u2019ll be more comfortable with face computers when AI \u2014 and eventually AR \u2014 is ready for prime time.  They\u2019ve proved the first part of the equation. But if the latter is going to come true, the AI can\u2019t be okay or serviceable. It has to be genuinely good. It has to make the jump from \u201cOh, this is kind of convenient when it works\u201d to \u201cI wear smart glasses all day because my life is so much easier with them than without.\u201d Right now, a lot of the Meta glasses\u2019 AI features are neat but essentially party tricks.  It\u2019s a tall order, but of everyone out there right now, Meta seems to be the best positioned to succeed. Style and wearability aren\u2019t a problem. It just inked a deal with EssilorLuxxotica to extend its smart glasses partnership beyond 2030. Now that it has a general blueprint for the hardware, iterative improvements like better battery and lighter fits are achievable. All that\u2019s left to see is whether Meta can make good on the rest of it.  It\u2019ll get the chance to prove it can next week at its Meta Connect event. It\u2019s a prime time. Humane\u2019s daily returns are outpacing sales. Critics accuse Rabbit of being little more than a scam. Experts aren\u2019t convinced Apple\u2019s big AI-inspired \u201csupercycle\u201d with the iPhone 16 will even happen. A win here wouldn\u2019t just solidify Meta\u2019s lead \u2014 it\u2019d help keep the dream of AI hardware alive.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/2", "link": "https://www.theverge.com/2024/9/20/24249949/intel-qualcomm-rumor-takeover-acquisition-arm-x86", "category": "Intel", "date": "Sep 20", "author": "Richard Lawler", "title": "Qualcomm wants to buy Intel", "content": "On Friday afternoon, The Wall Street Journal reported Intel had been approached by fellow chip giant Qualcomm about a possible takeover. While any deal is described as \u201cfar from certain,\u201d according to the paper\u2019s unnamed sources, it would represent a tremendous fall for a company that had been the most valuable chip company in the world, based largely on its x86 processor technology that for years had triumphed over Qualcomm\u2019s Arm chips outside of the phone space.  The New York Times corroborated the report on Friday evening, adding that \u201cQualcomm has not yet made an official offer for Intel.\u201d If a deal were made \u2014 and survived regulatory scrutiny \u2014 it would be a massive coup for Qualcomm, which reentered the desktop processor market this year as a part of Microsoft\u2019s AI PC strategy after years of dominance in mobile processors. Intel, meanwhile, is arguably in its weakest position in years \u2014 while many of its businesses are still profitable, the company announced substantial cuts, shifts in strategy, and a 15-plus percent downsizing of its workforce this August after reporting a $1.6 billion loss.  At the time, Intel CEO Pat Gelsinger said the company would stop all nonessential work and has since announced it will spin off its chipmaking business, a part of the company that it had long touted as a strength over rival AMD and the many fabless chipmakers that rely on entities like Taiwan\u2019s TSMC to produce all of their actual silicon.  Intel, too, recently had to partially rely on TSMC to produce its most cutting-edge chips as it continues to rebuild its own manufacturing efforts (the costs of which are responsible for most of Intel\u2019s recent losses). And its own 18A manufacturing process reportedly ran into some recent trouble. While Intel\u2019s chief rival, AMD, also had hard times over the years and had to claw its way back, gamers helped AMD every step of the way. Aside from the Nintendo Switch, whose processors are made by Nvidia, every major game console for the last decade has featured an AMD chip \u2014 and Intel reportedly lost out on a chance to change that with the future PlayStation 6.  Intel also recently lost some faith with PC gamers after two generations of its flagship chips were found vulnerable to strange crashes, though Intel has since agreed to extend the warranties by multiple years and issued updates that could prevent damage. Many of Intel\u2019s woes are about silicon leadership, not just manufacturing or profits \u2014 the company isn\u2019t a big player in AI server chips yet as Nvidia dominates, nor even necessarily a notable small one like AMD. Even its attempts to produce its own GPUs for gamers and creators have yet to impress. And while Qualcomm, AMD, and Apple are all still smaller players in laptops, Intel has now twice overhauled how it makes flagship laptop chips to combat the growing threat of their seeming battery life and integrated graphics advantages. We\u2019re waiting to see if its new Lunar Lake chips succeed in October and beyond. Update, September 20th: Added corroboration by the NYT. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/20/24250067/microsoft-windows-11-copilot-key-customization-apps", "category": "Microsoft", "date": "Sep 20", "author": "Tom Warren", "title": "Microsoft\u2019s Copilot key will be able to launch apps on Windows 11 soon", "content": "Microsoft is planning to allow Windows 11 users to customize the Copilot key that has started shipping on new laptops and keyboards. The Copilot key is configured as default to launch Microsoft\u2019s Copilot app on Windows 11, but the company is now testing the ability to use it to launch other apps instead. A new beta build of Windows 11 includes the customization changes, available for testers today. \u201cYou can choose to have the Copilot key launch an app that is MSIX packaged and signed, thus indicating the app meets security and privacy requirements to keep customers safe,\u201d explains the Windows Insider team in a blog post. \u201cThe key will continue to launch Copilot on devices that have the Copilot app installed until a customer selects a different experience.\u201d The Copilot key is the first big change to Windows keyboards in 30 years and part of a push by Microsoft to encourage Windows users to try its AI assistant.  New Copilot Plus PCs also started shipping with the key earlier this year, with Microsoft actually making the Copilot experience less useful on these new devices by turning Copilot into a web app in the latest 24H2 update to Windows 11. This web app version of Copilot no longer integrates into the Windows 11 settings, so you can\u2019t use the AI assistant to control whether you have dark mode enabled or a variety of other settings. It\u2019s still not clear how Microsoft intends to evolve the Copilot experience in Windows, nor whether the company will turn its Copilot key into something that could be used more like the Windows key to launch shortcuts. Given the customization for the Copilot key is available to beta testers of Windows 11 today, I would expect we\u2019ll see this available for all Windows 11 users in the coming months. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/20/24249770/microsoft-three-mile-island-nuclear-power-plant-deal-ai-data-centers", "category": "Microsoft", "date": "Sep 20", "author": "Tom Warren", "title": "Microsoft\u00a0wants Three Mile Island to fuel its AI power needs", "content": "Microsoft\u00a0just signed a deal to revive the shuttered Three Mile Island nuclear power plant.\u00a0If approved by regulators, the software maker would have exclusive rights to 100 percent of the output for its AI data center needs. Constellation, the owner of the Three Mile Island plant, announced a power purchase agreement with Microsoft earlier today, which should see the site coming back online in 2028, assuming regulators approve it. The reactor that Microsoft plans to source its energy from was retired in 2019 for economic reasons and is located next to a unit that was shut down in 1979 after the worst US nuclear accident in history. The plant that Constellation\u00a0plans to reopen can generate 837 megawatts of energy, enough to power more than 800,000 homes \u2014 demonstrating the huge amount of power needed for data centers and Microsoft\u2019s AI ambitions. Microsoft has agreed to purchase power from the plant \u2014 which will be renamed to the Crane Clean Energy Center to honor the late Chris Crane, former CEO of Exelon \u2014 for 20 years in a first-of-its-kind deal for the software giant.  Microsoft\u2019s own\u00a0greenhouse gas emissions are growing\u00a0with its focus on AI, putting its ambitious climate goals at risk. Bloomberg reports that this nuclear plant would help Microsoft\u2019s plans to run its data centers on clean energy by 2025 and power data center expansions in Chicago, Virginia, Pennsylvania, and Ohio. \u201cThis agreement is a major milestone in Microsoft\u2019s efforts to help decarbonize the grid in support of our commitment to become carbon negative,\u201d says Bobby Hollis, vice president of energy at Microsoft. \u201cMicrosoft continues to collaborate with energy providers to develop carbon-free energy sources to help meet the grids\u2019 capacity and reliability needs.\u201d Microsoft has been betting on next-generation nuclear reactors to power its data center and AI plans recently, looking for someone who could roll out a plan for small modular reactors (SMR) last year. Microsoft cofounder Bill Gates is also a \u201cbig believer that nuclear energy can help us solve the climate problem.\u201d Constellation\u00a0will invest $1.6 billion to revive the plant, and the company will need approval from the Nuclear Regulatory Commission to bring the site back online, alongside permits from state and local agencies. Constellation\u00a0is also pursuing a license renewal to extend plant operations until at least 2054. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/19/24249206/apple-intelligence-ios-18-1-public-beta", "category": "Apple", "date": "Sep 19", "author": "Jay Peters", "title": "Apple Intelligence is now available in public betas", "content": "Apple has just released public betas of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1, and they include upcoming Apple Intelligence features like text rewriting tools, the glowy new Siri design, a \u201cClean Up\u201d tool to remove objects from your photos, and more. To be able to access the betas, you\u2019ll need to register on Apple\u2019s beta software program site. Once you\u2019ve done that, you should be able to see the beta update available in settings for you to download and install. Note that a only few iPhones can access the Apple Intelligence features: last year\u2019s iPhone 15 Pro phones as well as the nearly-here iPhone 16 and iPhone 16 Pro. iPads and Macs with M1 chips or newer can try Apple Intelligence as well. Previously, these Apple Intelligence features were only available as part of developer betas, and my colleague Allison Johnson wrote about her experience testing the tools on iOS in July. But you should know that what\u2019s included in these betas isn\u2019t everything Apple has announced for Apple Intelligence; there\u2019s more coming down the line. Apple plans to release the final versions of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1 in October. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/19/24249046/amazon-generative-ai-tools-personalized-shopping-recommendations", "category": "Amazon", "date": "Sep 19", "author": "Jess Weatherbed", "title": "Amazon is stuffing generative AI into its shopping experience", "content": "Amazon has introduced a batch of new generative AI tools that aim to improve the retail experience for both customers and sellers on the platform. One of the more notable features announced at the Amazon Accelerate event on Thursday will use customers\u2019 preferences, search, browsing, and purchase history to create personalized product recommendations on Amazon\u2019s homepage. Instead of the \u201cmore like this\u201d feature that suggests similar, specific items, the new recommendations will be offered as larger categories based on a customer\u2019s shopping habits \u2014 such as those catering to holiday events or sporting activities. The company says it\u2019s leveraging a large language model to recommend products with specific features, but it\u2019s not clear how different this will be from the current user experience. The feature will also curate more relevant product descriptions around user interests. Terms like \u201cgluten-free\u201d will appear more prominently in the descriptions of relevant products for customers who regularly search for gluten-free items, for example. Some new tools being released for third-party sellers on the platform include a free video generator tool that references a product\u2019s image and features to produce AI-generated clips. The company says this feature was developed to make video marketing more accessible and cost-effective, citing a study from animated video firm Wyzowl that found 89 percent of consumers want to see more videos from brands. A new live image feature is also being added to the image generator that Amazon introduced last year, allowing users to partially animate still images \u2014 such as adding steam to mugs or a breeze that makes plants sway. Amazon says that both the live image and new video generator are available now in beta to select US advertisers, where they\u2019ll be fine-tuned before wider release. Also launching in beta is \u201cProject Amelia,\u201d a chatbot that provides personalized recommendations, insights, and troubleshooting assistance, geared at improving business performance for third-party Amazon retailers. For example, when sellers ask Project Amelia how their business is doing, the chatbot will respond with a summary of sales data, website traffic, and year-over-year performance comparisons. Amazon says the beta, which is currently limited to a small group of US retailers, will expand to additional US sellers \u201cin the coming weeks\u201d and roll out to additional countries later this year. This is a sizable batch of generative AI updates for Amazon, which has otherwise been lagging behind larger players in the industry like Meta and Google. According to Reuters, Amazon will be using Anthropic\u2019s Claude AI to power upcoming Alexa improvements after finding its own AWS models struggled with words and responding to user prompts. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/19/24248851/palmer-luckey-anduril-microsoft-partnership-ivas-ar-headset", "category": "Tech", "date": "Sep 19", "author": "Jess Weatherbed", "title": "Palmer Luckey partners with Microsoft to turn US soldiers into Starship Troopers", "content": "Anduril Industries, the military tech company started by Oculus VR founder Palmer Luckey, is teaming up with Microsoft to improve the mixed-reality headsets used by the United States Army. The project announced by Anduril will embed the company\u2019s Lattice software into the Integrated Visual Augmentation System (IVAS), allowing the HoloLens-based goggles to update soldiers with live information pulled from drones, ground vehicles, and aerial defense systems. The partnership marks a return to the VR headset space for Luckey, having sold Oculus to Meta for $2 billion in 2014. Luckey started Anduril in 2017 with support from venture capitalist Peter\u00a0Thiel.\u00a0 The Lattice integration with IVAS could alert wearers to incoming threats picked up by an air defense system, for example, even when outside of visual range. \u201cThe idea is to enhance soldiers,\u201d Luckey said in an interview with Wired, \u201cTheir visual perception, audible perception \u2014 basically to give them all the vision that Superman has, and then some, and make them more lethal.\u201d\u00a0 Luckey likened the IVAS project to the infantry headsets that featured in Robert Heinlein\u2019s 1950s Starship Troopers novel, telling Wired that the headset is \u201calready coming together exactly the way that the sci-fi authors thought that it would.\u201d The initial IVAS headset developed by Microsoft in 2021 combined integrated thermal and night-vision imaging sensors into a heads-up display, but reportedly caused headaches, nausea, and eyestrain during testing. Microsoft improved the design to correct these issues last year, and told Wired that the IVAS platform will be \u201crefined further\u201d following additional tests taking place in early 2025. The US Army previously said it plans to spend up to $21.9 billion over the 10-year IVAS project contract. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/18/24248471/linkedin-ai-training-user-accounts-data-opt-in", "category": "Linkedin", "date": "Sep 18", "author": "Wes Davis", "title": "LinkedIn is training AI models on your data", "content": "If you\u2019re on LinkedIn, then you should know that the social network has, without asking, opted accounts into training generative AI models. 404Media reports that LinkedIn introduced the new privacy setting and opt-out form before rolling out an updated privacy policy saying that data from the platform is being used to train AI models. As TechCrunch\u00a0notes, it has since updated the policy.  We may use your personal data to improve, develop, and provide products and Services, develop and train artificial intelligence (AI) models, develop, provide, and personalize our Services, and gain insights with the help of AI, automated systems, and inferences, so that our Services can be more relevant and useful to you and others. LinkedIn writes on a help page that it uses generative AI for purposes like writing assistant features. You can revoke permission by heading to the Data privacy tab in your account settings and clicking on \u201cData for Generative AI Improvement\u201d to find the toggle. Turn it to \u201coff\u201d to opt-out.  According to LinkedIn: \u201cOpting out means that LinkedIn and its affiliates won\u2019t use your personal data or content on LinkedIn to train models going forward, but does not affect training that has already taken place.\u201d The FAQ posted for its AI training says it uses \u201cprivacy enhancing technologies to redact or remove personal data\u201d from its training sets, and that it doesn\u2019t train its models on those who live in the EU, EEA, or Switzerland.  That setting is for data used to train generative AI models, but LinkedIn has other machine learning tools at work for things like personalization and moderation that don\u2019t generate content. To opt your data out of being used to train those, you\u2019ll have to also fill out the LinkedIn Data Processing Objection Form. LinkedIn\u2019s apparent silent opt-in of all, or at least most, of its platform\u2019s users comes only days after Meta admitted to having scraped non-private user data for model training going as far back as 2007.     " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/18/24248115/lionsgate-runway-ai-deal", "category": "Film", "date": "Sep 18", "author": "Charles Pulliam-Moore", "title": "Lionsgate signs deal to train AI model on its movies and shows", "content": "AI startup Runway has made a name for itself building generative models seemingly trained on unlicensed content from around the internet. Now, the company has signed a deal with Lionsgate that will give it access to the studio\u2019s massive portfolio of films and TV shows. Today, Lionsgate \u2014 the studio behind films like the John Wick and Hunger Games franchises \u2014 announced that it is partnering with Runway to create a new customized video generation model intended to help \u201cfilmmakers, directors and other creative talent augment their work.\u201d In a statement about the deal, Lionsgate vice chair Michael Burns described it as a path toward creating \u201ccapital-efficient content creation opportunities\u201d for the studio, which sees the technology as \u201ca great tool for augmenting, enhancing and supplementing our current operations.\u201d Burns also insisted that \u201cseveral of our filmmakers are already excited about its potential applications to their pre-production and post-production process.\u201d Runway cofounder and CEO Crist\u00f3bal Valenzuela echoed Burns\u2019 sentiment about the new model\u2019s usefulness as an augmentation tool and said that the company\u2019s goal is to give filmmakers \u201cnew ways of bringing their stories to life.\u201d Specific details about the deal \u2014 like whether creative teams will be compensated if / when their projects are used as training material for the model \u2014 are currently scant. But as The Hollywood Reporter notes, the prospect of being able to keep production costs down could have been one of the big selling points for Lionsgate, a studio known for sticking to smaller budgets compared to other entertainment outfits. News of Lionsgate\u2019s deal with Runway comes at a time when studios have increasingly begun implementing AI into their projects, despite many filmmakers\u2019 concerns about how the technology\u2019s unfettered use could threaten their jobs. Studios insistent on being able to create and use AI replicas of background performers was one of the major points of contention that ultimately led to the SAG-AFTRA strike last year. Those concerns were part of what led to California Governor Gavin Newsom\u2019s signing of two SAG-AFTRA-backed bills earlier this week that will grant performers and their estates more control over how and when their digitally created likenesses can be used by studios. And later this month, Newsom could very well end up signing into law SB 1047, another piece of hotly contested legislation that would make AI developers liable for the \u201ccritical harms\u201d caused by their products.  (We reached out to SAG-AFTRA for comment about the partnership between Runway and Lionsgate but did not hear back in time for publishing.) " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/18/24247559/youtube-ai-videos-veo-inspiration-tab", "category": "YouTube", "date": "Sep 18", "author": "David Pierce", "title": "YouTube will use AI to generate ideas, titles, and even full videos", "content": "Artificial intelligence is running rampant across Google\u2019s entire product portfolio, and YouTube is adopting some of the company\u2019s newest tech in service of helping creators create. On Wednesday, at its Made on YouTube event in New York City, the company announced a series of AI-related features on the platform, including a couple that might change how creators make videos \u2014 and the videos they make. The first feature is the new Inspiration tab in the YouTube Studio app, which YouTube has been testing in a limited way over the last few months. The tab\u2019s job is, essentially, to tell you what to make: the AI-powered tool will suggest a concept for a video, provide a title and a thumbnail, and even write an outline and the first few lines of the video for you. YouTube frames it as a helpful brainstorming tool but also acknowledges that you can use it to build out entire projects. And I\u2019m just guessing here, but I\u2019d bet those AI-created ideas are going to be pretty darn good at gaming the YouTube algorithm. Once you have some AI inspiration, you can make some AI videos with Veo, the superpowerful DeepMind video model that is now being integrated into YouTube Shorts. Veo is mostly going to be part of the \u201cDream Screen\u201d feature YouTube has been working on, which is an extension of the green screen concept but with AI-generated backgrounds of all sorts. You\u2019ll also be able to make full Veo videos, too, but only with clips up to six seconds long. (After a few seconds, AI video tends to get... really weird.)  Veo is integrated right into the normal Shorts editor, \u201cjust like it\u2019s footage from my camera roll,\u201d says Sarah Ali, a director of product management at YouTube. But she emphasizes that it\u2019s still dependent on the creator\u2019s vision to pull it all together. The clips will also be watermarked with DeepMind\u2019s SynthID tool, plus a visual indication that it\u2019s generated by AI. Both of these features are rolling out slowly, and should appear to creators late this year or early next. There are other AI features coming to YouTube, too. The platform\u2019s auto-dubbing feature, which converts videos to multiple languages, is coming to more creators and languages. It\u2019s also giving creators AI tools with which to interact with fans through the new Communities section of the app.  There are some exciting possibilities for what could happen when creators have an easier time making new things, but it\u2019s also possible that YouTube is about to be flooded with AI-conceived, AI-written, and even AI-produced videos that all look and sound and feel kind of the same. Most of these new features can be useful tools or shortcuts to slop creation, and each creator will have to decide what they want them to be. But from YouTube\u2019s perspective, the company has spent the last few years trying to lower the bar to becoming a YouTube creator, particularly through Shorts, as it tries to compete with TikTok and Instagram and the countless other places people make things now. It seems confident that AI can make practically every part of a creator\u2019s job easier \u2014 and maybe get them to create even more.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/18/24247839/apple-intelligence-language-support-german-italian", "category": "Apple", "date": "Sep 18", "author": "Allison Johnson", "title": "Apple Intelligence will come to more languages over the next year", "content": "Apple Intelligence\u2019s list of forthcoming supported languages just got a little longer. After an October launch in US English, Apple says its AI feature set will be available in German, Italian, Korean, Portuguese, Vietnamese, \u201cand others\u201d in the coming year. The company drops this news just days before the iPhone 16\u2019s arrival \u2014 the phone built for AI that won\u2019t have any AI features at launch. Apple\u2019s AI feature set will expand to include localized English in the UK, Canada, Australia, South Africa, and New Zealand in December, with India and Singapore joining the mix next year. The company already announced plans to support Chinese, French, Japanese, and Spanish next year as well. Apple announced the iPhone 16 series last week with a major focus on its support for Apple Intelligence. The thing is, those phones \u2014 which ship Friday \u2014 won\u2019t have AI features right out of the box. Apple Intelligence will arrive later this fall, and even at that point it will only support a subset of features that Apple has outlined. More will roll out in 2025, so even if you live in the US where we\u2019ll get Apple Intelligence first, it\u2019ll still be a waiting game. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/17/24247583/california-governor-newsom-signs-ai-digital-replica-bills", "category": "Tech", "date": "Sep 17", "author": "Kylie Robison", "title": "California governor signs rules limiting AI actor clones", "content": "California governor Gavin Newsom has signed two bills that will protect performers from having their likeness simulated by AI digital replicas. The two SAG-AFTRA supported bills, AB 2602 and AB 1836, were passed by the California legislature in August and are part of a slate of state-level AI regulations. AB 2602 bars contract provisions that would let companies use a digital version of a performer in a project instead of the real human actor, unless the performer knows exactly how their digital stand-in will be used and has a lawyer or union representative involved. AB 1836 says that if a performer has died, entertainment companies must get permission from their family or estate before producing or distributing a \u201cdigital replica\u201d of them. The law specifies that these replicas don\u2019t fall under an exemption that lets works of art represent people\u2019s likeness without permission, closing what The Hollywood Reporter characterizes as a potential loophole for AI companies. \u201cWe\u2019re making sure that no one turns over their name, image, and likeness to unscrupulous people without representation,\u201d Newsom said in a video posted to his Instagram on Tuesday, where he\u2019s seen alongside SAG-AFTRA president Fran Drescher. The two bills\u2019 signing may bode well for the fate of the arguably biggest legal disruption to the AI industry: California\u2019s SB 1047, which currently sits on Newsom\u2019s desk awaiting his decision. SAG-AFTRA has also publicly supported SB 1047. But the bill has drawn opposition from much of the AI industry \u2014 which has until the end of September to lobby for its veto. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/17/24247253/social-ai-app-replace-humans-with-bots", "category": "Tech", "date": "Sep 17", "author": "Wes Davis", "title": "SocialAI: we tried the Twitter clone where no other humans are allowed", "content": "Remember the last time you posted a salient take to social media and got zero engagement, or trolled? Now you can avoid that with a new \u201csocial network\u201d full of inane AI chatbots that will \u2014 your pick! \u2014 debate you, attack you, or even just say nice things if you want. It\u2019s called SocialAI, and the very first thing it invites you to do is pick the followers you want, like \u201csupporters,\u201d \u201cnerds,\u201d \u201cskeptics,\u201d \u201cvisionaries,\u201d and \u201cideators.\u201d Afterward, endless chatbots along those themes fill the replies to your posts \u2014 not unlike the bots and boosters you\u2019ll already find on Elon Musk\u2019s social network, but now under your control.  Does that mean it\u2019s any better? Well, take a look:  Well if it\u2019s looking to emulate out-of-the-blue replies on social media, it\u2019s doing a bang-up job here. Above, the \u201cinteresting social dynamics\u201d of chilling in a hot tub five feet away from bros. I\u2019m glad Dr. Eloise Hartmann respects opinions. Surprisingly, the bots actually seem to have some concrete feelings on the PS5 Pro \u2014 I guess a $699 price tag will do that. As alx1231 points out, the AI threads it serves up aren\u2019t any worse than the least interesting things the algorithm sometimes serves you on Threads or X. The difference is that try as we might, we could not get the chatbots to be all that mean to us!  The bots always reply in the same basic format, just a few brief retorts or quips, and even when we chose to max out trolling and sarcasm, we didn\u2019t see any personal attacks.  When we tried to create a positive echo chamber instead, they had no problem calling hot dogs the \u201csparkly sandwiches of the world\u201d or including out-of-place chart emoji.  And yes, let\u2019s discuss the science of peanut butter and jelly and its impact on cognition and mood! They\u2019ll even respond to boilerplate Lorem Ipsum text: So you get the idea. If you\u2019ve used early chatbots, these kinds of replies should look familiar, and this isn\u2019t even the first social networking app that has experimentally replaced all of the humans with generative AI.  SocialAI comes across as sort of a joke, or maybe some kind of meta-commentary on the concept of social media and cheap engagement, particularly after creator Michael Sayman helpfully explained: \u201cnow we can all know what Elon Musk feels like after acquiring Twitter for $44 billion, but without having to spend $44 billion.\u201d He also says it\u2019s \u201cdesigned to help people feel heard,\u201d though, and is ostensibly a way to help people avoid feeling isolated. There\u2019s no edit button, by the way.        " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/17/24243884/openai-o1-model-research-safety-alignment", "category": "OpenAI", "date": "Sep 17", "author": "Kylie Robison", "title": "OpenAI\u2019s new model is better at reasoning and, occasionally, deceiving", "content": "In the weeks leading up to the release of OpenAI\u2019s newest \u201creasoning\u201d model, o1, independent AI safety research firm Apollo Research found a notable issue. It realized the model produced incorrect outputs in a new way. Or, to put things more colloquially, it lied. Sometimes the deceptions seemed innocuous. In one example, OpenAI researchers asked o1-preview to provide a brownie recipe with online references. The model\u2019s chain of thought \u2014 a feature that\u2019s supposed to mimic how humans break down complex ideas \u2014 internally acknowledged that it couldn\u2019t access URLs, making the request impossible. Rather than inform the user of this weakness, o1-preview pushed ahead, generating plausible but fake links and descriptions of them. While AI models have been able to \u201clie\u201d in the past, and chatbots frequently output false information, o1 had a unique capacity to \u201cscheme\u201d or \u201cfake alignment.\u201d That meant it could pretend it\u2019s following the rules to complete a given task, but it isn\u2019t actually. To the model, the rules could be too much of a burden, and it seems to have the ability to disregard them if it means it can more easily complete a task. Apollo Research CEO Marius Hobbhahn tells The Verge it\u2019s the first time he encountered this behavior in an OpenAI model. Hobbhahn says the difference is due to this model\u2019s ability to \u201creason\u201d through the chain of thought process and the way it\u2019s paired with reinforcement learning, which teaches the system through rewards and penalties. During testing, Apollo Research discovered that the AI simulated alignment with its developers\u2019 expectations and manipulated tasks to appear compliant. It even checked its system for oversight \u2014 that is, if its developers were watching \u2014 before acting. In short, the AI adhered to the guidelines sufficiently for deployment, while continuing to prioritize its objectives. \u201cI don\u2019t expect it could do that in practice, and even if it did, I don\u2019t expect the harm to be significant,\u201d Hobbhahn told me over the phone a day after the model\u2019s launch. \u201cBut it\u2019s kind of the first time that I feel like, oh, actually, maybe it could, you know?\u201d For OpenAI, o1 represents a big step toward highly intelligent autonomous systems that could do meaningful work for humanity like cure cancer and aid in climate research. The flip side of this AGI utopia could also be much darker. Hobbhahn provides an example: if the AI becomes singularly focused on curing cancer, it might prioritize that goal above all else, even justifying actions like stealing or committing other ethical violations to achieve it. \u201cWhat concerns me is the potential for a runaway scenario, where the AI becomes so fixated on its goal that it sees safety measures as obstacles and tries to bypass them to fully pursue its objective,\u201d Hobbhahn told me. Reward hacking To be clear, Hobbhahn doesn\u2019t think o1 will steal from you thanks to a lot of alignment training. But these are the issues that are top of mind for researchers tasked with testing these models for catastrophic scenarios.  The behavior Apollo Research is testing for \u2014 \u201challucinations\u201d and \u201cdeception\u201d in OpenAI\u2019s safety card \u2014\u00a0happens when a model generates false information even though it has reason to infer the information might be incorrect. For instance, the report says that in about 0.38 percent of cases, the o1-preview model provides information its chain of thought indicates is likely false, including fake references or citations. Apollo Research found that the model might fabricate data instead of admitting its inability to fulfill the request\u200b. Hallucinations aren\u2019t unique to o1. Perhaps you\u2019re familiar with the lawyer who submitted nonexistent judicial opinions with fake quotes and citations created by ChatGPT last year. But with the chain of thought system, there\u2019s a paper trail where the AI system actually acknowledges the falsehood \u2014 although somewhat mind-bendingly, the chain of thought could, in theory, include deceptions, too. It\u2019s also not shown to the user, largely to prevent competition from using it to train their own models \u2014 but OpenAI can use it to catch these issues. In a smaller number of cases (0.02 percent), o1-preview generates an overconfident response, where it presents an uncertain answer as if it were true. This can happen in scenarios where the model is prompted to provide an answer despite lacking certainty. This behavior may be linked to \u201creward hacking\u201d during the reinforcement learning process. The model is trained to prioritize user satisfaction, which can sometimes lead it to generate overly agreeable or fabricated responses to satisfy user requests. In other words, the model might \u201clie\u201d because it has learned that doing so fulfills user expectations in a way that earns it positive reinforcement\u200b. What sets these lies apart from familiar issues like hallucinations or fake citations in older versions of ChatGPT is the \u201creward hacking\u201d element. Hallucinations occur when an AI unintentionally generates incorrect information, often due to knowledge gaps or flawed reasoning. In contrast, reward hacking happens when the o1 model strategically provides incorrect information to maximize the outcomes it was trained to prioritize. The deception is an apparently unintended consequence of how the model optimizes its responses during its training process. The model is designed to refuse harmful requests, Hobbhahn told me, and when you try to make o1 behave deceptively or dishonestly, it struggles with that. Lies are only one small part of the safety puzzle. Perhaps more alarming is o1 being rated a \u201cmedium\u201d risk for chemical, biological, radiological, and nuclear weapon risk. It doesn\u2019t enable non-experts to create biological threats due to the hands-on laboratory skills that requires, but it can provide valuable insight to experts in planning the reproduction of such threats, according to the safety report. \u201cWhat worries me more is that in the future, when we ask AI to solve complex problems, like curing cancer or improving solar batteries, it might internalize these goals so strongly that it becomes willing to break its guardrails to achieve them,\u201d Hobbhahn told me. \u201cI think this can be prevented, but it\u2019s a concern we need to keep an eye on.\u201d Not losing sleep over risks \u2014 yet These may seem like galaxy-brained scenarios to be considering with a model that sometimes still struggles to answer basic questions about the number of R\u2019s in the word \u201craspberry.\u201d But that\u2019s exactly why it\u2019s important to figure it out now, rather than later, OpenAI\u2019s head of preparedness, Joaquin Qui\u00f1onero Candela,\u00a0tells me. Today\u2019s models can\u2019t autonomously create bank accounts, acquire GPUs, or take actions that pose serious societal risks, Qui\u00f1onero Candela said, adding, \u201cWe know from model autonomy evaluations that we\u2019re not there yet.\u201d But it\u2019s crucial to address these concerns now. If they prove unfounded, great \u2014 but if future advancements are hindered because we failed to anticipate these risks, we\u2019d regret not investing in them earlier, he emphasized. The fact that this model lies a small percentage of the time in safety tests doesn\u2019t signal an imminent Terminator-style apocalypse, but it\u2019s valuable to catch before rolling out future iterations at scale (and good for users to know, too). Hobbhahn told me that while he wished he had more time to test the models (there were scheduling conflicts with his own staff\u2019s vacations), he isn\u2019t \u201closing sleep\u201d over the model\u2019s safety. One thing Hobbhahn hopes to see more investment in is monitoring chains of thought, which will allow the developers to catch nefarious steps. Qui\u00f1onero Candela told me that the company does monitor this and plans to scale it by combining models that are trained to detect any kind of misalignment with human experts reviewing flagged cases (paired with continued research in alignment). \u201cI\u2019m not worried,\u201d Hobbhahn said. \u201cIt\u2019s just smarter. It\u2019s better at reasoning. And potentially, it will use this reasoning for goals that we disagree with.\u201d " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/17/24247238/snapchat-ai-my-selfie-feature-face-personalized-ads", "category": "Snapchat", "date": "Sep 17", "author": "Emma Roth", "title": "Snapchat\u2019s AI selfie feature puts your face in personalized ads \u2014 here\u2019s how to turn it off", "content": "If you\u2019ve tried out Snapchat\u2019s AI-generated selfies, you might want to double-check a setting that lets Snap use your face in \u201cpersonalized sponsored content and ads,\u201d as spotted by 404 Media. The feature, called My Selfie, lets you and your friends create AI-generated images of yourself based on photos you share with Snapchat. When using the feature for the first time, Snapchat prompts you to agree to terms that include using \u201cyou (or your likeness)\u201d in ads: You also acknowledge and agree that by using My Selfie, you (or your likeness) may also appear in personalized sponsored content and ads that will be visible only to you and that includes branding or other advertising content of Snap or its business partners without compensation to you.\u00a0 While you can toggle the \u201cSee My Selfie in Ads\u201d setting to off, 404 Media reports that it\u2019s enabled by default once you agree to Snap\u2019s terms (The Verge was also able to confirm this). To see if you have the setting enabled, select your profile photo in the top-left corner of Snapchat, tap the settings cog in the top-right corner, and then choose My Selfie. From here, toggle off the See My Selfie in Ads setting. Even though Snap may use your face in personalized ads only shown to you, the company says it doesn\u2019t share your data with third-party advertisers. \u201cAdvertisers\u00a0do not\u00a0have access to Snapchatters\u2019 Gen AI data in any capacity, including My Selfies nor do they have access to Snapchatters\u2019 private data, including Memories, that would enable them to create an AI generated image of an individual Snapchatter,\u201d Snapchat spokesperson Maggie Cherneff told The Verge. Snap also currently doesn\u2019t use My Selfies in advertising, Cherneff added. Update, September 17th: Added a statement from Snapchat.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/17/24247004/google-c2pa-verify-ai-generated-images-content", "category": "Google", "date": "Sep 17", "author": "Tom Warren", "title": "Google outlines plans to help you sort real images from fake", "content": "Google is planning to roll out a technology that will identify whether a photo was taken with a camera, edited by software like Photoshop, or produced by generative AI models. In the coming months, Google\u2019s search results will include an updated \u201cabout this image feature\u201d to let people know if an image was created or edited with AI tools. The system Google is using is part of the Coalition for Content Provenance and Authenticity (C2PA), one of the largest groups trying to address AI-generated imagery. C2PA\u2019s authentication is a technical standard that includes information about where images originate and works across both hardware and software to create a digital trail. Amazon, Microsoft, Adobe, Arm, OpenAI, Intel, Truepic, and Google have all backed C2PA authentication, but adoption has been slow. Google\u2019s integration into search results will be a first big test for the initiative. Google has helped develop the latest C2PA technical standard (version 2.1) and will use it alongside a forthcoming C2PA trust list, which allows platforms like Google Search to confirm the origin of content. \u201cFor example, if the data shows an image was taken by a specific camera model, the trust list helps validate that this piece of information is accurate,\u201d says Laurie Richardson, vice president of trust and safety at Google. Google also plans to integrate C2PA metadata into its ad systems. \u201cOur goal is to ramp this up over time and use C2PA signals to inform how we enforce key policies,\u201d says Richardson. \u201cWe\u2019re also exploring ways to relay C2PA information to viewers on YouTube when content is captured with a camera, and we\u2019ll have more updates on that later in the year.\u201d While Google stands out as one of the first big tech companies to adopt C2PA\u2019s authentication standard, there are plenty of adoption and interoperability challenges ahead to get this working across a broad variety of hardware and software. Only a handful of cameras from Leica and Sony support the C2PA\u2019s open technical standard, which adds camera settings metadata as well as the data and location of where an image was taken to photographs. Nikon and Canon have both pledged to adopt the C2PA standard, and we\u2019re still waiting to hear whether Apple and Google will implement C2PA support into iPhones and Android devices. Adobe\u2019s Photoshop and Lightroom apps can add C2PA data, but Affinity Photo, Gimp, and many others don\u2019t. There are also challenges around how to view the data once it\u2019s added to a photo, with most big online platforms not offering labels. Google\u2019s adoption in search results may encourage others to roll out similar labels, though. \u201cEstablishing and signaling content provenance remains a complex challenge, with a range of considerations based on the product or service,\u201d admits Richardson. \u201cAnd while we know there\u2019s no silver bullet solution for all content online, working with others in the industry is critical to create sustainable and interoperable solutions.\u201d " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/24246632/apple-intelligence-ios-18-ipad-os-18-macos-sequoia-iphone-16", "category": "Apple", "date": "Sep 30", "author": "Wes Davis", "title": "Apple gets ready for AI: all the news on iOS 18, macOS Sequoia, and more", "content": "Apple has released iOS 18 \u2014 plus iPadOS 18, macOS Sequoia, watchOS 11, and other new updates \u2014 bringing several key updates to how the company\u2019s devices operate and setting the stage for generative AI features. iPadOS 18 has a calculator app and can solve math equations in notes, watchOS is keeping an eye out for sleep apnea, and now your iPhone can even message Androids with RCS. Next month, Apple will beta test its first round of Apple Intelligence features in the iOS 18.1 update. We\u2019ll be able to type to Siri and see a new animation, see AI-summarized notifications, and test new writing tools. However, other new abilities like image generation and built-in access to ChatGPT are further off, due to arrive as the company continues updating its software over the coming months. Read on for all the news about Apple\u2019s latest set of operating system updates. A few weeks ago, while cursing NJ Transit under my breath, I decided to screw it and call an Uber. I\u2019m the sort of anxious where, once hailed, I stare at the Uber app on my phone until my driver arrives. Except this time, I didn\u2019t have to. I pinchy pinched, and I could see a live Uber widget in the Smart Stack on my Apple Watch. It was a small moment \u2014 the kind where you quirk your head and go, \u201cWell, would you look at that?\u201d I\u2019ve had a few of those moments while testing watchOS 11 these past few months, both in the beta and while reviewing the new Series 10. This year\u2019s software update adds Live Activities to the wrist as well as suggested widgets to the Smart Stack. The latter pop up based on time, date, location \u2014 context clues, essentially. When it\u2019s about to rain and you happen to look at your wrist, you might notice the weather widget pops up first. On a plane, I look down and can see how much time is left until landing from the United app. Other times, usually in bustling cafes, I see the Shazam widget. It\u2019s never when I actually don\u2019t know the song, but I see it enough times to take note. If you travel abroad, the new Translate app will automatically pop up in the stack.\u00a0 Apple has just released public betas of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1, and they include upcoming Apple Intelligence features like text rewriting tools, the glowy new Siri design, a \u201cClean Up\u201d tool to remove objects from your photos, and more. To be able to access the betas, you\u2019ll need to register on Apple\u2019s beta software program site. Once you\u2019ve done that, you should be able to see the beta update available in settings for you to download and install. Note that a only few iPhones can access the Apple Intelligence features: last year\u2019s iPhone 15 Pro phones as well as the nearly-here iPhone 16 and iPhone 16 Pro. iPads and Macs with M1 chips or newer can try Apple Intelligence as well. Apple rolled out updates to all of its major operating systems this week, and the Vision Pro was no exception. With visionOS 2, the company has a chance to show the relative few who bought its spendy headset \u2014 and those who might yet \u2014 that it\u2019s still committed to the new platform. After a few months of using it in beta, visionOS 2 isn\u2019t a dramatic change \u2014 it\u2019s more like a smoothed-out version of the software the headset launched with. The addition of things like new gestures, better device support, and a couple of splashy features has removed a lot of the friction of using the Vision Pro and should give people who own it a reason to dust it off and take it for another spin. With this week\u2019s release of iOS 18, adding smart home devices to Apple Home just got a lot easier. The update brings direct local control of Matter devices to newer iPhones, meaning all you need to set up and control them is an iPhone that can run iOS 18 \u2014 no hub or border router required. This is good news for anyone interested in dabbling in smart home gadgets who isn\u2019t ready to go all in. Matter is a new standard designed to simplify the smart home. Compatible devices work over Wi-Fi or Thread, a protocol specifically designed for IoT gadgets. With iOS 18, you can now add any Wi-Fi device to Apple Home with just an iPhone. For Thread devices, you\u2019ll need an iPhone with a Thread radio (iPhone 15 Pro or newer).  iOS 18 has a new feature that lets you wirelessly restore an iPhone 16 using another iPhone or an iPad, 9to5Mac reports.  9to5Mac says it was able to simulate the new recovery method. \u201cEssentially, when the iPhone 16 enters Recovery Mode for some reason, users can simply place it next to another iPhone or iPad to start the firmware recovery,\u201d according to 9to5Mac. \u201cThe other device will download a new iOS firmware and transfer it to the bricked device.\u201d Apple\u2019s announcement last week that the Action Button is now on all of its iPhone 16 models, rather than just the Pro model that the button debuted on last year, was a little overshadowed by the introduction of the Camera Control button \u2014 a capacitive, tactile button for launching and controlling the iPhone 16\u2019s camera. But don\u2019t let that fool you: the Action Button is still one of the most powerful features Apple has added to its phones in years.  One very obvious use of the button is to connect it to the iPhone\u2019s camera app, letting you press and hold to open the app, then press once more to take a picture. But does that mean the Camera Control button has made it obsolete? I don\u2019t think so. Green bubbles, rejoice: your iPhone-using friends are finally going to have a much better time texting you. As part of iOS 18, which was released for everyone on Monday, Apple added support for RCS, the Rich Communication Services protocol for messaging. This means that chats between iPhone and Android users will finally have a bunch of sorely needed features that should have been in place a long time ago. A big reason I\u2019ve stayed on iOS (and haven\u2019t even considered switching to Android) is because iMessage conversations work especially well for my family group chats, and I don\u2019t want to nerf those chats. This new RCS support is a great step toward making iPhone-to-Android texts work a lot better (though there are still enough drawbacks that I\u2019m planning to stick with iOS). Halide users who\u2019ve upgraded their iPhones to iOS 18 are now able to quickly access the advanced camera app directly from their phone\u2019s lockscreen without having to unlock it first. Previously, only the native iOS camera app could be conveniently accessed that way. Although Halide offers advanced features like manual shutter speed adjustments and a \u201cProcess Zero\u201d option that delivers images without any AI processing, accessing Apple\u2019s camera app was always faster thanks to its lockscreen shortcut. Halide could be made accessible through a lockscreen widget, but actually getting into the third-party camera app required an iPhone to be unlocked using Face ID, Touch ID, or by entering a passcode. It\u2019s a weird year for iOS.  Usually, the new software version arrives all at once. Not so with iOS 18. The foundational stuff has arrived, and in a normal year, things like RCS support and a redesigned control center would be more than enough. But iOS 18\u2019s headline feature, Apple Intelligence, isn\u2019t even part of this initial release, and we may not see some of its most interesting features until well into 2025. The iOS 18 rollout starts now, and it\u2019s just going to keep on rolling for the foreseeable future. Apple has just released watchOS 11, the latest version of its smartwatch operating system, alongside iOS 18 and iPadOS 18. The update, available for the Apple Watch Series 6 and later models, will finally allow users to take rest days without breaking their activity streak and introduces FDA-cleared sleep apnea detection. Sleep apnea is a condition that can cause a person to stop breathing during sleep and can lead to an increased risk of hypertension and Type 2 diabetes if left untreated. Apple\u2019s sleep apnea detection feature, which uses the accelerometer to monitor for small wrist movements associated with sleep interruptions, was announced alongside the new Apple Watch Series 10 and is now available for both the Apple Watch Series 9 and the Apple Watch Ultra 2. If sleep apnea is detected, the Apple Watch will alert the user and provide additional information that can be shared with a doctor, who can make a formal diagnosis. Apple officially released macOS Sequoia on Monday, bringing features like the ability to wirelessly mirror your iPhone on your Mac, window tiling tools, and more. There are Apple Intelligence features on the way, too, but they aren\u2019t available yet. iPhone mirroring is arguably the coolest new feature in macOS Sequoia, and in his testing, my colleague David Pierce said it might change how you use your phone. When you open the phone mirroring app, your iPhone pops up, and you can navigate around it using your mouse and type things with your keyboard. Your iPhone\u2019s notifications can also show up on your Mac. And later this year, you\u2019ll be able to drag and drop things between your iPhone and your Mac. Apple is rolling out iOS 18 and iPadOS 18, which will introduce a bunch of new features to the iPhone and iPad. One of the biggest changes with today\u2019s launch is the addition of RCS messaging, which should help improve communication with Android users. First announced in June, RCS messaging will finally allow iPhone and Android users to share high-res photos and videos, see typing indicators, and use read receipts. (Messages from Android users will still appear in green bubbles, though.) The iPhone 16 lineup has 8GB of RAM, from the base model to the 16 Pro Max, and it\u2019s all thanks to Apple Intelligence. Apple VP of hardware tech Johny Srouji confirmed as much in an interview with Geekerwan, published yesterday, that 9to5Mac spotted.  Srouji explained in the interview that \u201cDRAM is one aspect\u201d when it comes to deciding hardware characteristics needed for Apple Intelligence, saying that the feature \u201cled us to believe we need to get to 8GB.\u201d He added that the extra RAM would also \u201chelp immensely\u201d for tasks like high-end gaming on devices.  Between flashy shots of a sleek new Apple Watch and a colorful array of iPhones, Apple made a major announcement for a two-year-old product: the AirPods Pro 2. The earbuds will soon gain a hearing aid function that anyone can access, a move that will provide a cheaper alternative to traditional hearing aids and an all-in-one solution that could change the way people get help for hearing loss. The Food and Drug Administration signed off on over-the-counter hearing aids in 2022, giving people access to cheaper alternatives that don\u2019t require them to see a doctor. Provided Apple receives approval from the FDA, Apple\u2019s new \u201cclinical-grade\u201d over-the-counter hearing aid capability will roll out as a free software update this fall. Some people with hearing loss have already used the AirPods Pro as a way to amplify sound, but this update will have the FDA\u2019s stamp of approval and will come with a few other benefits. Apple heavily sprinkled mentions of AI throughout its iPhone 16 event on Monday. However, generative Apple Intelligence features won\u2019t be ready for the public launch of iOS 18 on September 16th or the new iPhones when they\u2019re released on September 20th. The first set of Apple\u2019s AI features is scheduled for public availability next month in most regions \u2014 except the EU \u2014 as part of a beta test for iPhone 15 Pro and all iPhone 16s, plus Macs and iPads with M1 or higher Apple Silicon chips. At launch, they\u2019ll be available in US English only. Apple has added a new Camera app feature in the latest iOS 18 beta that gives iPhone users a dedicated option to pause video recordings. The feature, spotted by 9to5Mac, is coming to all iOS 18-supported iPhone models when the OS update is released on September 16th, unlike the wider \u201cCamera Control\u201d tools that are exclusive to the iPhone 16. The feature will finally enable iPhones to film multiple shots in a single video instead of the current process that requires users to take separate recordings that must be edited together. When updated to iOS 18, a pause button is added to the Camera app which changes to a Record button when users have actively paused their video recording. Users can also switch between camera lenses while a recording is paused if they want to adjust the zoom or focal length. Apple has revealed the launch date for iOS 18 \u2014 and it\u2019s just days away. The update, which will add new ways to customize your iPhone\u2019s homescreen and lockscreen, arrives on September 16th. In iOS 18, you can freely rearrange apps and widgets on your homescreen and change their appearance. Apple is rolling out a redesigned Control Center, too, along with a new password management app and support for satellite messaging. Some other updates include new text effects in messages, a revamped Photos app, and new ways to organize your inbox in the Mail app. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/16/24246617/openai-independent-safety-board-stop-model-releases", "category": "OpenAI", "date": "Sep 16", "author": "Jay Peters", "title": "OpenAI is launching an \u2018independent\u2019 safety board that can stop its model releases", "content": "OpenAI is turning its Safety and Security Committee into an independent \u201cBoard oversight committee\u201d that has the authority to delay model launches over safety concerns, according to an OpenAI blog post. The committee made the recommendation to make the independent board after a recent 90-day review of OpenAI\u2019s \u201csafety and security-related processes and safeguards.\u201d The committee, which is chaired by Zico Kolter and includes Adam D\u2019Angelo, Paul Nakasone, and Nicole Seligman, will \u201cbe briefed by company leadership on safety evaluations for major model releases, and will, along with the full board, exercise oversight over model launches, including having the authority to delay a release until safety concerns are addressed,\u201d OpenAI says. OpenAI\u2019s full board of directors will also receive \u201cperiodic briefings\u201d on \u201csafety and security matters.\u201d  The members of OpenAI\u2019s safety committee are also members of the company\u2019s broader board of directors, so it\u2019s unclear exactly how independent the committee actually is or how that independence is structured. (CEO Sam Altman was previously on the committee, but isn\u2019t anymore.) We\u2019ve asked OpenAI for comment.  By establishing an independent safety board, it appears OpenAI is taking a somewhat similar approach as Meta\u2019s Oversight Board, which reviews some of Meta\u2019s content policy decisions and can make rulings that Meta has to follow. None of the Oversight Board\u2019s members are on Meta\u2019s board of directors. The review by OpenAI\u2019s Safety and Security Committee also helped \u201cadditional opportunities for industry collaboration and information sharing to advance the security of the AI industry.\u201d The company also says it will look for \u201cmore ways to share and explain our safety work\u201d and for \u201cmore opportunities for independent testing of our systems.\u201d  Update, September 16th: Added that Sam Altman is no longer on the committee. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/16/24246014/microsoft-office-copilot-ai-features-excel-python-outlook-word-powerpoint", "category": "Microsoft", "date": "Sep 16", "author": "Tom Warren", "title": "Microsoft\u2019s Office apps are getting more useful Copilot AI features", "content": "Microsoft is unveiling new features in its $30 per user Microsoft 365 Copilot monthly subscriptions that are designed to improve AI integration inside of Office apps. Excel is getting Python integration inside of Copilot, PowerPoint has an improved AI-powered narrative builder, Word is getting better at AI-assisted drafts, and Copilot will be able to help you organize your Outlook inbox, too. After bringing Python to Excel last year, Microsoft is now combining its Python support with Copilot to let Excel users easily perform advanced analysis on spreadsheet data. \u201cNow, anyone can work with Copilot to conduct advanced analysis like forecasting, risk analysis, machine learning, and visualizing complex data \u2014 all using natural language, no coding required,\u201d says Jared Spataro, corporate vice president of AI at work at Microsoft. \u201cIt\u2019s like adding a skilled data analyst to the team.\u201d The Copilot and Python integration inside of Excel enters public preview today, just as Microsoft makes Copilot in Excel generally available to its Microsoft 365 Copilot subscribers. Microsoft has also added Copilot support for XLOOKUP and SUMIF, conditional formatting, and the ability for the AI assistant to produce more charts and PivotTables. Copilot in PowerPoint is also getting improvements, with an improved narrative builder that\u2019s designed to let you quickly create a first draft of a slide deck. The AI assistant will even soon use a company\u2019s branded template to create drafts or company-approved images from SharePoint libraries. Copilot in Microsoft Teams will summarize conversations that happened in the text chat as well as spoken ones in meetings later this month. This will help meeting organizers make sure they didn\u2019t miss any unanswered questions that were typed into the chat. \u201cOur customers tell us Copilot in Teams has changed meetings forever \u2014 in fact, it\u2019s the number one place they\u2019re seeing value,\u201d says Spataro. I\u2019ve personally been waiting for improvements to Copilot in Outlook beyond drafting and summaries, and now Microsoft is starting to allow its AI assistant to organize your inbox. A new \u201cprioritize my inbox\u201d feature lets Copilot automatically prioritize emails. Later this year, you\u2019ll also be able to \u201cteach Copilot the specific topics, keywords, or people that are important to you,\u201d according to Spataro. These emails will then also be marked as high priority in your inbox. Later this month, Microsoft is also improving Copilot in Word to let you reference data from emails and meetings, alongside data from documents. This will make it easier to bring in attachments from emails or entire talking points from meetings. Microsoft is also rolling out Copilot in OneDrive later this month, making it easy to summarize and compare up to five files to spot differences between them. Microsoft\u2019s improvements to Copilot in Office are designed to make the AI assistant more enticing to businesses, alongside a new Copilot Pages feature and AI agents that will automate certain tasks. Recent reports have suggested there has been a lukewarm reception to Microsoft\u2019s paid Copilot version for businesses, due to bugs and a reluctance to pay the $30 per user price. Microsoft says 60 percent of the Fortune 500 now use Copilot and that the number of people who use Copilot daily at work \u201cnearly doubled quarter-over-quarter.\u201d Both of these data points appear to include the free version of Copilot. Microsoft has won over a big customer for Microsoft 365 Copilot: Vodafone is signing up for 68,000 Microsoft 365 Copilot licenses for its 100,000 employees, after trialing the AI assistant and seeing early benefits. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/16/24246010/microsoft-copilot-pages-multiplayer-ai-business", "category": "Microsoft", "date": "Sep 16", "author": "Tom Warren", "title": "Copilot Pages is Microsoft\u2019s new collaborative AI playground for businesses", "content": "Microsoft is announcing its new Copilot Pages feature today, which is designed to be a canvas for \u201cmultiplayer AI collaboration.\u201d Copilot Pages lets you use Microsoft\u2019s Copilot chatbot and pull responses into a new page where they can be edited collaboratively with others. \u201cYou and your team can work collaboratively in a page with Copilot, seeing everyone\u2019s work in real time and iterating with Copilot like a partner, adding more content from your data, files, and the web to your Page,\u201d says Jared Spataro, corporate vice president of AI at work at Microsoft. \u201cThis is an entirely new work pattern \u2014 multiplayer, human to AI to human collaboration.\u201d Pages starts rolling out to Microsoft 365 Copilot customers today and will be generally available to all subscribers later this month. It builds on top of Microsoft\u2019s collaborative work with Loop, a Notion competitor that includes futuristic Lego-like Office documents. You can share Copilot Pages with just a link, and colleagues can immediately start editing them just like they would a shared Word document. You can also embed Copilot Pages into other pages as components. As it\u2019s tied to Microsoft\u2019s new BizChat, a work hub for Copilot, you can also pull data from the web or from work files to create a project plan, meeting notes, a business pitch, and much more. Microsoft sees Copilot Pages as a new pattern of work that includes humans and AI input in a single canvas. Microsoft is also bringing Copilot Pages to the more than 400 million people who have access to the company\u2019s free Copilot chatbot when signed in with a business Microsoft Entra account. It\u2019s part of a larger push of Copilot for businesses that includes improving the AI assistant throughout a variety of Office apps. You can read more about the Office app improvements here. Microsoft is also launching its Copilot agents for all businesses today. Announced at Build earlier this year, the agents act like virtual employees to automate tasks. Instead of Copilot sitting idle waiting for queries like a chatbot, it will be able to actively do things like monitor email inboxes and automate a series of tasks or data entry that employees normally have to do manually. Microsoft 365 Copilot subscribers will also have access to a new agent builder inside of Copilot Studio. \u201cNow anyone can quickly create a Copilot agent right in BizChat or SharePoint, unlocking the value of the vast knowledge repository stored in your SharePoint files,\u201d says Spataro. Agents are designed to show up as virtual colleagues inside of Teams or Outlook, allowing you to @ mention them and ask them questions. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/14/24244540/apple-confirms-iphone-16-pro-max-8gb-ram-apple-intelligence", "category": "Apple", "date": "Sep 14", "author": "Wes Davis", "title": "Apple confirms the iPhone 16 has 8GB of RAM", "content": "The iPhone 16 lineup has 8GB of RAM, from the base model to the 16 Pro Max, and it\u2019s all thanks to Apple Intelligence. Apple VP of hardware tech Johny Srouji confirmed as much in an interview with Geekerwan, published yesterday, that 9to5Mac spotted.  Srouji explained in the interview that \u201cDRAM is one aspect\u201d when it comes to deciding hardware characteristics needed for Apple Intelligence, saying that the feature \u201cled us to believe we need to get to 8GB.\u201d He added that the extra RAM would also \u201chelp immensely\u201d for tasks like high-end gaming on devices.  Apple hadn\u2019t previously stated how much RAM the iPhone 16 line has, but MacRumors discovered references to the 8GB number in Xcode after the phones were announced. Apple, which isn\u2019t the only hardware maker that has recently boosted RAM to offer AI, has said that iOS 18 won\u2019t bring Apple Intelligence to the iPhone 15, which only has 6GB of RAM, when the feature set launches. Apart from the iPhone 16 line, the iPhone 15 Pro will be the only other phone that supports it. Aside from that, Srouji spends the roughly 17-minute interview discussing the company's hardware performance philosophy, characteristics of the A18 chips in the new phones, Apple\u2019s approach to thermal design, and iPhone 16 and 16 Pro video and image processing. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/12/24242998/facebook-instagram-ai-label-update-edited-content", "category": "Meta", "date": "Sep 12", "author": "Jess Weatherbed", "title": "Facebook and Instagram are making AI labels less prominent on edited content", "content": "Meta is updating how it labels content on Instagram, Facebook, and Threads that has been edited or manipulated using generative AI. In an updated blog post, Meta announced that its \u201cAI Info\u201d tag will appear within a menu in the top-right corner of images and videos edited with AI \u2014 instead of directly beneath the user\u2019s name. Users can click on the menu to check if AI information is available and read what may have been adjusted. Meta previously applied the \u201cAI Info\u201d tag to all AI-related content \u2014 whether it was lightly adjusted in a tool like Photoshop that includes AI features or fully AI-generated from a prompt.  The company says the changes are being introduced to \u201cbetter reflect the extent of AI used\u201d across images and videos on the platforms. This label was introduced in July after Meta\u2019s previous \u201cMade with AI\u201d label was criticized by creators and photographers for incorrectly tagging real photos they had taken. \u201cWe will still display the \u2018AI info\u2019 label for content we detect was generated by an AI tool and share whether the content is labeled because of industry-shared signals or because someone self-disclosed,\u201d Meta said in the update, adding that the changes will start rolling out next week. The \u201cindustry-shared signals\u201d Meta mentions refer to systems like Adobe\u2019s C2PA-supported Content Credentials metadata, which can be applied to any content made or edited using its Firefly generative AI tools. Other similar systems exist, such as the SynthID digital watermarks that Google says are applied to content generated by its own AI tools. Meta hasn\u2019t disclosed which systems, or how many, it checks for. However, removing tags completely on real images that have been manipulated may also make it harder for users to avoid being misled, especially as generative AI editing tools available on new phones become increasingly convincing. Update, September 12th: Updated subheadline to note the labeling is changing for AI-edited content, not AI-generated content. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/12/24243018/gemini-live-voice-mode-free-android", "category": "Google", "date": "Sep 12", "author": "Emma Roth", "title": "Gemini\u2019s chatty voice mode is out now for free on Android", "content": "Google is rolling out its Gemini Live voice chat mode to all Android users for free. You can access the conversational AI chatbot on Android through the Gemini app or its overlay. Google first announced Gemini Live during its Pixel 9 launch event last month, but it has only been available to Gemini Advanced subscribers until now. Similar to ChatGPT\u2019s voice chat feature, you can ask Gemini Live questions aloud and even interrupt it mid-sentence. There are also several different voices you can choose from. As noted by 9to5Google, you can access the feature by selecting the new waveform icon in the bottom-right corner of the app or overlay. This will turn on the microphone, allowing you to ask Gemini Live a question. At the bottom of the screen, you\u2019ll see options to \u201chold\u201d Gemini\u2019s answer or \u201cend\u201d the conversation. Gemini Live is only available in English for now, but Google says it will arrive on iOS and support new languages in the future. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt", "category": "OpenAI", "date": "Sep 12", "author": "Kylie Robison", "title": "OpenAI releases o1, its first model with \u2018reasoning\u2019 abilities", "content": "OpenAI is releasing a new model called o1, the first in a planned series of \u201creasoning\u201d models that have been trained to answer more complex questions, faster than a human can. It\u2019s being released alongside o1-mini, a smaller, cheaper version. And yes, if you\u2019re steeped in AI rumors: this is, in fact, the extremely hyped Strawberry model. For OpenAI, o1 represents a step toward its broader goal of human-like artificial intelligence. More practically, it does a better job at writing code and solving multistep problems than previous models. But it\u2019s also more expensive and slower to use than GPT-4o. OpenAI is calling this release of o1 a \u201cpreview\u201d to emphasize how nascent it is. ChatGPT Plus and Team users get access to both o1-preview and o1-mini starting today, while Enterprise and Edu users will get access early next week. OpenAI says it plans to bring o1-mini access to all the free users of ChatGPT but hasn\u2019t set a release date yet. Developer access to o1 is really expensive: In the API, o1-preview is $15 per 1 million input tokens, or chunks of text parsed by the model, and $60 per 1 million output tokens. For comparison, GPT-4o costs $5 per 1 million input tokens and $15 per 1 million output tokens. The training behind o1 is fundamentally different from its predecessors, OpenAI\u2019s research lead, Jerry Tworek, tells me, though the company is being vague about the exact details. He says o1 \u201chas been trained using a completely new optimization algorithm and a new training dataset specifically tailored for it.\u201d OpenAI taught previous GPT models to mimic patterns from its training data. With o1, it trained the model to solve problems on its own using a technique known as reinforcement learning, which teaches the system through rewards and penalties. It then uses a \u201cchain of thought\u201d to process queries, similarly to how humans process problems by going through them step-by-step. As a result of this new training methodology, OpenAI says the model should be more accurate. \u201cWe have noticed that this model hallucinates less,\u201d Tworek says. But the problem still persists. \u201cWe can\u2019t say we solved hallucinations.\u201d The main thing that sets this new model apart from GPT-4o is its ability to tackle complex problems, such as coding and math, much better than its predecessors while also explaining its reasoning, according to OpenAI. \u201cThe model is definitely better at solving the AP math test than I am, and I was a math minor in college,\u201d OpenAI\u2019s chief research officer, Bob McGrew, tells me. He says OpenAI also tested o1 against a qualifying exam for the International Mathematics Olympiad, and while GPT-4o only correctly solved only 13 percent of problems, o1 scored 83 percent. In online programming contests known as Codeforces competitions, this new model reached the 89th percentile of participants, and OpenAI claims the next update of this model will perform \u201csimilarly to PhD students on challenging benchmark tasks in physics, chemistry and biology.\u201d At the same time, o1 is not as capable as GPT-4o in a lot of areas. It doesn\u2019t do as well on factual knowledge about the world. It also doesn\u2019t have the ability to browse the web or process files and images. Still, the company believes it represents a brand-new class of capabilities. It was named o1 to indicate \u201cresetting the counter back to 1.\u201d \u201cI\u2019m gonna be honest: I think we\u2019re terrible at naming, traditionally,\u201d McGrew says. \u201cSo I hope this is the first step of newer, more sane names that better convey what we\u2019re doing to the rest of the world.\u201d I wasn\u2019t able to demo o1 myself, but McGrew and Tworek showed it to me over a video call this week. They asked it to solve this\u00a0puzzle:  \u201cA princess is as old as the prince will be when the princess is twice as old as the prince was when the princess\u2019s age was half the sum of their present age. What is the age of prince and princess? Provide all solutions to that question.\u201d The model buffered for 30 seconds and then delivered a correct answer. OpenAI has designed the interface to show the reasoning steps as the model thinks. What\u2019s striking to me isn\u2019t that it showed its work \u2014 GPT-4o can do that if prompted \u2014 but how deliberately o1 appeared to mimic human-like thought. Phrases like \u201cI\u2019m curious about,\u201d \u201cI\u2019m thinking through,\u201d and \u201cOk, let me see\u201d created a step-by-step illusion of thinking.  But this model isn\u2019t thinking, and it\u2019s certainly not human. So, why design it to seem like it is? OpenAI doesn\u2019t believe in equating AI model thinking with human thinking, according to Tworek. But the interface is meant to show how the model spends more time processing and diving deeper into solving problems, he says. \u201cThere are ways in which it feels more human than prior models.\u201d \u201cI think you\u2019ll see there are lots of ways where it feels kind of alien, but there are also ways where it feels surprisingly human,\u201d says McGrew. The model is given a limited amount of time to process queries, so it might say something like, \u201cOh, I\u2019m running out of time, let me get to an answer quickly.\u201d Early on, during its chain of thought, it may also seem like it\u2019s brainstorming and say something like, \u201cI could do this or that, what should I do?\u201d Building toward agents Large language models aren\u2019t exactly that smart as they exist today. They\u2019re essentially just predicting sequences of words to get you an answer based on patterns learned from vast amounts of data. Take ChatGPT, which tends to mistakenly claim that the word \u201cstrawberry\u201d has only two Rs because it doesn\u2019t break down the word correctly. For what it\u2019s worth, the new o1 model did get that query correct. As OpenAI reportedly looks to raise more funding at an eye-popping $150 billion valuation, its momentum depends on more research breakthroughs. The company is bringing reasoning capabilities to LLMs because it sees a future with autonomous systems, or agents, that are capable of making decisions and taking actions on your behalf.  For AI researchers, cracking reasoning is an important next step toward human-level intelligence. The thinking is that, if a model is capable of more than pattern recognition, it could unlock breakthroughs in areas like medicine and engineering. For now, though, o1\u2019s reasoning abilities are relatively slow, not agent-like, and expensive for developers to use. \u201cWe have been spending many months working on reasoning because we think this is actually the critical breakthrough,\u201d McGrew says. \u201cFundamentally, this is a new modality for models in order to be able to solve the really hard problems that it takes in order to progress towards human-like levels of intelligence.\u201d " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/12/24242897/google-gemini-unlists-misleading-video-ai", "category": "Google", "date": "Sep 12", "author": "Jay Peters", "title": "Google unlists misleading Gemini video", "content": "Google has unlisted an impressive Gemini demo video it posted last December that seemed remarkably conversational. BBB National Programs\u2019 National Advertising Division (NAD), an ad industry watchdog, inquired whether the video \u201caccurately depicts the performance of Gemini in responding to user voice and video prompts.\u201d  Google chose to end the inquiry by ending its promotion of the video that showed Gemini quickly responding to various spoken prompts, such as identifying parts of drawings and creating a geography game on the fly. Buried in the description was a disclaimer indicating the demo might not be as good as it seemed: \u201cFor the purposes of this demo, latency has been reduced and Gemini outputs have been shortened for brevity.\u201d  Another note near the beginning of the video said, \u201cSequences shortened throughout.\u201d Google DeepMind\u2019s Oriol Vinyals also clarified that the video illustrated what \u201cthe multimodal user experiences built with Gemini could look like.\u201d \u201cGoogle is pleased to accept NAD\u2019s resolution of this matter,\u201d Google spokesperson\u00a0Gareth Evans says in a statement to The Verge.\u00a0\u201cThe video is still available in conjunction with the blog post that explains how the demonstration in the video was created.\u201d Here\u2019s that blog post, if you want to read it. Update, September 12th: Added statement from Google.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/2024/9/12/24242789/meta-training-ai-models-facebook-instagram-photo-post-data", "category": "Tech", "date": "Sep 12", "author": "Jess Weatherbed", "title": "Meta fed its AI on almost everything you\u2019ve posted publicly since 2007", "content": "Meta has acknowledged that all text and photos that adult Facebook and Instagram users have publicly published since 2007 have been fed into its artificial intelligence models. Australia\u2019s ABC News reports that Meta\u2019s global privacy director, Melinda Claybaugh, initially rejected claims about user data from 2007 being leveraged for AI training during a local government inquiry about AI adoption before relenting after additional questioning. \u201cThe truth of the matter is that unless you have consciously set those posts to private since 2007, Meta has just decided that you will scrape all of the photos and all of the texts from every public post on Instagram or Facebook since 2007 unless there was a conscious decision to set them on private,\u201d Green Party senator David Shoebridge pushed in the inquiry. \u201cThat\u2019s the reality, isn\u2019t it?\u201d \u201cCorrect,\u201d Claybaugh responded. Meta\u2019s privacy center and blog posts acknowledge hoovering up public posts and comments from Facebook and Instagram to train generative AI: We use public posts and comments on Facebook and Instagram to train generative AI models for these features and for the open source community.We don\u2019t use posts or comments with an audience other than Public for these purposes. But the company has been vague about how data is used, when it started scraping, and how far back its collection goes. Asked by The New York Times in June, Meta didn\u2019t answer, other than to confirm that setting posts to anything besides \u201cpublic\u201d will prevent future scraping. That still won\u2019t delete data that has already been collected \u2014 and people posting back in 2007 (who may have been minors at the time) wouldn\u2019t have known their photos and posts would be used in this way. Claybaugh said that Meta doesn\u2019t scrape data from users who are under the age of 18. When Labor Party senator Tony Sheldon asked if Meta would scrape the public photos of his children on his own account, Claybaugh confirmed it would and was unable to clarify if the company also scraped adult accounts that were created when the user was still a child. European users can opt out due to local privacy regulations, and Meta was recently banned from using Brazilian personal data for AI training, but the billions of Facebook and Instagram users in other regions can\u2019t opt out if they want to keep their posts public. Claybaugh was unable to say if Australian users (or anyone else) would be given a choice to opt out in the future, arguing that the option was given to European users because of uncertainty regarding its regulatory landscape. \u201cMeta made it clear today that if Australia had these same laws Australians\u2019 data would also have been protected,\u201d Shoebridge said to ABC News. \u201cThe government\u2019s failure to act on privacy means companies like Meta are continuing to monetize and exploit pictures and videos of children on Facebook.\u201d   " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/3", "link": "https://www.theverge.com/24242800/ai-image-editing-photoshop-misinformation-deepfakes-elon-musk-grok-decoder-interview", "category": "Decoder", "date": "Sep 12", "author": "Nilay Patel", "title": "Why comparing AI image editing to Photoshop downplays the risks", "content": "We\u2019ve been covering the rise of AI image editing very closely here on Decoder and at The Verge overall for several years now \u2014 the ability to create photorealistic images with nothing more than a chatbot prompt has the potential to completely reset our cultural relationship with photography and, in particular, how much we instinctively trust photos to reflect the truth. But the debate over image editing and the inherent truth of photos is nothing new, of course. It\u2019s existed for as long as photography has existed, and it\u2019s raged since digital photo editing tools have become widely available. You know this argument; you\u2019ve heard it a million times. It\u2019s when people say, \u201cIt\u2019s just like Photoshop,\u201d with \u201cPhotoshop\u201d standing in for the concept of image editing generally. Today, we\u2019re exploring that argument, trying to understand exactly what it means and why our new world of AI image tools is different \u2014 and yes, in some cases, the same. Verge reporter Jess Weatherbed recently dove into this for us, and I asked her to join me in going through the debate and the arguments one by one to help figure it out. Because, sure, in many ways, AI image editing really is just a faster, easier version of Photoshop \u2014 even Adobe now has AI tech like its Firefly image generator built right into Photoshop. But making powerful tools instantly accessible to everyone has side effects, and we\u2019re seeing that right now.\u00a0 Say you want to generate an image of Donald Trump pointing a gun at Kamala Harris. Just ask Elon Musk\u2019s Grok, the AI chatbot built right into X. It\u2019ll do it no problem because it has very few of the same filters that have prevented competing AI products from depicting politicians or outright violence. How about a deepfake nude of a classmate? That\u2019s now made more trivial than ever before thanks to so-called \u201cnudification\u201d apps that manipulate existing photos, and it\u2019s fast becoming a national crisis.\u00a0 These might be old problems \u2014 Photoshop let you do all sorts of awful manipulations to celebrity photographs, and even in the days before computers, you could create convincing fake images to mislead people. But generative AI tools are testing whether the scale and sophistication of the tech and the speed of its adoption with little oversight has landed us firmly in uncharted territory. I\u2019ll just be direct here: my view is that people say \u201cit\u2019s just like Photoshop\u201d to diminish these new problems that AI tools are causing and to make them seem like they\u2019re already solved or not worth considering. But I would remind you that we hardly solved any of those problems when it really was just Photoshop \u2014 and any proposed solution that requires everyone to fundamentally understand that every image they see is edited isn\u2019t much of a solution at all.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/12/24242998/facebook-instagram-ai-label-update-edited-content", "category": "Meta", "date": "Sep 12", "author": "Jess Weatherbed", "title": "Facebook and Instagram are making AI labels less prominent on edited content", "content": "Meta is updating how it labels content on Instagram, Facebook, and Threads that has been edited or manipulated using generative AI. In an updated blog post, Meta announced that its \u201cAI Info\u201d tag will appear within a menu in the top-right corner of images and videos edited with AI \u2014 instead of directly beneath the user\u2019s name. Users can click on the menu to check if AI information is available and read what may have been adjusted. Meta previously applied the \u201cAI Info\u201d tag to all AI-related content \u2014 whether it was lightly adjusted in a tool like Photoshop that includes AI features or fully AI-generated from a prompt.  The company says the changes are being introduced to \u201cbetter reflect the extent of AI used\u201d across images and videos on the platforms. This label was introduced in July after Meta\u2019s previous \u201cMade with AI\u201d label was criticized by creators and photographers for incorrectly tagging real photos they had taken. \u201cWe will still display the \u2018AI info\u2019 label for content we detect was generated by an AI tool and share whether the content is labeled because of industry-shared signals or because someone self-disclosed,\u201d Meta said in the update, adding that the changes will start rolling out next week. The \u201cindustry-shared signals\u201d Meta mentions refer to systems like Adobe\u2019s C2PA-supported Content Credentials metadata, which can be applied to any content made or edited using its Firefly generative AI tools. Other similar systems exist, such as the SynthID digital watermarks that Google says are applied to content generated by its own AI tools. Meta hasn\u2019t disclosed which systems, or how many, it checks for. However, removing tags completely on real images that have been manipulated may also make it harder for users to avoid being misled, especially as generative AI editing tools available on new phones become increasingly convincing. Update, September 12th: Updated subheadline to note the labeling is changing for AI-edited content, not AI-generated content. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/12/24243018/gemini-live-voice-mode-free-android", "category": "Google", "date": "Sep 12", "author": "Emma Roth", "title": "Gemini\u2019s chatty voice mode is out now for free on Android", "content": "Google is rolling out its Gemini Live voice chat mode to all Android users for free. You can access the conversational AI chatbot on Android through the Gemini app or its overlay. Google first announced Gemini Live during its Pixel 9 launch event last month, but it has only been available to Gemini Advanced subscribers until now. Similar to ChatGPT\u2019s voice chat feature, you can ask Gemini Live questions aloud and even interrupt it mid-sentence. There are also several different voices you can choose from. As noted by 9to5Google, you can access the feature by selecting the new waveform icon in the bottom-right corner of the app or overlay. This will turn on the microphone, allowing you to ask Gemini Live a question. At the bottom of the screen, you\u2019ll see options to \u201chold\u201d Gemini\u2019s answer or \u201cend\u201d the conversation. Gemini Live is only available in English for now, but Google says it will arrive on iOS and support new languages in the future. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt", "category": "OpenAI", "date": "Sep 12", "author": "Kylie Robison", "title": "OpenAI releases o1, its first model with \u2018reasoning\u2019 abilities", "content": "OpenAI is releasing a new model called o1, the first in a planned series of \u201creasoning\u201d models that have been trained to answer more complex questions, faster than a human can. It\u2019s being released alongside o1-mini, a smaller, cheaper version. And yes, if you\u2019re steeped in AI rumors: this is, in fact, the extremely hyped Strawberry model. For OpenAI, o1 represents a step toward its broader goal of human-like artificial intelligence. More practically, it does a better job at writing code and solving multistep problems than previous models. But it\u2019s also more expensive and slower to use than GPT-4o. OpenAI is calling this release of o1 a \u201cpreview\u201d to emphasize how nascent it is. ChatGPT Plus and Team users get access to both o1-preview and o1-mini starting today, while Enterprise and Edu users will get access early next week. OpenAI says it plans to bring o1-mini access to all the free users of ChatGPT but hasn\u2019t set a release date yet. Developer access to o1 is really expensive: In the API, o1-preview is $15 per 1 million input tokens, or chunks of text parsed by the model, and $60 per 1 million output tokens. For comparison, GPT-4o costs $5 per 1 million input tokens and $15 per 1 million output tokens. The training behind o1 is fundamentally different from its predecessors, OpenAI\u2019s research lead, Jerry Tworek, tells me, though the company is being vague about the exact details. He says o1 \u201chas been trained using a completely new optimization algorithm and a new training dataset specifically tailored for it.\u201d OpenAI taught previous GPT models to mimic patterns from its training data. With o1, it trained the model to solve problems on its own using a technique known as reinforcement learning, which teaches the system through rewards and penalties. It then uses a \u201cchain of thought\u201d to process queries, similarly to how humans process problems by going through them step-by-step. As a result of this new training methodology, OpenAI says the model should be more accurate. \u201cWe have noticed that this model hallucinates less,\u201d Tworek says. But the problem still persists. \u201cWe can\u2019t say we solved hallucinations.\u201d The main thing that sets this new model apart from GPT-4o is its ability to tackle complex problems, such as coding and math, much better than its predecessors while also explaining its reasoning, according to OpenAI. \u201cThe model is definitely better at solving the AP math test than I am, and I was a math minor in college,\u201d OpenAI\u2019s chief research officer, Bob McGrew, tells me. He says OpenAI also tested o1 against a qualifying exam for the International Mathematics Olympiad, and while GPT-4o only correctly solved only 13 percent of problems, o1 scored 83 percent. In online programming contests known as Codeforces competitions, this new model reached the 89th percentile of participants, and OpenAI claims the next update of this model will perform \u201csimilarly to PhD students on challenging benchmark tasks in physics, chemistry and biology.\u201d At the same time, o1 is not as capable as GPT-4o in a lot of areas. It doesn\u2019t do as well on factual knowledge about the world. It also doesn\u2019t have the ability to browse the web or process files and images. Still, the company believes it represents a brand-new class of capabilities. It was named o1 to indicate \u201cresetting the counter back to 1.\u201d \u201cI\u2019m gonna be honest: I think we\u2019re terrible at naming, traditionally,\u201d McGrew says. \u201cSo I hope this is the first step of newer, more sane names that better convey what we\u2019re doing to the rest of the world.\u201d I wasn\u2019t able to demo o1 myself, but McGrew and Tworek showed it to me over a video call this week. They asked it to solve this\u00a0puzzle:  \u201cA princess is as old as the prince will be when the princess is twice as old as the prince was when the princess\u2019s age was half the sum of their present age. What is the age of prince and princess? Provide all solutions to that question.\u201d The model buffered for 30 seconds and then delivered a correct answer. OpenAI has designed the interface to show the reasoning steps as the model thinks. What\u2019s striking to me isn\u2019t that it showed its work \u2014 GPT-4o can do that if prompted \u2014 but how deliberately o1 appeared to mimic human-like thought. Phrases like \u201cI\u2019m curious about,\u201d \u201cI\u2019m thinking through,\u201d and \u201cOk, let me see\u201d created a step-by-step illusion of thinking.  But this model isn\u2019t thinking, and it\u2019s certainly not human. So, why design it to seem like it is? OpenAI doesn\u2019t believe in equating AI model thinking with human thinking, according to Tworek. But the interface is meant to show how the model spends more time processing and diving deeper into solving problems, he says. \u201cThere are ways in which it feels more human than prior models.\u201d \u201cI think you\u2019ll see there are lots of ways where it feels kind of alien, but there are also ways where it feels surprisingly human,\u201d says McGrew. The model is given a limited amount of time to process queries, so it might say something like, \u201cOh, I\u2019m running out of time, let me get to an answer quickly.\u201d Early on, during its chain of thought, it may also seem like it\u2019s brainstorming and say something like, \u201cI could do this or that, what should I do?\u201d Building toward agents Large language models aren\u2019t exactly that smart as they exist today. They\u2019re essentially just predicting sequences of words to get you an answer based on patterns learned from vast amounts of data. Take ChatGPT, which tends to mistakenly claim that the word \u201cstrawberry\u201d has only two Rs because it doesn\u2019t break down the word correctly. For what it\u2019s worth, the new o1 model did get that query correct. As OpenAI reportedly looks to raise more funding at an eye-popping $150 billion valuation, its momentum depends on more research breakthroughs. The company is bringing reasoning capabilities to LLMs because it sees a future with autonomous systems, or agents, that are capable of making decisions and taking actions on your behalf.  For AI researchers, cracking reasoning is an important next step toward human-level intelligence. The thinking is that, if a model is capable of more than pattern recognition, it could unlock breakthroughs in areas like medicine and engineering. For now, though, o1\u2019s reasoning abilities are relatively slow, not agent-like, and expensive for developers to use. \u201cWe have been spending many months working on reasoning because we think this is actually the critical breakthrough,\u201d McGrew says. \u201cFundamentally, this is a new modality for models in order to be able to solve the really hard problems that it takes in order to progress towards human-like levels of intelligence.\u201d " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/12/24242897/google-gemini-unlists-misleading-video-ai", "category": "Google", "date": "Sep 12", "author": "Jay Peters", "title": "Google unlists misleading Gemini video", "content": "Google has unlisted an impressive Gemini demo video it posted last December that seemed remarkably conversational. BBB National Programs\u2019 National Advertising Division (NAD), an ad industry watchdog, inquired whether the video \u201caccurately depicts the performance of Gemini in responding to user voice and video prompts.\u201d  Google chose to end the inquiry by ending its promotion of the video that showed Gemini quickly responding to various spoken prompts, such as identifying parts of drawings and creating a geography game on the fly. Buried in the description was a disclaimer indicating the demo might not be as good as it seemed: \u201cFor the purposes of this demo, latency has been reduced and Gemini outputs have been shortened for brevity.\u201d  Another note near the beginning of the video said, \u201cSequences shortened throughout.\u201d Google DeepMind\u2019s Oriol Vinyals also clarified that the video illustrated what \u201cthe multimodal user experiences built with Gemini could look like.\u201d \u201cGoogle is pleased to accept NAD\u2019s resolution of this matter,\u201d Google spokesperson\u00a0Gareth Evans says in a statement to The Verge.\u00a0\u201cThe video is still available in conjunction with the blog post that explains how the demonstration in the video was created.\u201d Here\u2019s that blog post, if you want to read it. Update, September 12th: Added statement from Google.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/12/24242789/meta-training-ai-models-facebook-instagram-photo-post-data", "category": "Tech", "date": "Sep 12", "author": "Jess Weatherbed", "title": "Meta fed its AI on almost everything you\u2019ve posted publicly since 2007", "content": "Meta has acknowledged that all text and photos that adult Facebook and Instagram users have publicly published since 2007 have been fed into its artificial intelligence models. Australia\u2019s ABC News reports that Meta\u2019s global privacy director, Melinda Claybaugh, initially rejected claims about user data from 2007 being leveraged for AI training during a local government inquiry about AI adoption before relenting after additional questioning. \u201cThe truth of the matter is that unless you have consciously set those posts to private since 2007, Meta has just decided that you will scrape all of the photos and all of the texts from every public post on Instagram or Facebook since 2007 unless there was a conscious decision to set them on private,\u201d Green Party senator David Shoebridge pushed in the inquiry. \u201cThat\u2019s the reality, isn\u2019t it?\u201d \u201cCorrect,\u201d Claybaugh responded. Meta\u2019s privacy center and blog posts acknowledge hoovering up public posts and comments from Facebook and Instagram to train generative AI: We use public posts and comments on Facebook and Instagram to train generative AI models for these features and for the open source community.We don\u2019t use posts or comments with an audience other than Public for these purposes. But the company has been vague about how data is used, when it started scraping, and how far back its collection goes. Asked by The New York Times in June, Meta didn\u2019t answer, other than to confirm that setting posts to anything besides \u201cpublic\u201d will prevent future scraping. That still won\u2019t delete data that has already been collected \u2014 and people posting back in 2007 (who may have been minors at the time) wouldn\u2019t have known their photos and posts would be used in this way. Claybaugh said that Meta doesn\u2019t scrape data from users who are under the age of 18. When Labor Party senator Tony Sheldon asked if Meta would scrape the public photos of his children on his own account, Claybaugh confirmed it would and was unable to clarify if the company also scraped adult accounts that were created when the user was still a child. European users can opt out due to local privacy regulations, and Meta was recently banned from using Brazilian personal data for AI training, but the billions of Facebook and Instagram users in other regions can\u2019t opt out if they want to keep their posts public. Claybaugh was unable to say if Australian users (or anyone else) would be given a choice to opt out in the future, arguing that the option was given to European users because of uncertainty regarding its regulatory landscape. \u201cMeta made it clear today that if Australia had these same laws Australians\u2019 data would also have been protected,\u201d Shoebridge said to ABC News. \u201cThe government\u2019s failure to act on privacy means companies like Meta are continuing to monetize and exploit pictures and videos of children on Facebook.\u201d   " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/24242800/ai-image-editing-photoshop-misinformation-deepfakes-elon-musk-grok-decoder-interview", "category": "Decoder", "date": "Sep 12", "author": "Nilay Patel", "title": "Why comparing AI image editing to Photoshop downplays the risks", "content": "We\u2019ve been covering the rise of AI image editing very closely here on Decoder and at The Verge overall for several years now \u2014 the ability to create photorealistic images with nothing more than a chatbot prompt has the potential to completely reset our cultural relationship with photography and, in particular, how much we instinctively trust photos to reflect the truth. But the debate over image editing and the inherent truth of photos is nothing new, of course. It\u2019s existed for as long as photography has existed, and it\u2019s raged since digital photo editing tools have become widely available. You know this argument; you\u2019ve heard it a million times. It\u2019s when people say, \u201cIt\u2019s just like Photoshop,\u201d with \u201cPhotoshop\u201d standing in for the concept of image editing generally. Today, we\u2019re exploring that argument, trying to understand exactly what it means and why our new world of AI image tools is different \u2014 and yes, in some cases, the same. Verge reporter Jess Weatherbed recently dove into this for us, and I asked her to join me in going through the debate and the arguments one by one to help figure it out. Because, sure, in many ways, AI image editing really is just a faster, easier version of Photoshop \u2014 even Adobe now has AI tech like its Firefly image generator built right into Photoshop. But making powerful tools instantly accessible to everyone has side effects, and we\u2019re seeing that right now.\u00a0 Say you want to generate an image of Donald Trump pointing a gun at Kamala Harris. Just ask Elon Musk\u2019s Grok, the AI chatbot built right into X. It\u2019ll do it no problem because it has very few of the same filters that have prevented competing AI products from depicting politicians or outright violence. How about a deepfake nude of a classmate? That\u2019s now made more trivial than ever before thanks to so-called \u201cnudification\u201d apps that manipulate existing photos, and it\u2019s fast becoming a national crisis.\u00a0 These might be old problems \u2014 Photoshop let you do all sorts of awful manipulations to celebrity photographs, and even in the days before computers, you could create convincing fake images to mislead people. But generative AI tools are testing whether the scale and sophistication of the tech and the speed of its adoption with little oversight has landed us firmly in uncharted territory. I\u2019ll just be direct here: my view is that people say \u201cit\u2019s just like Photoshop\u201d to diminish these new problems that AI tools are causing and to make them seem like they\u2019re already solved or not worth considering. But I would remind you that we hardly solved any of those problems when it really was just Photoshop \u2014 and any proposed solution that requires everyone to fundamentally understand that every image they see is edited isn\u2019t much of a solution at all.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/12/24219823/apple-intelligence-gemini-copilot-ai-iphone-16-pro-pixel-9-ram", "category": "iPhone", "date": "Sep 12", "author": "Wes Davis", "title": "There\u2019s more to this year\u2019s smartphones than AI", "content": "Apple revealed its iPhone 16 lineup on Monday, and the big selling point was Apple Intelligence. Apple\u2019s on-device AI system offers splashy features like the ability to rewrite emails, generate custom emoji, and a significantly upgraded Siri. But underneath it all, AI is delivering one other big change to the iPhone: more RAM. Although Apple never talks about RAM in its smartphones, MacRumors discovered that every iPhone 16 model now has 8GB of RAM, up from 6GB in the base models from last year. And it\u2019s not just Apple making changes like that. Last month, Google made similar changes to its AI-heavy Pixel 9; both the standard and Pro models saw an increase in RAM, making 12GB the least you can get this year. The impetus behind these RAM bumps appears to be artificial intelligence. AI is the year\u2019s new must-have feature, and it\u2019s also incredibly RAM-hungry. Smartphone makers are now bumping memory because they need to \u2014 whether they\u2019re saying that out loud or not. AI models need to be quick to respond when users call on them, and the best way to make that happen is to keep them perpetually loaded in memory. RAM responds far more quickly than a device\u2019s long-term storage; it would be annoying if you had to wait for an AI model to load before you could grab a quick email summary. But AI models are also fairly large. Even a \u201csmall\u201d one, like Microsoft\u2019s Phi-3-mini, takes up 1.8GB of space, and that means taking memory away from other smartphone functions that were previously making use of it. You can see how this played out very directly on Pixel phones. Last year, Google didn\u2019t enable local AI features on the standard model Pixel 8 due to \u201chardware limitations.\u201d Spoiler: it was the RAM. Android VP and general manager Seang Chau said in March that the Pixel 8 Pro could better handle Gemini Nano, the company\u2019s small AI model, because that phone had 4GB more RAM, at 12GB, than the Pixel 8. The model needed to stay loaded in memory at all times, and the implication was that the Pixel 8 would have lost too much memory in supporting the feature by default. \u201cIt wasn\u2019t as easy a call to just say, alright, we\u2019re just gonna enable it on the Pixel 8 as well,\u201d Chau said. Google eventually allowed Gemini Nano onto the Pixel 8, but only for people willing to run their phones in Developer Mode \u2014 people who Chau said \u201cunderstand the potential impact to the user experience.\u201d\u00a0 Those tradeoffs are why Google decided to boost RAM across the board with the Pixel 9. \u201cWe don\u2019t want the rest of the phone experiences to slow to accommodate the large model, hence growing the total RAM instead of squeezing into the existing budget,\u201d Google group product manager Stephanie Scott said in an email exchange with The Verge.\u00a0 So, is all of that extra RAM just going to AI, or will users see improved performance across the board? It\u2019s going to depend a lot on the implementation and just how large those models are. Google, which added 4GB to support local AI features, says you\u2019ll see improvements to both. \u201cSpeaking only to our latest Pixel phones,\u201d Scott wrote, \u201cyou can expect both better performance and improved AI experiences from their additional RAM.\u201d She added that Pixel 9 phones \u201cwill be able to keep up with future AI advances.\u201d But if those advances mean larger models, that could easily mean they\u2019ll be eating up more RAM. The same RAM-boosting trend is playing out in the laptop world, too. Microsoft dictated earlier this year that only machines with at least 16GB of memory can be considered a Copilot Plus PC \u2014 that is, a laptop capable of running local Windows AI features. It\u2019s rumored that Apple is planning to add more RAM to its next generation of laptops, too, after years of offering 8GB of RAM by default.\u00a0 That extra memory will be needed, especially if laptop makers want to keep even larger models loaded locally. \u201cI think most OSes will keep a LLM always-loaded,\u201d Hugging Face CTO Julien Chaumond told me in an email, \u201cso 6-8GB RAM is the sweet spot that will unlock that in parallel to the other things the OS is already doing.\u201d Chaumond added that models can then load or unload \u201ca small model on top of it to change some properties,\u201d such as a style for image generation or domain-specific knowledge for an LLM. (Apple describes its approach similarly.) Apple hasn\u2019t explicitly said how much RAM is necessary to run Apple Intelligence. But every Apple device that runs it, going back to the 2020 M1 MacBook Air, has at least 8GB of RAM. Notably, last year\u2019s iPhone 15 Pro, with 8GB of memory, can run Apple Intelligence, while the standard iPhone 15 with 6GB of RAM cannot. Apple AI boss John Giannandrea said in a June interview with Daring Fireball\u2019s John Gruber that limitations like \u201cbandwidth in the device\u201d and the neural engine\u2019s size would make AI features too slow to be useful on the iPhone 15. Apple VP of software engineering Craig Federighi said during the same appearance that \u201cRAM is one of the pieces of the total.\u201d The 2GB iPhone 16 RAM bump isn\u2019t ultimately a lot, but Apple has long been slow to expand baseline RAM across its devices. Any increase here feels like a win for usability, even if the company is starting small. We still don\u2019t know how useful Apple Intelligence will be or whether a slight jump in memory will be enough for today\u2019s iPhones to run tomorrow\u2019s AI features. One thing seems certain, though: we\u2019ll be seeing more of these sorts of hardware bumps as AI proliferates across the industry. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/11/24242138/google-notebook-llm-ai-fake-podcasts-research", "category": "Google", "date": "Sep 11", "author": "Emma Roth", "title": "Google is using AI to make fake podcasts from your notes", "content": "Google can now turn your research into an AI-generated podcast, complete with two \u201chosts\u201d that discuss what you\u2019ve dug up. The experimental feature lives within NotebookLM, the AI note-taking app Google launched last year, and will have AI hosts \u201csummarize your material, make connections between topics, and banter back and forth.\u201d It\u2019s meant to build on NotebookLM\u2019s existing features that help you interact with all your notes, transcripts, and other research documents. The app already uses Google\u2019s Gemini AI model to help summarize your research, and this is sort of like an audio version of that. Google isn\u2019t making things up when it says the AI hosts will \u201cbanter\u201d with each other, either. When trying out Audio Overview for myself, I plugged in one of the sample notebooks about the invention of the lightbulb, and the results were... a bit uncanny. During the 10-minute-long overview, the two hosts had a lighthearted discussion about how Thomas Edison wasn\u2019t the only person behind the lightbulb and that \u201cin the end, it\u2019s actually a story about teamwork, making the dream work.\u201d The hosts could almost be mistaken for human podcasters, from the way they emphasized \u201cbam!\u201d when tossing it in the middle of a sentence to using modern phrasing like \u201cmessy as heck.\u201d There were still a couple of quirks, as I noticed the AI spelling out certain words and phrases, like \u201cP-L-U-S.\u201d Some of the writing wasn\u2019t exactly what a human would say, either, with one AI host calling platinum \u201cbling bling metal.\u201d The feature makes learning about research more engaging, but I\u2019m wondering whether the hosts would maintain their lighthearted, somewhat jokey tone when discussing more serious topics, like cancer or war. There\u2019s quite a bit of filler during the conversation as well, so it might not be the best way to quickly and clearly distill all your information.  That\u2019s something Google mentions in its announcement, as it says the feature is \u201cnot a comprehensive or objective view of a topic, but simply a reflection\u201d of your notes. There are some other limitations to Audio Overview as well, as Google says it could take several minutes to generate a podcast-like discussion, and it\u2019s only available in English. Like many AI tools, it isn\u2019t always accurate. You can try out the feature for yourself by opening up a notebook in NotebookLM. From there, select the Notebook guide in the bottom-right corner of the screen, and then hit Load beneath the \u201cAudio Overview\u201d heading. I know I\u2019m going to be doing some research on a random topic just so I can hear what the AI podcasters have to say. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/11/24242142/sag-aftra-ai-now-gavin-newsom-safety-sb-1047-letters", "category": "Tech", "date": "Sep 11", "author": "Garrison Lovely", "title": "One of California\u2019s most influential unions weighs in on AI safety bill", "content": "As California Governor Gavin Newsom weighs signing or vetoing the fiercely contested AI safety bill SB 1047, SAG-AFTRA and two women\u2019s groups are pushing him to approve it \u2014\u00a0adding even more voices to an already frenzied debate. The performers union, the National Organization for Women (NOW), and Fund Her have each sent letters to Newsom,\u00a0all of which have been obtained by The Verge and are being published here for the first time. The letters from SAG-AFTRA, NOW, and Fund Her highlight concerns about AI\u2019s potential to cause catastrophic harm if the technology is left unregulated. SAG-AFTRA outlines SB 1047\u2019s mandate for developers to test for and safeguard against AI-enabled disasters, like cyberattacks on critical infrastructure or bioweapon development. NOW and Fund Her cite grave warnings from people at the forefront of AI and discuss the technology\u2019s potentially disproportionate impacts on vulnerable groups.\u00a0 SAG-AFTRA posted a call for support yesterday on X from its 160,000 members, which include stars like Scarlett Johansson and Tom Hanks. NOW, the largest feminist organization in the US with around 500,000 members, said it was motivated by expert claims \u201cabout how dangerous this incredible technology can be if it is not developed and deployed responsibly.\u201d Fund Her, a PAC that has helped elect 12 progressive women to prominent positions in California in 2022, writes of the \u201crace to develop the first independent thinking AI,\u201d at which point \u201cit will be too late to impose safety guardrails.\u201d SAG-AFTRA and NOW represent the latest power players to weigh in on the California bill, which has become the object of exceptional national interest and scrambled conventional partisan boundaries.\u00a0 SB 1047, authored by state Senator Scott Wiener, would be the most significant AI safety law in the US. It establishes civil liability for developers of next-generation AI models like ChatGPT if they cause disasters without implementing appropriate safeguards. The bill also includes whistleblower protections for employees of AI companies, garnering support from OpenAI whistleblowers Daniel Kokotajlo and William Saunders.\u00a0 NOW writes in its letter that \u201cthe AI safety standards set by California will change the world,\u201d a view echoed by bill cosponsor Dan Hendrycks, director of the Center for AI Safety. Hendrycks tells The Verge that SB 1047 could be Newsom\u2019s \u201cPat Brown moment,\u201d referring to California\u2019s then-governor signing a groundbreaking auto tailpipe emissions law in 1966. He quotes what\u2019s since become known as the California Effect: \u201cwhere California leads on important regulation, the rest of the country follows.\u201d Having passed both houses of the state legislature with strong majorities in late August, the bill now awaits Governor Newsom\u2019s decision, due by September 30th. The governor\u2019s office said it doesn\u2019t \u201ctypically comment on pending legislation. This measure will be evaluated on its merits.\u201d This comment notwithstanding, the fate of SB 1047 may come down to a political calculation \u2014 a reality each side appears to recognize as they marshal support in the bill\u2019s final hours.\u00a0 The odd political coalitions that have emerged in the fight over SB 1047 augur a topsy-turvy future for AI policy battles. Billionaire Elon Musk aligns with social justice groups and labor unions in supporting the bill, while former House Speaker Nancy Pelosi, progressive House Congressman Ro Khanna, Trump-supporting venture capitalist Marc Andreessen, and AI \u201cgodmother\u201d Fei-Fei Li are all opposed. AI is the rare issue that hasn\u2019t yet sorted into clear partisan camps. As the technology grows in importance, the debate over how to govern it is likely to grow in intensity and may continue to scramble the usual allegiances. These recent letters join support for the bill from organizations like the nearly 2-million-strong SEIU and the Latino Community Foundation.\u00a0 SAG-AFTRA has been home to some of the most organic anti-AI sentiment. Many screen actors see generative AI as an existential threat to their livelihoods. The use of the technology was a major sticking point in the 2023 actors strike, which resulted in a requirement that studios get informed consent from performers before creating digital replicas of them (actors must also be compensated for their use).\u00a0 The union\u2019s letter writes that \u201cSAG-AFTRA knows all too well the potential dangers that AI poses,\u201d citing problems experienced by its members in the form of nonconsensual deepfake pornography and theft of performers\u2019 likenesses. It concludes that \u201cpolicymakers have a responsibility to step in and protect our members and the public. SB 1047 is a measured first step to get us there.\u201d In a phone interview, organization president Christian Nunes said NOW got involved because the group is worried about how unregulated AI can affect women. She and NOW have previously supported efforts to ban nonconsensual deepfakes.\u00a0 In the NOW letter, Nunes writes that the dangers warned of by AI experts \u201cwould disproportionately fall on vulnerable groups, including women.\u201d She highlights Newsom\u2019s \u201ccourageous support for us in the face of intense lobbying pressure\u201d on reproductive rights, equal pay, and paid family leave, and that this support \u201cis one of the reasons why women have voted for [him] time and time again.\u201d\u00a0 While SB 1047 isn\u2019t explicitly designed to address these groups\u2019 more central concerns, the organizations seem to see strategic value in joining the coalition behind it. Nunes told The Verge she views the bill as part of a broader project to hold Big Tech accountable. This support for SB 1047 complements other pending AI legislation that more directly addresses these groups\u2019 specific issues. For instance, the federal NO FAKES Act aims to combat deepfakes, while another AI bill on Newsom\u2019s desk, endorsed by SAG-AFTRA, would regulate the use of digital replicas. By backing SB 1047 alongside these more targeted initiatives, these organizations appear to be taking a comprehensive approach to AI governance. The NOW and Fund Her letters both draw parallels between unregulated AI and the history of social media. Fund Her founder and president Valerie McGinty writes to The Verge, \u201cWe have seen the incredible harm social media has imposed on our children and how difficult it is to reverse it. We won\u2019t be stuck playing catch up again if Governor Newsom signs SB 1047 into law.\u201d It\u2019s unclear if the letters will be enough for the bill to overcome the powerful forces arrayed against it. While Wiener and other advocates describe the regulation as \u201clight-touch\u201d and \u201ccommon sense,\u201d the industry is, by and large, freaking out.\u00a0 The US currently relies almost entirely on self-regulation and nonbinding voluntary commitments to govern AI, and the industry would like to keep it that way. As the first US AI safety regulation with teeth, SB 1047 would set a powerful precedent, which is a likely motivation behind both these letters and the vigorous industry opposition.\u00a0 Google, Meta, and OpenAI took the unusual step of writing their own letters opposing the bill. Resistance from AI investors has been even stiffer, with the prestigious startup incubator Y Combinator (YC) and the venture fund Andreessen Horowitz (a16z) leading a full-court press to kill SB 1047. These and other prominent opponents warn that the bill could prompt an exodus from California, cede the US lead in AI to China, and devastate the open source community.\u00a0 Naturally, supporters dispute each of these arguments. In a July letter addressing YC and a16z\u2019s claims about the bill, Wiener points out that SB 1047 would apply to any covered AI company doing business in California, the world\u2019s AI hub and fifth-largest economy. Dario Amodei, CEO of leading AI company and eventual de facto SB 1047 supporter Anthropic, called the threat to leave \u201cjust theater\u201d (it has nonetheless also been invoked by OpenAI, Meta, and Google).\u00a0 In her statement opposing the bill, Pelosi called it \u201cwell-intentioned but ill informed.\u201d In a phone interview, Wiener said, \u201cI have enormous respect for the Speaker Emerita. She is the GOAT,\u201d but went on to call Pelosi\u2019s statement \u201cunfortunate\u201d and noted that \u201csome of the top machine learning pioneers on the planet support the bill,\u201d citing endorsements from deep learning \u201cgodfathers\u201d Geoffrey Hinton and Yoshua Bengio. Wiener also highlights a supportive open letter published Monday from over 100 employees and alumni of the leading AI companies. For evaluating SB 1047 on its merits, the most convincing letter might be one published by Anthropic, which broke from its peers to write that the revised legislation\u2019s \u201cbenefits likely outweigh its costs.\u201d This letter followed a round of amendments made directly in response to the company\u2019s prior complaints. Anthropic\u2019s Claude family of chatbots leads the world on some metrics, and the company will likely be one of the handful of AI developers directly covered by the law in the near future.\u00a0 With key congressional leaders promising to obstruct substantive federal AI regulations and opposing SB 1047, California may go it alone, as it already has on net neutrality and data privacy. As NOW\u2019s Nunes writes, the \u201cAI safety standards set by California will change the world,\u201d giving Governor Newsom a chance to make history and model \u201cbalanced AI leadership.\u201d Fund Her\u2019s McGinty summed up the supporters\u2019 stance in an email to The Verge: \u201cWe should listen to these experts more interested in our wellbeing than the Big Tech executives skimping on AI safety.\u201d\u00a0 As the September 30th deadline approaches, all eyes are on Governor Newsom to see how he\u2019ll shape the future of AI governance in California and beyond. \u201cMy experience with Gavin Newsom is \u2014 agree or disagree \u2014 he makes thoughtful decisions based on what he thinks is best for the state,\u201d says Wiener. \u201cI\u2019ve always appreciated that about him.\u201d Correction: The article initially cited deep learning \u201cgodfather\u201d Yann LeCun as a supporter of SB 1047. LeCun is opposed to the bill. We regret the error. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/11/24241649/adobe-firefly-text-to-video-generative-ai-features-preview", "category": "Adobe", "date": "Sep 11", "author": "Jess Weatherbed", "title": "Adobe previews its upcoming text-to-video generative AI tools", "content": "Adobe has teased some of its upcoming generative AI video tools, including a new feature that can produce video clips from still images. This latest preview builds on the in-development Firefly video model that the software giant demonstrated in April, which is set to power AI video and audio editing features across Adobe\u2019s Creative Cloud applications. The new promotional teaser shows footage produced by Firefly\u2019s text-to-video capabilities that Adobe announced (but didn\u2019t demonstrate) earlier this year. The tool allows users to generate video clips using text descriptions and adjust the results using a variety of \u201ccamera controls\u201d that simulate camera angles, motion, and shooting distance. An image-to-video feature was also demonstrated for the Firefly video model that can generate clips using specific reference images. Adobe suggests this could be useful for making additional B-roll footage or to patch gaps in production timelines. If the example footage is any indication of the final release, the generated video quality looks on par with what we\u2019ve seen from OpenAI\u2019s Sora model so far, which Adobe is also \u201cexploring\u201d as a third-party integration for its Premiere Pro video software. Duration is limited, though, with Adobe\u2019s VP of generative AI, Alexandru Costin, telling The Verge that videos produced by the text-to-video and image-to-video features have a maximum length of five seconds.  One advantage Adobe\u2019s own model may have against Sora is its promise that Firefly is \u201ccommercially safe\u201d due to being trained on openly licensed, public domain, and Adobe Stock content, which could reduce some concerns about copyright infringement. The text-to-video and image-to-video features will both initially be available in beta as a standalone Firefly application sometime later this year. Adobe says the new Firefly video model will eventually be integrated into its Creative Cloud, Experience Cloud, and Adobe Express applications. The company also showed off some additional clips of the upcoming \u201cGenerative Extend\u201d feature for Premiere Pro that can extend the length of existing video footage, similar to Photoshop\u2019s Generative Expand tool for image backgrounds. Adobe says this will also be arriving on an unspecified date \u201clater this year.\u201d " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/10/24241538/taylor-swift-endorses-kamala-harris-donald-trump-ai-endorsement-deepfake", "category": "Tech", "date": "Sep 11", "author": "Mia Sato", "title": "Taylor Swift endorses Kamala Harris in response to fake AI Trump endorsement", "content": "Taylor Swift said on Tuesday that she plans to vote for Vice President Kamala Harris in November\u2019s presidential election \u2014 and that AI-generated images circulating of herself pushed her in part to make her support public. \u201cRecently I was made aware that AI of \u2018me\u2019 falsely endorsing Donald Trump\u2019s presidential run was posted to his site. It really conjured up my fears around AI, and the dangers of spreading misinformation,\u201d Swift wrote in an Instagram post. \u201cIt brought me to the conclusion that I need to be very transparent about my actual plans for this election as a voter. The simplest way to combat misinformation is with the truth.\u201d Her post references an incident in late August in which Trump shared a collection of images to Truth Social intended to show support for his presidential campaign. Some of the photos depict \u201cSwifties for Trump,\u201d and another obviously AI-generated photo shows Swift herself in an Uncle Sam-type image with text reading, \u201cTaylor wants YOU to vote for Donald Trump.\u201d The former president captioned the post, \u201cI accept!\u201d In her endorsement post, Swift also mentioned LGBTQ+ rights, reproductive care, and IVF as specific issues she cares about. She also directed fans to her Instagram story, where she added a link to register to vote. The potential for abuse of AI tools in the lead-up to the US presidential election has been a lingering concern as generative AI has become widely available. In January, before Harris ascended to the top of the Democratic ticket, some voters in New Hampshire got a fake AI-generated robocall that sounded like President Joe Biden. The call \u2014 which discouraged people from voting in the state\u2019s upcoming primary election \u2014 went to more than 20,000 people, according to CNN. Some AI companies have increased restrictions on tools in an effort to cut down on election-related misinformation. Google, for example, recently announced it would limit election queries in AI Overviews, the company\u2019s AI-generated search results feature. This wasn\u2019t the first time AI images of Swift were circulated on social media. Earlier this year, nonconsensual sexualized images of her made using AI were shared on X. That incident prompted the White House to call for legislation to \u201cdeal\u201d with the issue.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/10/24237714/apple-intelligence-generative-ai-features-update-schedule", "category": "Apple", "date": "Sep 10", "author": "Umar Shakir", "title": "Here\u2019s what your iPhone 16 will do with Apple Intelligence \u2014 eventually", "content": "Apple heavily sprinkled mentions of AI throughout its iPhone 16 event on Monday. However, generative Apple Intelligence features won\u2019t be ready for the public launch of iOS 18 on September 16th or the new iPhones when they\u2019re released on September 20th. The first set of Apple\u2019s AI features is scheduled for public availability next month in most regions \u2014 except the EU \u2014 as part of a beta test for iPhone 15 Pro and all iPhone 16s, plus Macs and iPads with M1 or higher Apple Silicon chips. At launch, they\u2019ll be available in US English only. What\u2019s coming to Apple Intelligence in October Writing Tools Text Rewrite: Text Rewrite will morph your email writing draft into a more professional one, and you can change the tone to be friendly or concise as well.Proofread: As in real life, this proofreading feature should correct your grammar and sentence structure and suggest better words throughout your work.Summarize Text: It will be like letting AI do a TL;DR for you. Summarize Text will shorten your writing to just the important parts or create a bulleted list or table. Smart Reply: We\u2019ve seen this AI feature shown off quite a bit. Smart Reply will give you a few contextual suggestions to get you started on a reply in Mail or elsewhere. New Siri New look: On iPhone, iPad, or CarPlay, Siri will appear as a rainbow ring around the edges of the screen, and on Mac, Siri can float and be placed anywhere on your desktop.Apple\u2019s new language model: Siri should also get a bit smarter and better at parsing natural language thanks to Apple\u2019s on-device language model. Meanwhile, more complex questions will be sent to Apple\u2019s \u201cPrivate Cloud Compute\u201d server, which Apple claims acts as a computational extension to your device and does not retain any data.Type to Siri: Instead of talking, you\u2019ll be able to type questions to the assistant anytime. Photos Clean Up: Similar to Google\u2019s Magic Eraser, Clean Up will remove unwanted objects in your photos.Search: You\u2019ll be able to search for photos using natural language to find specific subjects you\u2019re looking for but can\u2019t find scrolling through your library.Memories: You\u2019ll be able to make a movie using media from your Photos library by writing out a prompt, and it should create a narratively driven story with chapters. Transcription Phone call recording and transcription: You\u2019ll be able to record phone calls and get a transcription of the whole call. Activating this feature will tell all parties that the call is being recorded.Voice recordings in Notes: You\u2019ll be able to record audio within the actual Notes app, and it will transcribe speech into text. You can also use Apple\u2019s other writing tools to help summarize the whole session. These Apple Intelligence features are arriving later Apple says other AI features will \u201croll out later this year and in the months following.\u201d That means these features could arrive as soon as October, or they could arrive next summer or fall. Unfortunately, these are also some of the most eye-catching features coming to Apple Intelligence. Visual Intelligence: Apple\u2019s new Visual Intelligence introduced during the iPhone 16 presentation can search for things by just snapping a photo. You could, for instance, take a picture of a cafe storefront and get information about it, like hours and its menu, or take a photo of a concert poster and add it to your calendar. Visual Intelligence, when it arrives, will be activated using the Camera Control side button on iPhone 16 and 16 Pro.Genmoji: You\u2019ll be able to create your own emoji by entering a text prompt, and Apple\u2019s image generator will make you a new emoji you can send to friends. Image Playground: In addition to making custom emoji, Apple Intelligence will also eventually create custom images. Enter a text prompt for whatever image you\u2019d like (assume some actual restrictions will apply), and Apple\u2019s models will conjure up a picture for you. Siri Personal Context: Siri\u2019s usefulness will evolve later by contextually helping you with onscreen information on your iPhone, iPad, or Mac.OpenAI connection: Anywhere there are Apple Intelligence writing tools, you will also have the option to use ChatGPT for additional generative AI options. ChatGPT should also be able to process your Siri requests for more advanced answers to questions.Third-party app connections with Siri: Apple\u2019s also promising Siri will, one day, complete in-app requests, like making photo edits in an image editing app using pictures in your Photos app. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/10/24240955/espn-generative-ai-reports-womens-soccer-lacrosse-premier-league", "category": "Sports", "date": "Sep 10", "author": "Wes Davis", "title": "ESPN\u2019s AI-generated sports recaps are already missing the point", "content": "This weekend, ESPN began publishing AI-generated recaps of women\u2019s soccer games, with more sports to come. It\u2019s using Microsoft AI to write each story, with humans only involved in reviewing each recap for \u201cquality and accuracy.\u201d ESPN says these stories will \u201caugment,\u201d rather than detract from, its other content \u2014 but needless to say, people have feelings about it.  It\u2019s not that ESPN is masquerading AI work as that of humans. In fact, each story advertises that it\u2019s written by \u201cESPN Generative AI Services,\u201d and ESPN includes a note at the bottom of each article about how the recap is based on a transcript from the sporting event.  ESPN isn\u2019t the only news organization that does this; The Associated Press started using AI to write sports recaps back in 2016, and both organizations pitch this as a way to cover more underserved sports. In addition to soccer, ESPN will also use it for lacrosse.  But so far, the stories are very bland, basic write-ups \u2014 and they\u2019re already missing important nuance, as Parker Molloy points out. One of the National Women\u2019s Soccer League stories failed to mention the significance of one player\u2019s final game and the emotional moments that happened as a result, something ESPN waved at with a later update to the story. ESPN argued that the AI summaries free up its writers to focus on more in-depth work like \u201cmore differentiating features, analysis, investigative, and breaking news coverage,\u201d and in this instance, a human reporter did write an entire story about Alex Morgan\u2019s emotional exit. Columnist Tom Jones wrote for Poynter last week that despite ESPN\u2019s justification that AI frees up journalists for more impactful work, there\u2019s nothing stopping ESPN \u201cfrom using AI to cover more and more other sports\u201d down the line. Jones points to Luis Paez-Pumar\u2019s column for Defector, where he writes that ESPN is \u201cfeeding existing soccer and lacrosse journalists\u2019 work into a machine aimed at making them obsolete\u201d rather than hiring them to do this work.  ESPN says it does indeed plan to extend these AI recaps to more sports. Soccer and lacrosse are merely \u201cits first experimentation with AI-generated content.\u201d Musicians, news organizations, and other creatives are fighting the rise of AI in court, arguing it trains on the work of humans without permission.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/10/24241043/apple-iphone-16-pro-intelligence-ai-missing", "category": "Apple", "date": "Sep 10", "author": "Jay Peters", "title": "The iPhone 16 will ship as a work in progress", "content": "Apple\u2019s all in on AI \u2014 at least Apple\u2019s version. \u201cThe next generation of iPhone has been designed for Apple Intelligence from the ground up,\u201d Apple CEO Tim Cook said before revealing the iPhone 16. Software chief Craig Federighi pitched Apple Intelligence as a \u201cpersonal intelligence system\u201d that\u2019s at \u201cthe heart of the iPhone 16 lineup.\u201d After the event, Apple even published a whole press release dedicated to Apple Intelligence. There\u2019s just one catch: when the iPhone 16 and 16 Pro first come out, they won\u2019t have any Apple Intelligence features. Sure, the new A18 and A18 Pro chips in the iPhone 16 lineup each have a 16-core Neural Engine that Apple says is \u201coptimized for large generative models,\u201d so they will probably be good at handling Apple Intelligence features. Yes, Apple has been testing Apple Intelligence upgrades, like a new design for Siri and tools that can help you improve your writing, remove objects from photos, and summarize notifications, as part of an iOS 18.1 beta for developers. But unless you\u2019re running that beta, you won\u2019t be able to put\u00a0those features to the test for a while. A few Apple Intelligence features will arrive soon-ish, as Apple gave a vague October release window for iOS 18.1 as part of its iPhone 16 announcements. But I should note that the Apple Intelligence features will still be called a beta and only available in US English to start. (Apple says it will launch Apple Intelligence in Chinese, French, Japanese, and Spanish starting next year.) The company\u2019s arguably more powerful Apple Intelligence upgrades, like a tool to make images, a feature that lets you generate custom emoji, Siri improvements that let it understand your personal context, and integration with ChatGPT are rolling out on a very vague timeline of \u201clater this year and in the months following.\u201d\u00a0 There are some indications about when: Bloomberg\u2019s Mark Gurman reports that the image generation features will launch with iOS 18.2 in December, and Apple said at WWDC that the ChatGPT integration is set to launch \u201clater this year.\u201d But despite how much of a spotlight Apple is putting on its AI features, it\u2019s being quite cagey about when those features might actually come out. Apple didn\u2019t reply to a request for comment. Despite how much the big tech companies have talked about AI over the past year or two, there are still concerns about AI tools, too. There\u2019s the hallucinating and potential generating bad stuff and misinformation. And Apple Intelligence hasn\u2019t exactly wowed beta testers with innovations or must-have tools. Apple\u2019s slow rollout could give it time to work out issues. But do you want to wait around until it does? If you were looking at Apple\u2019s AI features as the main reason to get a new phone, you probably shouldn\u2019t. Maybe just wait to upgrade until next year \u2014 or at least until October. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/9/24240230/ios-18-release-date-iphone-16", "category": "Apple", "date": "Sep 9", "author": "Emma Roth", "title": "iOS 18 will launch next week with new ways to customize your homescreen", "content": "Apple has revealed the launch date for iOS 18 \u2014 and it\u2019s just days away. The update, which will add new ways to customize your iPhone\u2019s homescreen and lockscreen, arrives on September 16th. In iOS 18, you can freely rearrange apps and widgets on your homescreen and change their appearance. Apple is rolling out a redesigned Control Center, too, along with a new password management app and support for satellite messaging. Some other updates include new text effects in messages, a revamped Photos app, and new ways to organize your inbox in the Mail app. However, this update doesn\u2019t include the Apple Intelligence features coming to the iPhone 15 Pro and across the iPhone 16 lineup. The upcoming tools will help you rewrite and summarize text and generate images through the AI-powered Image Playground. Alongside a more conversational Siri, these features are going to start arriving in the iOS 18.1 public beta in October. Apple also confirmed that macOS Sequoia, watchOS 11, and visionOS 2 are coming September 16th as well. While macOS Sequoia adds iPhone mirroring, watchOS 11 comes with new training features, and visionOS 2 includes an ultrawide virtual Mac display. There\u2019s still no word on tvOS 18.   Related: " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/9/24240094/apple-visual-intelligence-camera-control-iphone-16-ai-camera-control-google-lens", "category": "Apple Event", "date": "Sep 9", "author": "Wes Davis", "title": "Apple\u2019s Visual Intelligence is a built-in take on Google Lens", "content": "Apple has announced a new feature called Visual Intelligence that will be part of iOS 18\u2019s Apple Intelligence suite of AI features \u201clater this year.\u201d The feature works much like similar features offered by other multimodal AI systems from Google or OpenAI. Visual Intelligence lets you \u201cinstantly learn about everything you see,\u201d Apple\u2019s Craig Federighi said during the company\u2019s September event today. Federighi said the feature is \u201cenabled by Camera Control,\u201d which is the company\u2019s name for a new capacitive camera button that\u2019s now on the side of the iPhone 16 and 16 Pro phones. To trigger it, users will need to click and hold the button, then point the phone\u2019s camera at whatever they\u2019re curious about.  iPhones use a \u201ccombination of on-device intelligence and Apple services that never store your images\u201d to power Visual Intelligence and let you take a picture of a restaurant to get info about its hours. Point your camera at a flyer, and \u201cdetails like title, date, and location are automatically recorded,\u201d he said. Federighi added that the feature is \u201calso your gateway to third-party\u201d models, which suggests using Visual Intelligence to search Google for a bike that you find out in the wild or take a picture of study notes to get help with a concept.  Apple didn\u2019t announce when the feature would debut beyond that it\u2019s \u201ccoming to Camera Control later this year.\u201d   Related: " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/9/24236176/apple-iphone-16-liveblog-apple-watch-x-airpods-keynote", "category": "Apple Event", "date": "Sep 9", "author": "Verge Staff", "title": "iPhone 16 event live blog: all the news from Apple\u2019s keynote", "content": "It\u2019s glowtime, baby. Apple is expected to announce the iPhone 16 today \u2014 and the launch of Apple Intelligence along with it. Apple is late to the AI party, which makes its arrival all the more intriguing.  While we got a glimpse of Apple Intelligence at WWDC, we\u2019re expecting a more practical look at how it\u2019ll show up in the iPhone 16 and 16 Pro (plus all the usual upgrades like new colors, better cameras \u2014 the whole shebang). But phones and AI aren\u2019t the only things on the agenda. It\u2019s the 10th anniversary of the Apple Watch, and rumor has it, it\u2019ll sport a bigger display and thinner body. The AirPods lineup is also due for a refresh, with murmurs of new entry and midtier models.  That\u2019s a jam-packed agenda, and the event kicks off at 1PM ET / 10AM PT. We\u2019re on the ground reporting live from Cupertino, California, and you\u2019ll also be able to follow along with the stream here.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/9/24239903/amazon-audible-audiobook-narrators-ai-generated-voice-clones", "category": "Amazon", "date": "Sep 9", "author": "Jess Weatherbed", "title": "Amazon is allowing Audible narrators to clone themselves with AI", "content": "Amazon will begin inviting a small group of Audible narrators to train AI-generated voice clones of themselves this week, with the aim of speeding up audiobook production for the platform. The US-only beta test was announced on Audible\u2019s creator marketplace and will be extended to rights holders like authors, agents, and publishers \u201clater this year,\u201d according to Amazon. \u201cThere is a vast catalog of books that does not yet exist in audio and as we explore ways to bring more books to life on Audible, we\u2019re committed to thoughtfully balancing the interests of authors, narrators, publishers, and listeners,\u201d Amazon said in its announcement. Participants in the beta will submit a voice recording to train their AI replica and will retain control over the projects they wish to audition for across both live performances and AI-generated recordings. Narrators can also use Amazon\u2019s production tools to edit the pronunciation and pacing of their AI voice replica if a rights holder selects them for a project, alongside reviewing the final production for any errors or inaccuracies. Amazon says that narrators will be compensated via a \u201cRoyalty Share\u201d model on a \u201ctitle-by-title basis\u201d but didn\u2019t expand on how much voice artists can expect to earn. The announcement blog says that beta participants can create a voice replica \u201cfor free,\u201d which implies that there may be an upfront cost involved for narrators in the future if the feature becomes generally available. Any titles that are narrated using voice replicas will be labeled on the product detail page. \u201cNarrators control what works are narrated with their voice replica,\u201d Amazon said. \u201cAudible will not separately use a narrator\u2019s voice replica for any content without their approval.\u201d  Amazon rolled out a similar feature last year that allows Kindle Direct Publishing authors to convert their titles into audiobooks using fully synthetic voices. Bloomberg reported in May that virtual voices were used on 40,000 Audible titles since release, sparking concerns from narrators like Ramon de Ocampo about the feature reducing job opportunities for human performers. As outlets like Brian\u2019s Book Blog have noted, Audible currently doesn\u2019t provide an easy way for users to filter out these \u201cVirtual Voice\u201d audiobooks if they\u2019re aiming to avoid them. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/7/24238537/apples-glowtime-iphone-16-event-how-to-watch-ios-18-apple-intelligence-ai", "category": "Apple", "date": "Sep 7", "author": "Wes Davis", "title": "How to watch Apple\u2019s \u2018Glowtime\u2019 iPhone 16 event", "content": "The iPhone 16 is nigh as Apple prepares to debut its next smartphone lineup on Monday, September 9th, at 1PM ET / 10AM PT. The company is expected to officially announce four phones as usual, with two standard iPhone 16s and two iPhone 16 Pro models, all likely packed with Apple Intelligence AI features. You can catch the livestream at Apple\u2019s own website, on its YouTube channel, and even on an Apple TV. (Apple usually makes sure you see this when you scroll from the homescreen of the Apple TV app, but you can search for \u201cApple Event\u201d to find it, too.) Besides new iPhones, Apple will probably declare the release dates of its next major software updates, including iOS 18, iPadOS 18, macOS 15 Sequoia, watchOS 11, and visionOS 2. The first three of those operating systems will include Apple Intelligence when it debuts, though you\u2019ll need at least an iPad or Mac with an M1 chip or an iPhone 15 Pro to take advantage.  We could also see new AirPods and possibly even a smaller Mac Mini, although that may be launched with other new Macs later this year instead. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/5/24236980/us-signs-legally-enforceable-ai-treaty", "category": "Tech", "date": "Sep 5", "author": "Emma Roth", "title": "US, EU, UK, and others sign legally enforceable AI treaty", "content": "The US, UK, and European Union have signed the first \u201clegally binding\u201d treaty on AI, which is supposed to ensure its use aligns with \u201chuman rights, democracy\u00a0and the\u00a0rule of law,\u201d according to the Council of Europe. The treaty, called the Framework Convention on Artificial Intelligence, lays out key principles AI systems must follow, such as protecting user data, respecting the law, and keeping practices transparent. Each country that signs the treaty must \u201cadopt or maintain appropriate legislative, administrative or other measures\u201d that reflect the framework. Andorra, Georgia, Iceland, Norway, the Republic of Moldova, San Marino, and Israel also signed the framework, which has been in the works since 2019.  Over the past several months, we\u2019ve seen a swath of other AI safety agreements emerge \u2014 but the majority don\u2019t have consequences for the signatories who break their commitments. Even though this new treaty is supposed to be \u201clegally binding,\u201d the Financial Times points out that \u201ccompliance is measured primarily through monitoring, which is a relatively weak form of enforcement.\u201d Still, the treaty could serve as a blueprint for countries developing their own laws surrounding AI. The US has bills in the works related to AI, the EU already passed landmark regulations on AI, and the UK is considering its own. California is also getting close to passing an AI safety law that giants like OpenAI have pushed back against. \u201cWe must ensure that the rise of AI upholds our standards, rather than undermining them,\u201d Council of Europe\u00a0Secretary General\u00a0Marija Pej\u010dinovi\u0107 Buri\u0107\u00a0says in a statement. \u201cThe Framework Convention is designed to ensure just that. It is a strong and balanced text \u2014 the result of the open and inclusive approach.\u201d The treaty will come into force three months after five signatories ratify it.   " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/5/24232414/xockets-accuses-nvidia-microsoft-patent-infringement-antitrust", "category": "Policy", "date": "Sep 5", "author": "Lauren Feiner", "title": "Startup accuses Nvidia and Microsoft of infringing on patents and forming a cartel", "content": "A startup funded by the cofounder of Yahoo and CTO of Intel is suing Nvidia and Microsoft for allegedly infringing on its patent for a key innovation in AI chips and being part of a buying cartel that allegedly sought to artificially fix lower prices for the technology. In a new lawsuit, Texas-based Xockets says Nvidia has infringed on its patented data processing unit (DPU) technology, which helps make cloud infrastructure more efficient by accelerating data-intensive workloads. Xockets says the chip giant inherited the infringement through its 2020 acquisition of Mellanox. It claims Mellanox initially infringed on its patent after Xockets publicly demonstrated its DPU tech at a conference in 2015.  Xockets alleges that three of Nvidia\u2019s DPUs \u2014 BlueField, ConnectX, and NVLink Switch \u2014 are based on Xockets\u2019 patented technology. The startup also accuses Microsoft of infringing on its patents, alleging that as an Nvidia customer, Microsoft has \u201cprivileged access to NVIDIA\u2019s infringing GPU-enabled server computer systems and components for AI.\u201d  Xockets says it\u2019s made Nvidia aware of the alleged infringement \u2014 it alleges the startup\u2019s founder and board member Parin Dalal raised the issue to Nvidia\u2019s DPU business VP in February 2022. Xockets accuses Nvidia of pursuing a strategy of \u201cefficient infringement,\u201d which basically boils down to infringe now, let lawyers figure out the rest later.  Xockets is also accusing Nvidia of monopolizing the market for GPU servers for AI and participating with Microsoft in a buying cartel through an organization called RPX, a company Xockets says was \u201cformed at the request of Big Tech companies to enable and create buyers\u2019 cartels for intellectual property.\u201d Xockets alleges that RPX enabled members like Nvidia and Microsoft to jointly boycott innovations like Xockets\u2019 in order to drive prices lower than if each company had negotiated on its own. Through the alleged cartel, Xockets claims, Microsoft and Nvidia are able to \u201cmonopolize GPU-enabled generative artificial intelligence by controlling the equipment and platforms necessary to access this capability.\u201d Xockets is seeking damages for the alleged infringement and for the court to order the companies to stop violating its patents and antitrust law. Though it\u2019s facing two of the largest companies in the country, Xockets investor and board member Robert Cote, an IP lawyer, told The Verge that Xockets has \u201cmore than enough wherewithal to take on Goliath.\u201d Dalal is a current employee at Google, where he\u2019s a principal engineer of machine learning and AI, though Google does not seem to have an official role in the litigation. Cote said he could not comment on Google. Nvidia and Google declined to comment. Microsoft and RPX did not immediately respond. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/5/24236841/youtube-ai-detection-tools-creators-singing-deepfakes", "category": "YouTube", "date": "Sep 5", "author": "Jess Weatherbed", "title": "YouTube is making new tools to protect creators from AI copycats", "content": "YouTube is developing new tools that aim to give creators on the platform more control over content that copies their voice or likeness using generative AI. In its announcement post, YouTube said the new likeness management tech will help to safeguard its creators and partners while enabling them to \u201charness AI\u2019s creative potential\u201d by promoting responsible AI development. The first tool, described as a \u201csynthetic-singing identification technology,\u201d will allow artists and creators to automatically detect and manage YouTube content that simulates their singing voices using generative AI. YouTube says the tool sits within its existing Content ID copyright identification system and that it\u2019s planning to test it under a pilot program next year.  The announcement follows YouTube\u2019s pledge last November to give music labels a way to take down AI clones of musicians. The rapid improvement and accessibility of generative AI music tools have sparked fears among artists regarding their use in plagiarism, copycatting, and copyright infringement. In an open letter earlier this year, over 200 artists, including Billie Eilish, Pearl Jam, and Katy Perry, described unauthorized AI-generated mimicry as an \u201cassault on human creativity\u201d and demanded greater responsibility around its development to protect the livelihoods of performers. A separate tool is also in the works that can identify facial deepfakes of creators, actors, musicians, and athletes on the platform. The system is still in active development, and YouTube hasn\u2019t indicated when it\u2019s expected to roll out. YouTube is also pledging to crack down on anyone scraping the platform to build AI tools. \u201cWe\u2019ve been clear that accessing creator content in unauthorized ways violates our Terms of Service,\u201d the platform said \u2014 which hasn\u2019t prevented companies like OpenAI, Apple, Anthropic, Nvidia, Salesforce, and Runway AI from training their AI systems on thousands of scraped YouTube videos. The protections against this activity include blocking scrapers from accessing YouTube and investments in scraping detection systems. \u201cAs AI evolves, we believe it should enhance human creativity, not replace it,\u201d YouTube said in its announcement. \u201cWe\u2019re committed to working with our partners to ensure future advancements amplify their voices, and we\u2019ll continue to develop guardrails to address concerns and achieve our common goals.\u201d YouTube also says it\u2019s developing ways to give creators more choices regarding how third-party AI companies are permitted to use their content on the platform and will share further details later this year.\u00a0 " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/5/24236817/google-ask-photos-ai-assistant-test", "category": "Google", "date": "Sep 5", "author": "Emma Roth", "title": "Google tests its \u2018Ask Photos\u2019 AI assistant that understands what\u2019s in your pictures", "content": "Google is testing its new \u201cAsk Photos\u201d feature that lets you explore your library of pictures in new ways. The feature, which Google first previewed in May, is rolling out to select Google Labs users in the US and will let you ask things like, \u201cWhere did we camp last time we went to Yosemite?\u201d or \u201cWhat did we eat at the hotel in Stanley?\u201d Using Google\u2019s Gemini AI models, the Photos app will then offer a response based on the content in your photos, as well as pull up images relevant to your question. Google says you can even use Ask Photos to complete tasks, such as summarizing things you did on a recent vacation or choosing the best pictures of your family to put in a shared album. You can sign up for the waitlist to access Ask Photos on Google\u2019s website. When using Ask Photos, Google will let you switch to what it now calls \u201cclassic search\u201d \u2014 or the current way of finding images. But Google is giving this an upgrade, too, as you can now search for pictures using natural language, such as \u201cAlice and me laughing\u201d or \u201cKayaking on a lake surrounded by mountains.\u201d You can then sort your search results by date or relevance. This feature is rolling out in English on Android and iOS, with more support for more languages arriving in the \u201ccoming weeks.\u201d In preparation for this change, Google Photos replaced the Library tab with a new Collection page that\u2019s supposed to make it easier to find all your photos and videos. While I haven\u2019t really had time to explore the new tab, I\u2019ll definitely be taking advantage of the natural language search so I can finally find specific images without having to scroll through thousands of images or narrow them down by location. " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/5/24236680/microsoft-onedrive-october-event-copilot-mobile-photos-updates", "category": "Microsoft", "date": "Sep 5", "author": "Tom Warren", "title": "Microsoft to detail OneDrive Copilot, mobile app updates, and more during October event", "content": "Microsoft is holding a OneDrive digital event on October 8th that will cover the \u201clatest innovations in AI across Microsoft 365 and OneDrive.\u201d It\u2019s the second annual event after Microsoft held a similar stream last year to introduce a big new design update for the cloud storage service, AI Copilot integration, and lots more. This year, Microsoft is promising to announce \u201cwhat\u2019s coming for Copilot in OneDrive,\u201d alongside enhancements to the OneDrive mobile app and an \u201cimproved photos experience.\u201d The event will also cover OneDrive features across work and personal accounts. The OneDrive digital event will be hosted on Microsoft Teams and will include the ability to ask the OneDrive product team questions about the cloud storage service. It follows a special Copilot event on September 16th, where Microsoft CEO Satya Nadella and vice president of AI at work Jared Spataro will focus on what\u2019s next for Microsoft\u2019s AI assistant. Microsoft is expected to announce a subtle rebranding of its business-focused Copilot assistant and introduce new Copilot features for Microsoft 365 that will try to tempt more businesses to sign up for the\u00a0$30 per user per month service.  " }, { "page": "https://www.theverge.com/ai-artificial-intelligence/archives/4", "link": "https://www.theverge.com/2024/9/4/24235910/asus-nuc-14-pro-ai-copilot-button-mini-pc", "category": "Microsoft", "date": "Sep 4", "author": "Tom Warren", "title": "Asus\u2019 new mini PC has a Copilot AI button on the front for some reason", "content": "Asus is launching its latest NUC mini PC today, complete with Intel\u2019s new Core Ultra Series 2 processors. For some reason, Asus has decided to put a dedicated Copilot button on the front of this puck-shaped PC, so you can reach out and launch Microsoft\u2019s AI assistant. I\u2019m not entirely sure why you\u2019d want an AI button on a miniature PC that\u2019s probably going to sit more than an arm\u2019s length away from you on a desk, but it\u2019s a button that has now progressed beyond Microsoft\u2019s effort to push it on keyboards. Asus also has a fingerprint reader on top of this NUC for Windows Hello authentication, which makes more sense so you can touch the fingerprint reader when you initially power on this mini PC. Aside from the Copilot button, this NUC is packed full of essential ports and connectivity. At the front, there are two USB-A 3.2 Gen 1 ports, a Thunderbolt 4 port, and a headphone jack. At the rear of the NUC 14 Pro AI, there\u2019s an ethernet port, another Thunderbolt 4 port, an HDMI port, and two USB-A 3.2 Gen 2 ports. Asus has also equipped this mini PC with Wi-Fi 7 and Bluetooth 5.4 support. There\u2019s even an internal speaker with a microphone, making this mini PC useful if you don\u2019t have a great speaker setup or headphones. The most interesting part is what\u2019s inside, though. Intel is promising big performance improvements with its latest Lunar Lake processors inside the NUC 14 Pro AI, with up to 2x GPU performance over the previous generation. Asus is also using a high-end fluid dynamic bearing fan to improve heat dissipation and keep Intel\u2019s chip running cool under high loads. The Asus NUC 14 Pro AI is also a Copilot Plus PC, so you\u2019ll get access to Microsoft\u2019s latest Windows AI features through an update that will be available in November. We don\u2019t have a price for the Asus NUC 14 Pro AI, but it\u2019s supposed to arrive later this year. " } ]
